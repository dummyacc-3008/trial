++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Assessment Details
Type: Proctored certification

Total number of questions: 45

Time limit: 90 minutes

Registration fee: $200

Question types: Multiple choice

Languages: English

Delivery method: Online proctored

Prerequisites: None, but related training highly recommended

Recommended experience: 6+ months of hands-on experience performing the generative AI solutions tasks outlined in the exam guide

Validity period: 2 years

Recertification: Recertification is required every two years to maintain your certified status. To recertify, you must take the current version of the exam. Please review the “Getting Ready for the Exam” section below to prepare for your recertification exam.

Unscored content: Exams may include unscored items to gather statistical information for future use. These items are not identified on the form and do not impact your score. Additional time is factored into the exam to account for this content.

❌ ✅ 

Generative AI Engineering Learning Path
01. Generative AI Fundamentals
02. Generative AI Solution Development
03. Generative AI Application Development
04. Generative AI Application Evaluation and Governance
05. Generative AI Application Deployment and Monitoring




Generative AI Fundamentals
https://partner-academy.databricks.com/learn/courses/1765/generative-ai-fundamentals/lessons/12496/introduction-to-generative-ai-fundamentals


Generative AI Solution Development
https://partner-academy.databricks.com/learn/courses/2706/generative-ai-solution-development?hash=8303a20b2cb56c118b981d43447e112ad73f07bc&generated_by=459209


Generative AI Application Development
https://partner-academy.databricks.com/learn/courses/2716/generative-ai-application-development?hash=47bda03e5eabde26bd5e91bb464e48523181ee89&generated_by=459209


Generative AI Application Evaluation and Governance
https://partner-academy.databricks.com/learn/courses/2717/generative-ai-application-evaluation-and-governance?hash=f7fe429e2506b3a6ffe4554490d82d28373e2873&generated_by=459209

 
Generative AI Application Deployment and Monitoring
https://partner-academy.databricks.com/learn/courses/2713/generative-ai-application-deployment-and-monitoring?hash=f160f1406b89ff21e850c77946333a4c4352678e&generated_by=459209


https://learn.microsoft.com/en-us/azure/databricks/machine-learning/


Skill Sections for Databricks GEN AI Certifications 

Section 1: Design Applications (14%)
● Design a prompt that elicits a specifically formatted response
● Select model tasks to accomplish a given business requirement
● Select chain components for a desired model input and output
● Translate business use case goals into a description of the desired inputs and outputs for the AI pipeline
● Define and order tools that gather knowledge or take actions for multi-stage reasoning

Section 2: Data Preparation (14%)
● Apply a chunking strategy for a given document structure and model constraints
● Filter extraneous content in source documents that degrades quality of a RAG application
● Choose the appropriate Python package to extract document content from provided source data and format.
● Define operations and sequence to write given chunked text into Delta Lake tables in Unity Catalog
● Identify needed source documents that provide necessary knowledge and quality for a given RAG application
● Identify prompt/response pairs that align with a given model task
● Use tools and metrics to evaluate retrieval performance

Section 3: Application Development (30%)
● Create tools needed to extract data for a given data retrieval need
● Select Langchain/similar tools for use in a Generative AI application.
● Identify how prompt formats can change model outputs and results
● Qualitatively assess responses to identify common issues such as quality and safety
● Select chunking strategy based on model & retrieval evaluation
● Augment a prompt with additional context from a user's input based on key fields, terms and intents
● Create a prompt that adjusts an LLM's response from a baseline to a desired output
● Implement LLM guardrails to prevent negative outcomes
● Write metaprompts that minimize hallucinations or leaking private data
● Build agent prompt templates exposing available functions
● Select the best LLM based on the attributes of the application to be developed
● Select a embedding model context length based on source documents, expected queries and optimization strategy
● Select a model for from a model hub or marketplace for a task based on model metadata/model cards
● Select the best model for a given task based on common metrics generated in experiments

Section 4: Assembling and Deploying Applications (22%)
● Code a chain using a pyfunc model with pre- and post-processing
● Control access to resources from model serving endpoints
● Code a simple chain according to requirements
● Code a simple chain using langchain
● Choose the basic elements needed to create a RAG application: model flavor, embedding model, retriever, dependencies, input examples, model signature
● Register the model to Unity Catalog using MLflow
● Sequence the steps needed to deploy an endpoint for a basic RAG application
● Create and query a Vector Search index
● Identify how to serve an LLM application that leverages Foundation Model APIs
● Identify resources needed to serve features for a RAG application


Section 5: Governance (8%)
● Use masking techniques as guard rails to meet a performance objective
● Select guardrail techniques to protect against malicious user inputs to a Gen AI application
● Recommend an alternative for problematic text mitigation in a data source feeding a RAG application
● Use legal/licensing requirements for data sources to avoid legal risk


Section 6: Evaluation and Monitoring (12%)
● Select an LLM choice (size and architecture) based on a set of quantitative evaluation metrics
● Select key metrics to monitor for a specific LLM deployment scenario
● Evaluate model performance in a RAG application using MLflow
● Use inference logging to assess deployed RAG application performance
● Use Databricks features to control LLM costs for RAG applications


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Sample Questions

These questions are similar to actual question items and give you a general sense of how questions are asked on this exam. 
They include exam objectives as they are stated on the exam guide and give you a sample question that aligns to the objective. 
The exam guide lists all of the objectives that could be covered on an exam. 
The best way to prepare for a certification exam is to review theexam outline in the exam guide.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
1

Skill Section: 

 
Question:  
A Generative Al Engineer has created a RAG application to look up answers to questions about a series of fantasy novels that are being asked on the author’s web forum. The fantasy novel texts are chunked and embedded into a vector store with metadata (page number, chapter number, book title), retrieved with the user’s query, and provided to an LLM for response generation. The Generative AI Engineer used their intuition to pick the chunking strategy and associated configurations but now wants to more methodically choose the best values.
Which TWO strategies should the Generative AI Engineer take to optimize their chunking strategy and parameters? (Choose two.)

Answer: 
3 and 5


Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Change embedding models and compare performance.

Option 2:
Add a classifier for user queries that predicts which book will best contain the answer. Use this to filter retrieval.

Option 3:
Choose an appropriate evaluation metric (such as recall or NDCG) and experiment with changes in the chunking strategy, such as splitting chunks by paragraphs or chapters. Choose the strategy that gives the best performance metric.

Option 4:
Pass known questions and best answers to an LLM and instruct the LLM to provide the best token count. Use a summary statistic (mean, median, etc.) of the best token counts to choose chunk size.

Option 5:
Create an LLM-as-a-judge metric to evaluate how well previous questions are answered by the most appropriate chunk. Optimize the chunking parameters based upon the values of the metric.


Explanation:
Explanation for Right Options:
Option 3:

Explanation: This option suggests using evaluation metrics like recall or NDCG (Normalized Discounted Cumulative Gain) to assess how well different chunking strategies retrieve relevant information. By experimenting with chunking strategies (e.g., splitting by paragraphs or chapters), the engineer can identify which configuration maximizes the chosen metric.
Justification: This method is systematic and data-driven, ensuring that the chunking strategy is optimized based on objective performance measures rather than intuition.

Option 5:

Explanation: This approach involves using an LLM-as-a-judge metric to evaluate the quality of responses generated from different chunking strategies. By comparing how well each chunk answers previous questions, the engineer can fine-tune the chunking parameters to improve overall retrieval and response accuracy.
Justification: This strategy leverages the capabilities of LLMs to assess content relevance and accuracy, providing a practical way to optimize chunking parameters based on real-world performance.


Explanation for Wrong Options:
Option 1:

Explanation: Changing embedding models and comparing performance focuses on the embeddings themselves rather than the chunking strategy.
Justification: The question is about optimizing chunking strategy and parameters, not about experimenting with different embedding models. While important, this option doesn't directly address the chunking strategy optimization.

Option 2:

Explanation: Adding a classifier to predict which book might contain the answer is about improving the retrieval process but not specifically related to optimizing chunk sizes or strategies.
Justification: This option introduces a different layer of filtering but does not focus on the core issue of chunking strategy optimization, which is the main concern of the engineer.

Option 4:

Explanation: This option involves using LLMs to predict optimal token counts for chunks based on known questions and answers.
Justification: While this could help in choosing chunk sizes, it relies on token count as a heuristic rather than performance metrics like recall or NDCG. It’s less methodical and may not yield the best performance in terms of information retrieval and relevance.


Summary:
The best strategies for optimizing the chunking strategy and parameters involve using evaluation metrics (Option 3) and leveraging LLM-based assessments (Option 5). 
These methods provide systematic and data-driven approaches to fine-tuning the chunking process, ensuring the highest possible performance for retrieving relevant information. 
Other options either focus on different aspects (like embedding models or classifiers) or rely on heuristics that are less directly tied to improving chunking strategies.


References:

Hands-On: 


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
2

Skill Section: 
 
Question:  
A Generative AI Engineer is designing a RAG application for answering user questions on technical regulations as they learn a new sport.
What are the steps needed to build this RAG application and deploy it?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Ingest documents from a source –> Index the documents and saves to Vector Search –> User submits queries against an LLM –> LLM retrieves relevant documents –> Evaluate model –> LLM generates a response –> Deploy it using Model Serving

Option 2:
Ingest documents from a source –> Index the documents and save to Vector Search –> User submits queries against an LLM –> LLM retrieves relevant documents –> LLM generates a response -> Evaluate model–> Deploy it using Model Serving


Option 3:
Ingest documents from a source –> Index the documents and save to Vector Search –> Evaluate model –> Deploy it using Model Serving

Option 4:
User submits queries against an LLM –> Ingest documents from a source –> Index the documents and save to Vector Search –> LLM retrieves relevant documents –> LLM generates a response –> Evaluate model–> Deploy it using Model Serving



Explanation:
Explanation for the Right Option:
Option 2:

Explanation: This option provides a clear and logical sequence for building and deploying a RAG (Retrieval-Augmented Generation) application. It starts with document ingestion, indexing, and saving to vector search. Then, it covers query submission, document retrieval by the LLM, response generation, model evaluation, and deployment.
Justification: The sequence ensures that the documents are prepared and indexed before any queries are processed. The model is evaluated before deployment to ensure its effectiveness, making this a comprehensive and methodical approach to building and deploying the RAG application.


Explanation for Wrong Options:
Option 1:

Explanation: Although similar to Option 2, the sequence places model evaluation before response generation, which is slightly out of logical order.
Justification: Evaluating the model before generating responses from user queries might miss potential performance issues that can only be detected during the response generation phase. Option 2 provides a more logical flow by evaluating after response generation.


Option 3:

Explanation: This option skips important steps, such as user query submission, document retrieval by the LLM, and response generation.
Justification: It omits critical components of the RAG workflow, making it an incomplete process for deploying the application. Without these steps, the system wouldn't be fully functional.

Option 4:

Explanation: This sequence starts with user query submission before document ingestion and indexing, which is not practical.
Justification: Documents need to be ingested and indexed before queries can be processed. Starting with query submission disrupts the logical flow of building the application, making this option incorrect.


Summary:
The correct steps for building and deploying a RAG application (Option 2) include 
1. 	Document ingestion
2. 	Indexing
3. 	Query processing
4. 	Document retrieval
5. 	Response generation
6. 	Model evaluation
7. 	Deployment
This sequence ensures that the system is built logically and comprehensively. 
The other options either misorder steps or omit critical components, making them less suitable for the task.
References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
3

Skill Section: 
 
Question:  
A Generative AI Engineer just deployed an LLM application at a digital marketing company that assists with answering customer service inquiries.
Which metric should they monitor for their customer service LLM application in production?

Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):
E

Guideline Time in Seconds (30, 45, 60):
30

Option 1:
Number of customer inquiries processed per unit of time

Option 2:
Energy usage per query

Option 3:
Final perplexity scores for the training of the model

Option 4:
HuggingFace Leaderboard values for the base LLM



Explanation:
Explanation for the Right Option:
Option 1:

Explanation: Monitoring the number of customer inquiries processed per unit of time is essential for understanding the efficiency and throughput of the LLM application. This metric helps gauge how well the application handles the workload and whether it meets performance expectations in a customer service context.
Justification: In a production environment, ensuring that customer inquiries are processed quickly and efficiently is crucial for maintaining customer satisfaction and operational effectiveness. This metric directly measures the application's performance in real-time usage.
Explanation for Wrong Options:


Option 2:

Explanation: Energy usage per query focuses on the operational cost and environmental impact of each query processed by the LLM.
Justification: While important for assessing sustainability and cost efficiency, this metric doesn't directly reflect the application's effectiveness in processing customer inquiries, which is the primary goal in this context.


Option 3:

Explanation: Final perplexity scores are used to measure how well the model was trained, indicating the model's ability to predict text.
Justification: This metric is relevant during the training phase but not for monitoring an already deployed application. It doesn't provide insights into the production performance of the LLM in handling customer inquiries.

Option 4:

Explanation: HuggingFace Leaderboard values are benchmarks for the base LLM's performance on standard datasets.
Justification: These values are useful for comparing different models during development but don't provide relevant information for monitoring the specific performance of the deployed customer service LLM in production.


Summary:
The correct metric to monitor for a customer service LLM application in production is the number of customer inquiries processed per unit of time (Option 1). This metric directly reflects the application's efficiency and capacity to handle customer service tasks. Other options, like energy usage, perplexity scores, and HuggingFace Leaderboard values, are less relevant for monitoring real-time performance in a customer service environment.


References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
4

Skill Section: 
 
Question:  
A Generative AI Engineer is building a Generative AI system that suggests the best matched employee team member to newly scoped projects. 
The team member is selected from a very large team. 
The match should be based upon project date availability and how well their employee profile matches the project scope. 
Both the employee profile and project scope are unstructured text.
How should the Generative Al Engineer architect their system?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Create a tool for finding available team members given project dates. 
Embed all project scopes into a vector store, 
perform a retrieval using team member profiles to find the best team member.

Option 2:
Create a tool for finding team member availability given project dates, and another tool that uses an LLM to extract keywords from project scopes. 
Iterate through available team members’ profiles and perform keyword matching to find the best available team member.


Option 3:
Create a tool to find available team members given project dates. 
Create a second tool that can calculate a similarity score for a combination of team member profile and the project scope. 
Iterate through the team members and rank by best score to select a team member.

Option 4:
Create a tool for finding available team members given project dates. 
Embed team profiles into a vector store and 
use the project scope and filtering to perform retrieval to find the available best matched team members.



Explanation:
Explanation for the Right Option:
Option 4:

Explanation: This approach combines availability filtering with semantic matching using vector embeddings. By embedding team profiles into a vector store, the system can perform efficient and accurate retrieval based on the project scope, ensuring that the selected team member is both available and well-matched to the project's requirements.
Justification: Embedding team profiles allows for semantic understanding of both the employee profiles and project scopes, enabling more nuanced and accurate matches compared to keyword-based or simple similarity scores. This method efficiently narrows down the best candidates from a large team.


Explanation for Wrong Options:
Option 1:

Explanation: This option suggests embedding project scopes and performing retrieval using team member profiles.
Justification: Embedding project scopes instead of team profiles could complicate the retrieval process, as it is more intuitive to embed the constant (team profiles) and query with the varying input (project scopes). This approach might also overlook the importance of availability filtering.

Option 2:

Explanation: This approach uses keyword extraction and matching, which is a more simplistic method of finding relevant profiles.
Justification: Keyword matching lacks the semantic depth of embeddings and might miss nuanced matches. Additionally, iterating through profiles manually after keyword extraction is less efficient and scalable for large teams.

Option 3:

Explanation: This option involves calculating similarity scores by iterating through team members and ranking them.
Justification: While it does consider similarity, this approach could be less efficient than vector retrieval, especially with large datasets. It also doesn't explicitly handle the unstructured nature of the text as well as embedding-based methods.


Summary:
The best architecture for matching team members to projects is to embed team profiles into a vector store and use project scopes for retrieval, combined with filtering for availability (Option 4). 
This method ensures semantic matching and efficient retrieval. 
Other options either rely on simpler keyword-based methods or are less efficient, making them less suitable for handling large teams and complex matching requirements.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
5

Skill Section: 
 
Question:  
A Generative AI Engineer is designing an LLM-powered live sports commentary platform. 
The platform provides real-time updates and LLM-generated analyses for any users who would like to have live summaries, rather than reading a series of potentially outdated news articles.
Which tool below will give the platform access to real-time data for generating game analyses based on the latest game scores?


Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
DatabricksIQ

Option 2:
Foundation Model APIs


Option 3:
Feature Serving

Option 4:
AutoML



Explanation:
Explanation for the Right Option:


Option 3: Feature Serving:

Explanation: Feature Serving is a tool that provides real-time access to the latest data, such as game scores, which can be used to feed live sports commentary and analyses. It ensures that the LLM-powered platform receives the most up-to-date information for generating accurate and timely analyses.
Justification: Real-time data access is crucial for a live sports commentary platform to deliver relevant and immediate updates. Feature Serving is designed to provide this capability by serving fresh data efficiently.


Explanation for Wrong Options:
Option 1: DatabricksIQ

Explanation: DatabricksIQ is more focused on querying and analyzing large datasets rather than providing real-time data feeds.
Justification: While useful for data analytics, it doesn't specifically cater to the need for real-time data, which is essential for live sports commentary.


Option 2: Foundation Model APIs

Explanation: Foundation Model APIs allow interaction with pre-trained models but don't inherently provide real-time data feeds.
Justification: While they can generate text or analyses, they need to be fed data, and these APIs don't specialize in delivering the real-time data required for live commentary.


Option 4: AutoML

Explanation: AutoML focuses on automating the process of building and optimizing machine learning models.
Justification: AutoML is not designed for real-time data access or updates. It helps with model creation and optimization but doesn't address the need for continuous, real-time data streams.


Summary:
The correct tool for providing real-time data for an LLM-powered live sports commentary platform is Feature Serving (Option 3). This tool ensures that the platform has access to the latest game scores, enabling timely and accurate game analyses. Other options either focus on different functionalities or lack the real-time data access necessary for live sports commentary.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
6

Skill Section: 
 
Question:  
A Generative AI Engineer has a provisioned throughput model serving endpoint as part of a RAG application and would like to monitor the serving endpoint’s incoming requests and outgoing responses. The current approach is to include a micro-service in between the endpoint and the user interface to write logs to a remote server.
Which Databricks feature should they use instead which will perform the same task?


Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Vector Search

Option 2:
Lakeview

Option 3:
DBSQL

Option 4:
Inference Tables



Explanation:
Explanation for the Right Option:
Option 4: Inference Tables:

Explanation: Inference Tables in Databricks allow users to monitor and log incoming requests and outgoing responses for machine learning models deployed in production. This feature provides a structured way to track data flow without needing to add an external microservice, simplifying the architecture.
Justification: Inference Tables are specifically designed to track the performance and requests/responses of deployed models, making them the best fit for the task of logging and monitoring requests and responses in a serving endpoint.


Explanation for Wrong Options:
Option 1: Vector Search

Explanation: Vector Search is used for retrieving and searching through embeddings in a vector database.
Justification: While useful for retrieval in a RAG application, it does not handle logging or monitoring of requests and responses.

Option 2: Lakeview

Explanation: Lakeview is a feature in Databricks that provides a UI for exploring data in a Delta Lake.
Justification: Lakeview focuses on data exploration and visualization, not monitoring or logging serving endpoint activity.


Option 3: DBSQL

Explanation: DBSQL (Databricks SQL) is used for running SQL queries on data stored in Databricks.
Justification: While DBSQL is powerful for querying data, it doesn't provide built-in functionality for monitoring incoming requests and outgoing responses for model endpoints.


Summary:
The best Databricks feature for monitoring and logging incoming requests and outgoing responses for a serving endpoint in a RAG application is Inference Tables (Option 4). This feature is designed specifically for tracking inference data, making it a better alternative to adding a microservice. Other options focus on different aspects of the data pipeline and do not cater to logging or monitoring endpoint activity.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
7

Skill Section: 
 
Question:  
A Generative Al Engineer is tasked with improving the RAG quality by addressing its inflammatory outputs.
Which action would be most effective in mitigating the problem of offensive text outputs?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Increase the frequency of upstream data updates

Option 2:
Inform the user of the expected RAG behavior


Option 3:
Restrict access to the data sources to a limited number of users

Option 4:
Curate upstream data properly that includes manual review before it is fed into the RAG system



Explanation:
Explanation for the Right Option:
Option 4: Curate upstream data properly that includes manual review before it is fed into the RAG system

Explanation: Proper curation of upstream data, including manual review, ensures that the data being fed into the RAG system does not contain offensive or inflammatory content. This action directly addresses the root cause by preventing harmful content from being part of the input data in the first place.
Justification: This is the most effective and proactive measure. Ensuring that only clean, vetted data enters the system will significantly reduce the risk of generating offensive outputs. It also prevents any unintended harm from being perpetuated by the model.


Explanation for Wrong Options:

Option 1: Increase the frequency of upstream data updates

Explanation: Increasing the frequency of data updates could ensure the system uses the most current information, but it doesn’t address the quality of the data itself.
Justification: If the data being updated is still inflammatory or offensive, increasing the update frequency will only serve to propagate the same issues more quickly, rather than resolving them.


Option 2: Inform the user of the expected RAG behavior

Explanation: While informing users about expected behavior might set expectations for the outputs, it doesn't directly mitigate or prevent the occurrence of offensive content.
Justification: It is a passive approach that focuses on managing user expectations, not actively addressing the quality or appropriateness of the content being generated.


Option 3: Restrict access to the data sources to a limited number of users

Explanation: Restricting access to data sources may limit the amount of data used but doesn't solve the issue of inflammatory content within the data.
Justification: Restricting access doesn't ensure that the data being used is clean or that offensive content is not included. This measure is more about limiting usage rather than improving data quality.

Summary:
To effectively mitigate the problem of offensive text outputs in a RAG system, curating upstream data properly with manual review (Option 4) is the most effective approach. This ensures that only high-quality, non-inflammatory data is fed into the system, directly addressing the problem. Other options either focus on mitigating the impact without addressing the root cause or do not prevent the inclusion of offensive data in the system.


References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
8

Skill Section: 
 
Question:  
A Generative AI Engineer is creating an LLM-based application. 
The documents for its retriever have been chunked to a maximum of 512 tokens each. 
The Generative Al Engineer knows that cost and latency are more important than quality for this application. 
They have several context length levels to choose from.
Which will fulfill their need?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
E

Guideline Time in Seconds (30, 45, 60):
30

Option 1:
Context length 514; smallest model is 0.44GB and embedding dimension 768

Option 2:
Context length 2048: smallest model is 11GB and embedding dimension 2560

Option 3:
Context length 32768: smallest model is 14GB and embedding dimension 4096

Option 4:
Context length 512: smallest model is 0.13GB and embedding dimension 384


Explanation:
Explanation for the Right Option:

Option 4: Context length 512: smallest model is 0.13GB and embedding dimension 384

Explanation: The Generative AI Engineer is focused on cost and latency over quality. A context length of 512 tokens is aligned with the chunking size of the documents, and the model size (0.13GB) and embedding dimension (384) are small, ensuring low latency and reduced costs.
Justification: This option balances the need for efficiency (lower memory usage, smaller model) while keeping the context length in line with the document chunking. The smaller model size and token context are ideal for a use case where cost and latency are the primary concerns.


Explanation for Wrong Options:
Option 1:

Explanation: A context length of 514 tokens is slightly larger than the chunk size, and the model is 0.44GB with embedding dimensions of 768.
Justification: While the context length is very close to the chunk size, the model size is significantly larger than Option 4, which increases cost and latency, not aligning with the engineer's needs for low cost and low latency.

Option 2:

Explanation: A context length of 2048 tokens with a model size of 11GB and embedding dimensions of 2560.
Justification: This option is too large for the application where cost and latency are more important. A model of this size will incur high costs and slower processing times, which is the opposite of the engineer's goal.

Option 3:

Explanation: A context length of 32768 tokens with a model size of 14GB and embedding dimensions of 4096.
Justification: This is an extremely large context and model size, which would result in both high costs and very high latency, making it unsuitable for an application where cost and latency need to be minimized.

Summary:
For an application where cost and latency are prioritized over quality, Option 4 is the best choice. 
It aligns the context length with the chunk size (512 tokens) and offers the smallest model size (0.13GB) to ensure efficiency. 
The other options either have excessively large context lengths or model sizes that would increase costs and latency.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
9

Skill Section: 
 
Question:  
A small and cost-conscious startup in the cancer research field wants to build a RAG application using Foundation Model APIs.
Which strategy would allow the startup to build a good-quality RAG application while being cost-conscious and able to cater to customer needs?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Limit the number of relevant documents available for the RAG application to retrieve from

Option 2:
Pick a smaller LLM that is domain-specific

Option 3:
Limit the number of queries a customer can send per day

Option 4:
Use the largest LLM possible because that gives the best performance for any general queries




Explanation:
Explanation for the Right Option:
Option 2: Pick a smaller LLM that is domain-specific

Explanation: By selecting a smaller, domain-specific LLM, the startup can balance cost and performance effectively. A smaller model will be more cost-effective to run, and a domain-specific model (tailored to cancer research) will enhance the relevance and accuracy of the generated responses. This ensures good quality outputs without the need for the resource-intensive, large general models.
Justification: A domain-specific model is optimized for the type of queries the startup is expecting, which means it can provide better performance for cancer research-related queries, even with fewer resources. This approach allows the startup to build a high-quality RAG application without incurring excessive costs.


Explanation for Wrong Options:
Option 1:

Explanation: Limiting the number of relevant documents could reduce costs, but it risks compromising the quality and comprehensiveness of the information available for generating responses.
Justification: The goal of a RAG system is to provide relevant and accurate information by leveraging a wide range of documents. Limiting this would make the system less effective, especially in a field as complex as cancer research.

Option 3:

Explanation: Limiting the number of queries per customer is a cost-control mechanism, but it might lead to frustration for users and does not necessarily ensure that the system provides high-quality responses.
Justification: The quality of the responses is more important than the quantity of queries. While limiting queries might help reduce costs, it doesn't directly address the quality of the application or the domain-specific needs of cancer research.


Option 4:

Explanation: Using the largest LLM might provide the best general performance but would be highly expensive and unnecessary for a domain-specific application.
Justification: Large models are costly to run and often not optimized for niche areas like cancer research. A smaller, more focused model will perform better for the specific use case while being much more cost-effective.


Summary:
For a cost-conscious startup in cancer research, Option 2 (picking a smaller, domain-specific LLM) is the best strategy. 
It provides a balance between cost and quality, ensuring that the application is efficient while still offering high-quality, relevant responses for the specific domain. The other options either compromise on quality or focus too much on cost-saving without considering the need for relevant, accurate outputs.


References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
10

Skill Section: 
 
Question:  
A Generative AI Engineer is responsible for developing a chatbot to enable their company’s internal HelpDesk Call Center team to more quickly find related tickets and provide resolution. While creating the GenAI application work breakdown tasks for this project, they realize they need to start planning which data sources (either Unity Catalog volume or Delta table) they could choose for this application. 

They have collected several candidate data sources for consideration: 

call_rep_history: a Delta table with primary keys representative_id, call_id. 
This table is maintained to calculate representatives’ call resolution from fields call_duration and call start_time. 


transcript Volume: a Unity Catalog Volume of all recordings as a *.wav files, but also a text transcript as *.txt files. 

call_cust_history: a Delta table with primary keys customer_id, cal1_id. 
This table is maintained to calculate how much internal customers use the HelpDesk to make sure that the charge back model is consistent with actual service use. 


call_detail: a Delta table that includes a snapshot of all call details updated hourly. 
It includes root_cause and resolution fields, but those fields may be empty for calls that are still active. 

maintenance_schedule :  a Delta table that includes a listing of both HelpDesk application outages as well as planned upcoming maintenance downtimes.


They need sources that could add context to best identify ticket root cause and resolution.
Which TWO sources do that? (Choose two.)

Answer: 
4 and 5

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
call_cust_history

Option 2:
maintenance_schedule


Option 3:
call_rep_history


Option 4:
call_detail


Option 5:
transcript Volume


Explanation:

Explanation for the Right Option:
Option 4: call_detail

Explanation: The call_detail table includes root cause and resolution fields, which directly relate to identifying the root cause and resolution of a ticket. Even though some fields might be empty for active calls, it’s still the most relevant source for resolving tickets and understanding the cause of an issue.
Justification: This table contains the most direct information related to ticket resolution and root causes, which is essential for identifying and resolving issues effectively in a HelpDesk system.

Option 5:
transcript Volume : The transcript text files provide detailed call information that can be analyzed for insights into the root cause of issues, customer queries, and resolutions discussed during calls.



Explanation for Wrong Options:
Option 1: call_cust_history

Explanation: The call_cust_history table tracks customer use of the HelpDesk, which is useful for chargeback models but does not directly help in identifying the root cause or resolution of specific tickets.
Justification: While it tracks how often customers use the service, it doesn’t provide relevant details about the causes or resolutions of the tickets themselves, making it less relevant for identifying root causes and resolutions.


Option 2: maintenance_schedule

Explanation: Although maintenance_schedule provides information about HelpDesk application downtimes, it doesn't directly provide ticket-specific data such as root causes or resolutions of tickets. It’s useful for adding context but not the primary source for root cause analysis.
Justification: Maintenance data can be useful for understanding when issues are related to system outages but doesn't provide direct insight into the individual root causes of tickets. It's better suited for adding context than being a primary data source for identifying root causes.

Option 3: call_rep_history

Explanation: The call_rep_history table tracks call duration and call start time but lacks specific information related to root cause and resolution. It focuses more on representative performance rather than the content or outcome of the calls.
Justification: This table is useful for understanding representative activity but does not provide details about the root cause or resolution of the tickets, making it less relevant for identifying ticket-specific issues.




References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
11

Skill Section: 
 
Question:  
What is the most suitable library for building a multi-step LLM-based workflow?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Pandas

Option 2:
TensorFlow

Option 3:
PySpark

Option 4:
LangChain



Explanation:
Explanation for the Right Option:
Option 4: LangChain

Explanation: LangChain is a specialized library designed for building multi-step workflows with Large Language Models (LLMs). It provides utilities to connect LLMs with other components like APIs, databases, or external tools, and to orchestrate multi-step processes or chains of reasoning. LangChain is built specifically for these types of workflows, making it the most suitable library for building complex, multi-step LLM-based applications.
Justification: LangChain excels at managing and chaining multiple LLM invocations together and integrating external systems into the workflow. It offers tools for prompt chaining, memory handling, and integrating with APIs, making it the optimal choice for multi-step workflows.


Explanation for Wrong Options:
Option 1: Pandas

Explanation: Pandas is a popular library for data manipulation and analysis. While it can be useful for handling structured data, it does not offer specialized features for building or managing LLM workflows.
Justification: Pandas is not designed for building workflows around LLMs or managing multi-step processes involving language models.

Option 2: TensorFlow

Explanation: TensorFlow is a powerful machine learning library primarily used for training and deploying models, including neural networks. However, it is not specifically designed for orchestrating multi-step workflows involving LLMs.
Justification: While TensorFlow can be used for machine learning tasks, it doesn't focus on managing or chaining LLM-based workflows, which is the core need for this use case.


Option 3: PySpark

Explanation: PySpark is a library for distributed data processing using Apache Spark. It is typically used for big data workloads and parallel processing. While it may be useful for processing large datasets, it isn't focused on managing multi-step LLM workflows.
Justification: PySpark is not geared toward handling LLM chains or workflows; it focuses on distributed computation and data processing at scale, making it less suitable for this scenario.


Summary:
The most suitable library for building a multi-step LLM-based workflow is LangChain (Option 4). It is specifically designed to work with LLMs and manage multi-step processes, making it the best fit for this task. The other options either focus on data manipulation, machine learning, or distributed computation, none of which are tailored for orchestrating LLM workflows.
References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
12

Skill Section: 
 
Question:  
When developing an LLM application, it’s crucial to ensure that the data used for training the model complies with licensing requirements to avoid legal risks. Which action is NOT appropriate to avoid legal risks?


Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Reach out to the data curators directly before you have started using the trained model to let them know.

Option 2:
Use any available data you personally created which is completely original and you can decide what license to use.

Option 3:
Only use data explicitly labeled with an open license and ensure the license terms are followed.

Option 4:
Reach out to the data curators directly after you have started using the trained model to let them know.


Explanation:
Explanation for the Right Option:
Option 2: Use any available data you personally created which is completely original and you can decide what license to use.

Explanation: While it might seem like using your personally created data is safe, the issue is that just because the data is original, it doesn't automatically mean you have the legal right to use or license it however you wish. The data might be subject to other constraints, such as contractual obligations, privacy laws, or proprietary information. Simply assuming full control over the licensing could lead to legal risks, especially if the data was created in a specific context that restricts its usage.
Justification: Even if the data is created by you, there may still be legal implications regarding its use, particularly if it involves sensitive information or is governed by external agreements. Using data without understanding potential legal restrictions can lead to legal issues, so it’s important to check these aspects before using it in a model.


Explanation for Wrong Options:
Option 1: Reach out to the data curators directly before you have started using the trained model to let them know.

Explanation: This is an appropriate action. Reaching out to the data curators to confirm the licensing terms before using the data ensures that you understand and follow the appropriate legal requirements. It's proactive and ensures you're not inadvertently violating any agreements or licenses.
Justification: Communicating with data curators beforehand is the responsible way to ensure the legality of your data usage.


Option 3: Only use data explicitly labeled with an open license and ensure the license terms are followed.

Explanation: This is the correct approach. Using data with explicit open licenses (like Creative Commons or public domain) and following the license terms ensures legal compliance.
Justification: Open-license data often comes with clear terms, which makes it easier to comply with licensing requirements and avoid legal risks.


Option 4: Reach out to the data curators directly after you have started using the trained model to let them know.

Explanation: While this action is somewhat acceptable, it's not ideal. It's better to reach out to the data curators before you start using the data in your model, as once the model has been trained, it could already be too late to address licensing concerns.
Justification: It is best to address licensing concerns upfront, before using the data, to avoid legal complications later on.


Summary:
The action that is NOT appropriate to avoid legal risks is Option 2 (using any available data you personally created without considering external legal constraints). Even if the data is original, it could still have legal restrictions. The other options either proactively engage with data curators or ensure the data has open licenses, making them suitable for ensuring legal compliance.

References:

Hands-On: 


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
13

Skill Section: 
 
Question:  
A Generative AI Engineer is testing a simple prompt template in LangChain using the code below, but is getting an error.

from langchain.chains import LLMChain
from langchain_community.llms import OpenAI
from langchain_core.prompts import PromptTemplate

prompt_template = "Tell me a {adjective} joke"

prompt = PromptTemplate(
    input_variables=["adjective"],
    template=prompt_template
)

llm = LLMChain(prompt=prompt)
llm.generate([{"adjective": "funny"}])


Assuming the API key was properly defined, what change does the Generative AI Engineer need to make to fix their chain?



Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
prompt_template = "Tell me a {adjective} joke"

prompt = PromptTemplate(
    input_variables=["adjective"],
    template=prompt_template
)

llm = LLMChain(prompt=prompt)
llm.generate("funny")


Option 2:
prompt_template = "Tell me a {adjective} joke"

prompt = PromptTemplate(
    input_variables=["adjective"],
    template=prompt_template
)

llm = LLMChain(prompt=prompt.format("funny"))
llm.generate()



Option 3:

prompt_template = "Tell me a {adjective} joke"

prompt = PromptTemplate(
    input_variables=["adjective"],
    template=prompt_template
)

llm=OpenAI()
llm = LLMChain(prompt=prompt)
llm.generate([{"adjective": "funny"}])


Option 4:
prompt_template = "Tell me a {adjective} joke"

prompt = PromptTemplate(
    input_variables=["adjective"],
    template=prompt_template
)

llm = LLMChain(llm=OpenAI(),prompt=prompt)
llm.generate([{"adjective": "funny"}])




Explanation:
Explanation for the Right Option:
Option 3:

prompt_template = "Tell me a {adjective} joke"

prompt = PromptTemplate(
    input_variables=["adjective"],
    template=prompt_template
)

llm = OpenAI()
llm_chain = LLMChain(prompt=prompt, llm=llm)
llm_chain.generate([{"adjective": "funny"}])


Explanation: In this option, the error is fixed by properly initializing the OpenAI model and passing it as a parameter when constructing the LLMChain object. The missing component in the original code is the LLM model (OpenAI in this case) being passed to the chain.
Justification: The LLMChain requires both a prompt and an LLM to function. In the original code, the OpenAI LLM was not provided to the LLMChain object. This solution properly initializes the LLM and passes it to the chain.


Explanation for Wrong Options:
Option 1:

python
Copy
Edit
prompt_template = "Tell me a {adjective} joke"

prompt = PromptTemplate(
    input_variables=["adjective"],
    template=prompt_template
)

llm = LLMChain(prompt=prompt)
llm.generate("funny")
Explanation: The code attempts to pass the string "funny" directly to the generate function, but this function expects a list of dictionaries, not a single string.
Justification: The input needs to be in the form of a dictionary that maps the variable name to its value. Thus, this will result in a mismatch error.


Option 2:

prompt_template = "Tell me a {adjective} joke"

prompt = PromptTemplate(
    input_variables=["adjective"],
    template=prompt_template
)

llm = LLMChain(prompt=prompt.format("funny"))
llm.generate()
Explanation: The .format() method is being applied to the PromptTemplate object, which is not appropriate. The PromptTemplate is supposed to remain a template, and the values should be passed as part of the generate() call, not in .format().
Justification: The error arises because .format() is used incorrectly. The template needs to stay as a format and be handled by the chain logic, not manually formatted.


Option 4:
prompt_template = "Tell me a {adjective} joke"

prompt = PromptTemplate(
    input_variables=["adjective"],
    template=prompt_template
)

llm = LLMChain(llm=OpenAI(), prompt=prompt)
llm.generate([{"adjective": "funny"}])
Explanation: This option looks correct, except that it tries to pass the llm argument in the wrong order. LLMChain expects the llm and prompt to be passed in the correct sequence in the constructor.
Justification: The LLMChain constructor expects the prompt to be specified as an argument directly and not after llm. Reversing the argument order might cause issues.

Summary:
The correct solution is Option 3, which initializes the OpenAI LLM properly and passes it to the LLMChain object alongside the PromptTemplate. The other options either misuse the argument order or incorrectly handle the template formatting or input.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
14

Skill Section: 
 
Question:  
A Generative AI Engineer is creating an LLM system that will retrieve news articles from the year 1918 and related to a user's query and summarize them. 
The engineer has noticed that the summaries are generated well but often also include an explanation of how the summary was generated, which is undesirable.
Which change could the Generative AI Engineer perform to mitigate this issue?


Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Split the LLM output by newline characters to truncate away the summarization explanation.

Option 2:
Tune the chunk size of news articles or experiment with different embedding models.

Option 3:
Revisit their document ingestion logic, ensuring that the news articles are being ingested properly.

Option 4:
Provide few-shot examples of desired output format to the system and/or user prompt.


Explanation:
Explanation for the Right Option:
Option 4: Provide few-shot examples of desired output format to the system and/or user prompt.

Explanation: One effective way to guide the LLM to generate only the desired content (e.g., a summary without explanations) is by providing few-shot examples. By including examples that explicitly show the expected output format—such as summaries without explanations—the model will better understand the task and produce output in the same style.
Justification: Few-shot learning helps LLMs understand the specific requirements of the task, such as avoiding unnecessary content like explanations about how the summary was generated. This method allows the system to "learn" the proper formatting through examples.


Explanation for Wrong Options:
Option 1: Split the LLM output by newline characters to truncate away the summarization explanation.

Explanation: Truncating the output based on newline characters is a rudimentary approach that would not be reliable in eliminating unwanted explanations. This method is not adaptive and could lead to cutting off important information or summaries.
Justification: This approach is error-prone and does not address the root cause of the issue, which is the LLM’s tendency to add explanations. It’s a mechanical solution, not a well-informed one.


Option 2: Tune the chunk size of news articles or experiment with different embedding models.

Explanation: While tuning chunk size and experimenting with embeddings can improve the retrieval and relevance of documents, it does not directly solve the issue of summarization format or the inclusion of explanations.
Justification: This approach focuses on optimizing the retrieval process rather than refining the model’s output generation. It may improve the quality of the summaries but will not address the specific problem of unnecessary explanation generation.


Option 3: Revisit their document ingestion logic, ensuring that the news articles are being ingested properly.

Explanation: Ensuring proper ingestion of documents is important, but it does not specifically solve the problem of unwanted summarization explanations. The problem is related to the model’s output formatting rather than how the articles are ingested.
Justification: This action may help with the accuracy of retrieval or summarization but does not address the issue of the model adding explanations in the summary.


Summary:
The most suitable approach is Option 4, where the engineer provides few-shot examples of the desired output format to help the model understand that it should only generate summaries and not explanations of how the summary was created. This is the most direct and effective way to guide the model's behavior. The other options either focus on unrelated issues or employ inefficient methods.


References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
15

Skill Section: 
 
Question:  
A Generative AI Engineer has developed an LLM application to answer questions about internal company policies. 
The Generative AI Engineer must ensure that the application doesn’t hallucinate or leak confidential data.
Which approach should NOT be used to mitigate hallucination or confidential data leakage?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):

Guideline Time in Seconds (30, 45, 60):

Option 1:
Add guardrails to filter outputs from the LLM before it is shown to the user

Option 2:
Fine-tune the model on your data, hoping it will learn what is appropriate and not


Option 3:
Limit the data available based on the user’s access level


Option 4:
Use a strong system prompt to ensure the model aligns with your needs.



Explanation:
Explanation for the Right Option:
Option 2: Fine-tune the model on your data, hoping it will learn what is appropriate and not.

Explanation: Fine-tuning an LLM with your data could indeed help the model better understand your company's policies and context. However, fine-tuning alone is not sufficient to prevent hallucinations or confidential data leakage. In fact, fine-tuning without rigorous safeguards might exacerbate the problem, as the model may learn to generate plausible-sounding but incorrect responses or inadvertently leak confidential information.
Justification: Fine-tuning does not guarantee that the model will reliably avoid hallucinations or data leakage, especially if the data provided during fine-tuning is not sufficiently curated or if there are no external safeguards in place.


Explanation for Wrong Options:
Option 1: Add guardrails to filter outputs from the LLM before it is shown to the user.

Explanation: Adding guardrails, such as a filtering mechanism that screens outputs for sensitive content or hallucinated information, is a very effective way to mitigate both hallucinations and confidential data leakage.
Justification: This is a proactive approach that allows for dynamic filtering of responses, ensuring that only safe and appropriate information is shared with users. It’s an essential safeguard for maintaining control over the model’s output.


Option 3: Limit the data available based on the user’s access level.

Explanation: Limiting access to sensitive data based on user roles or permissions is a key strategy to prevent unauthorized access to confidential information and reduce the risk of data leakage.
Justification: By restricting what information the model can retrieve or present based on the user's access level, the company can effectively ensure that confidential data is not exposed to unauthorized individuals.


Option 4: Use a strong system prompt to ensure the model aligns with your needs.

Explanation: A strong system prompt can help guide the model’s behavior by setting clear expectations and constraints for its responses. It is a highly effective way to align the model with specific requirements, such as avoiding hallucination and adhering to confidentiality.
Justification: By explicitly instructing the model on the expected behavior and limitations through a system prompt, the engineer can better control the model's responses and ensure compliance with the company's policies.


Summary:
The correct answer is Option 2, as fine-tuning the model alone is not enough to prevent hallucinations or confidential data leakage. It needs to be complemented with other strategies like filtering outputs, limiting data access and using system prompts. The other options offer effective, preventive measures to safeguard the system.


References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
16

Skill Section: 
 
Question:  
A Generative Al Engineer interfaces with an LLM with prompt/response behavior that has been trained on customer calls inquiring about product availability. The LLM is designed to output “In Stock” if the product is available or only the term “Out of Stock” if not.
Which prompt will work to allow the engineer to respond to call classification labels correctly?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Respond with “In Stock” if the customer asks for a product.

Option 2:
You will be given a customer call transcript where the customer asks about product availability. 
The outputs are either “In Stock” or “Out of Stock”. 
Format the output in JSON, for example: {“call_id”: “123”, “label”: “In Stock”}.


Option 3:
Respond with “Out of Stock” if the customer asks for a product.


Option 4:
You will be given a customer call transcript where the customer inquires about product availability. 
Respond with “In Stock” if the product is available or “Out of Stock” if not.


Explanation:
Explanation for the Right Option:
Option 2: You will be given a customer call transcript where the customer asks about product availability. The outputs are either “In Stock” or “Out of Stock”. Format the output in JSON, for example: {“call_id”: “123”, “label”: “In Stock”}.

Explanation: This prompt format provides specific instructions for the output format (in JSON), which includes both the product availability status and an associated call ID. It helps the model generate a structured output that includes all necessary information, making it easy to classify and process.
Justification: This approach ensures the output is not only correct but also structured in a way that allows for easy integration with downstream systems, especially for classification tasks where structured data is required for further processing.


Explanation for Wrong Options:
Option 1: Respond with “In Stock” if the customer asks for a product.

Explanation: This prompt is overly simplistic and would only produce a fixed response (“In Stock”) without considering whether the product is actually available. It does not allow for the necessary flexibility to handle both “In Stock” and “Out of Stock” responses based on the product’s availability.
Justification: It doesn't properly address the call classification task, where both potential responses (“In Stock” or “Out of Stock”) are needed. It would fail in cases where the product is not available.


Option 3: Respond with “Out of Stock” if the customer asks for a product.

Explanation: Similar to Option 1, this prompt restricts the model to always output “Out of Stock,” regardless of the actual product availability. This would lead to incorrect responses when a product is in fact available.
Justification: It introduces the same problem as Option 1, where the model would only generate one fixed output ("Out of Stock") instead of dynamically responding based on the product's actual availability.


Option 4: You will be given a customer call transcript where the customer inquires about product availability. Respond with “In Stock” if the product is available or “Out of Stock” if not.

Explanation: While this option correctly instructs the model to respond with “In Stock” or “Out of Stock,” it lacks the structured format required for proper classification (e.g., JSON output with a call ID). This makes it less suitable for integration into systems that need structured, machine-readable responses.
Justification: The absence of a structured output (like JSON) limits its utility in systems that require systematic data extraction, such as for classification or further processing.


Summary:
The most appropriate prompt is Option 2, as it specifies both the required content (“In Stock” or “Out of Stock”) and the correct output format (JSON), making it optimal for the engineer to classify calls correctly. The other options lack necessary structure or would lead to incorrect responses.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
17

Skill Section: 
 
Question:  
A Generative AI Engineer is tasked with developing a RAG application that will help a small internal group of experts at their company answer specific questions, augmented by an internal knowledge base. They want the best possible quality in the answers and neither latency nor throughput is a huge concern given that the user group is small and they’re willing to wait for the best answer. The topics are sensitive in nature and the data is highly confidential and so, due to regulatory requirements, none of the information is allowed to be transmitted to third parties.
Which model meets all the Generative AI Engineer’s needs in this situation?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
I

Guideline Time in Seconds (30, 45, 60):
60

Option 1:
Dolly 1.5B

Option 2:
OpenAI GPT-4

Option 3:
BGE-large

Option 4:
Llama2-70B


Explanation:
Explanation for the Right Option:
Option 4: Llama2-70B

Explanation: Llama2-70B is a large, state-of-the-art language model developed by Meta. It is known for its high performance and capabilities in terms of generating high-quality, context-aware responses. Importantly, it can be deployed in environments that do not require third-party data transmission, making it ideal for use with highly sensitive and confidential information, especially in a closed, internal setting.
Justification: Llama2-70B meets the requirements for the best quality answers and ensures that sensitive data is not sent to third parties because it can be deployed on-premise or in a private cloud environment. Additionally, it offers the model capacity necessary to handle complex questions and augment answers with the internal knowledge base.


Explanation for Wrong Options:
Option 1: Dolly 1.5B

Explanation: Dolly 1.5B is a smaller model with fewer parameters and is less capable in comparison to models like Llama2-70B. While it can be used for various tasks, it is less likely to provide the high-quality answers required in this scenario, especially given the complex and confidential nature of the information.
Justification: Dolly 1.5B may not generate the best possible quality answers and could lack the capacity to handle more intricate queries that require deeper reasoning.


Option 2: OpenAI GPT-4

Explanation: While GPT-4 is an excellent model known for its language generation capabilities, it typically requires cloud access, which can be a concern for situations involving highly confidential or regulated data that cannot be transmitted to third parties.
Justification: OpenAI GPT-4 is not suitable for scenarios where data privacy and regulatory compliance are critical, as it involves external servers and third-party data handling.


Option 3: BGE-large

Explanation: BGE-large is a specialized language model focused on generative tasks. It is powerful, but not as widely recognized for the same level of performance and flexibility as Llama2-70B, particularly in terms of quality of answers and adaptability to internal, sensitive environments.
Justification: While BGE-large may handle specific tasks well, it does not meet the broad requirements for high-quality, confidential use cases as effectively as Llama2-70B, especially in a highly sensitive context.


Summary:
The most appropriate model for the Generative AI Engineer’s needs is Option 4: Llama2-70B, as it provides the best quality answers, can be used in an internal, secure setting without third-party data transmission, and supports handling sensitive information. The other models either compromise on quality or involve third-party interactions that are not suitable for this highly regulated environment.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
18

Skill Section: 
 
Question:  
A Generative AI Engineer would like an LLM to generate formatted JSON from emails. 
This will require parsing and extracting the following information: order ID, date, and sender email. 

Here’s a sample email:

Date: April 23, 2024
Time: 4:22 PM
From: anjali.thayer@computex.org
To: cust_service@realtek.com
Subject: Shipment details

Hey there,

I have a shipment (order ID is CD34RFT) can you please send me an update?

Thank you,
Anjali


They will need to write a prompt that will extract the relevant information in JSON format with the highest level of output accuracy.
Which prompt will do that?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
You will receive customer emails and need to extract date, sender email, and order ID. You should return the date, sender email, and order ID information in JSON format.

Option 2:
You will receive customer emails and need to extract date, sender email, and order ID. 
Return the extracted information in JSON format.
Here’s an example: {“date”: “April 16, 2024”, “sender_email”: “sarah.lee925@gmail.com”, “order_id”: “RE987D”}


Option 3:
You will receive customer emails and need to extract date, sender email, and order ID. Return the extracted information in a human-readable format.


Option 4:
You will receive customer emails and need to extract date, sender email, and order ID. Return the extracted information in JSON format.




Explanation:
Explanation for the Right Option:
Option 2:

Explanation: This option provides the clearest, most structured prompt for extracting the relevant information (date, sender email, and order ID) from customer emails in the desired JSON format. By providing an example of the expected JSON output, it sets a precise expectation for the model, helping it understand exactly how to structure the response for maximum accuracy.
Justification: The example in the prompt acts as a template, allowing the LLM to follow it strictly and ensure high accuracy in output. The model will understand not just what to extract but how to format it.


Explanation for Wrong Options:
Option 1:

Explanation: This option instructs the LLM to return the extracted information in JSON format but lacks a clear example of what the JSON output should look like.
Justification: While it provides clear instructions, not including an example makes the task less explicit, which could lead to less accurate results as the model might not be entirely sure of the exact format.


Option 3:

Explanation: This prompt requests a "human-readable format" for the extracted data, which contradicts the requirement to produce JSON output.
Justification: This option does not fulfill the requirement of returning the extracted information in JSON format, which is necessary for the LLM to process the data in a structured and machine-readable way.


Option 4:

Explanation: This option lacks an example and a more specific instruction for the model, which makes it less effective than Option 2 in ensuring the desired output format.
Justification: Without an example or clearer guidance, there is ambiguity in how the extracted data should be structured in the JSON response, leading to potentially less accurate results.


Summary:
The most suitable prompt for the Generative AI Engineer is Option 2, as it provides both the necessary instructions and a concrete example, ensuring the LLM generates the output in the correct JSON format with the highest accuracy.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
19

Skill Section: 
 
Question:  
A Generative AI Engineer has been asked to build an LLM-based question-answering application. 
The application should take into account new documents that are frequently published. 
The engineer wants to build this application with the least cost and least development effort and have it operate at the lowest cost possible.
Which combination of chaining components and configuration meets these requirements?

Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
For the application a prompt, a retriever, and an LLM are required. 
The retriever output is inserted into the prompt which is given to the LLM to generate answers.

Option 2:
The LLM needs to be frequently with the new documents in order to provide most up-to-date answers.


Option 3:
For the question-answering application, prompt engineering and an LLM are required to generate answers.


Option 4:
For the application a prompt, an agent and a fine-tuned LLM are required. 
The agent is used by the LLM to retrieve relevant content that is inserted into the prompt which is given to the LLM to generate answers.



Explanation:
Explanation for the Right Option:
Option 1:

Explanation: This option correctly identifies a simple and efficient architecture for the LLM-based question-answering application. The combination of a retriever, a prompt, and an LLM is a cost-effective and low-effort approach. The retriever will handle the task of fetching the most relevant documents based on user queries, the retrieved documents will be used as input to the prompt, and the LLM will then generate the final answers based on the prompt.
Justification: This solution is both cost-efficient and easy to implement. The retriever can be updated to handle new documents frequently, minimizing the need for frequent retraining of the LLM and reducing both cost and development effort.


Explanation for Wrong Options:
Option 2:

Explanation: This option suggests that the LLM needs to be frequently updated with new documents. While keeping the model up to date with the latest documents is important, this approach would require continuous retraining, which would increase both costs and development effort.
Justification: Updating the LLM frequently is not a scalable or cost-effective solution, as it would involve frequent retraining and redeployment of the model.


Option 3:

Explanation: This option only mentions prompt engineering and an LLM, but it omits the crucial step of retrieving the relevant documents. This makes the approach less efficient, especially when the system needs to handle a large number of new documents frequently.
Justification: Without a retriever, the LLM would need to handle the entire document corpus, which is inefficient and costly. The retriever is essential for narrowing down the scope to only the most relevant documents.


Option 4:

Explanation: This option introduces an agent and a fine-tuned LLM, which would require more development effort and cost compared to using a simple retriever and prompt setup. Fine-tuning the LLM regularly would also incur high operational costs.
Justification: While this setup may improve performance, it is unnecessarily complex for the given requirement, and the use of a fine-tuned LLM adds 
additional costs and effort that could be avoided.

Summary:
Option 1 is the most suitable choice for building a question-answering application with frequent updates to documents. 
It ensures low cost and development effort while maintaining efficiency by using a retriever to select relevant documents and a prompt to guide the LLM.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
20

Skill Section: 
 
Question:  
A Generative AI Engineer is creating an agent-based LLM system for their favorite monster truck team. 
The system can answer text based questions about the monster truck team, lookup event dates via an API call, or query tables on the team’s latest standings.
How could the Generative AI Engineer best design these capabilities into their system?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Ingest PDF documents about the monster truck team into a vector store and query it in a RAG architecture.

Option 2:
Write a system prompt for the agent listing available tools and bundle it into an agent system that runs a number of calls to solve a query.


Option 3:
Instruct the LLM to respond with “RAG”, “API”, or “TABLE” depending on the query, then use text parsing and conditional statements to resolve the query.


Option 4:
Build a system prompt with all possible event dates and table information in the system prompt. 
Use a RAG architecture to lookup generic text questions and otherwise leverage the information in the system prompt.


Explanation:
Explanation for the Right Option:
Option 2:

Explanation: This option suggests writing a system prompt that defines the available tools (e.g., API calls, table queries) and integrating them into an agent-based architecture. This would allow the system to dynamically choose the appropriate tool to answer questions based on the user's query. By bundling these tools into an agent, the system can intelligently decide whether to pull information from the API, query tables, or rely on other available resources.
Justification: This approach is scalable, flexible, and provides the best capability to handle varied queries. The system prompt helps in defining the structure of available tools, and the agent can invoke the right tool as per the context, making the design both efficient and capable of handling multiple functionalities.

Explanation for Wrong Options:
Option 1:

Explanation: This option suggests ingesting PDF documents into a vector store for use in a RAG architecture, which is suitable for handling static text documents but doesn’t address the dynamic nature of querying APIs or tables. In this case, where real-time data like event dates and standings are involved, a static document-based approach (RAG) would not be sufficient.
Justification: This approach lacks flexibility in accessing external resources like APIs and real-time data, limiting its ability to answer dynamic queries effectively.

Option 3:

Explanation: This option suggests having the LLM choose between "RAG", "API", or "TABLE" based on the query and using text parsing to handle the response. While this method introduces conditional logic, it is overly simplistic and does not integrate the necessary dynamic decision-making process between the tools. Additionally, text parsing can lead to complications and errors in more complex queries.
Justification: This approach is less efficient than using an agent system that can intelligently decide which tool to use, resulting in more complexity and potential inaccuracies in handling varied queries.

Option 4:

Explanation: This option suggests hardcoding all event dates and table information directly into the system prompt. While this might work for a limited set of information, it’s not scalable, as the system will quickly become outdated with frequent changes (e.g., event dates or standings). It also doesn’t allow the system to handle dynamic queries or easily integrate API calls for updated data.
Justification: This method is inflexible, inefficient, and not scalable for real-time updates, making it less effective than an agent-based approach.

Summary:
Option 2 is the best approach for integrating an agent system that combines multiple tools (API, tables, etc.) into a flexible, scalable architecture, allowing the system to handle different types of queries and always use the most appropriate resource for the task at hand.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
21

Skill Section: 
 
Question:  
A Generative AI Engineer has been asked to design an LLM-based application that accomplishes the following business objective: answer employee HR questions using HR PDF documentation.
Which set of high level tasks should the Generative AI Engineer's system perform?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Calculate averaged embeddings for each HR document, compare embeddings to user query to find the best document. Pass the best document with the user query into an LLM with a large context window to generate a response to the employee.

Option 2:
Use an LLM to summarize HR documentation. Provide summaries of documentation and user query into an LLM with a large context window to generate a response to the user.


Option 3:
Create an interaction matrix of historical employee questions and HR documentation. Use ALS to factorize the matrix and create embeddings. Calculate the embeddings of new queries and use them to find the best HR documentation. Use an LLM to generate a response to the employee question based upon the documentation retrieved.

Option 4:
Split HR documentation into chunks and embed into a vector store. 
Use the employee question to retrieve best matched chunks of documentation, and use the LLM to generate a response to the employee based upon the documentation retrieved.


Explanation:
Correct Option: Option 4
Explanation:

This option uses a vector store to organize chunks of HR documentation. The documentation is split into smaller, manageable parts (chunks), which are then embedded (converted into numerical vectors) and stored in a vector store (such as FAISS or Pinecone).
When an employee submits a query, the system uses the query to retrieve the most relevant chunks of documentation from the vector store. These chunks are selected based on their semantic similarity to the query.
After retrieving the best-matched documentation, the system passes the relevant chunks along with the query into the LLM (Large Language Model). The LLM then generates a response to the employee based on the context of the retrieved documentation.
This method is efficient because:
Scalable: The HR documentation can be extensive, and chunking it allows the system to handle large volumes of text.
Contextual: It provides the LLM with a focused set of relevant information for answering the query, which increases the accuracy of the response.
Real-time querying: The vector search helps quickly find the right chunks, enabling rapid responses.

Wrong option Explanation 

Option 1:
"Calculate averaged embeddings for each HR document, compare embeddings to user query to find the best document. Pass the best document with the user query into an LLM with a large context window to generate a response to the employee."

Why it's less effective: Averaging embeddings for entire documents can lose important context and specific details within the document. A single document may contain a broad range of topics, so choosing it based on an average embedding could result in a less relevant document. Additionally, comparing full document embeddings to a query may not be as precise as retrieving smaller, specific chunks that are contextually more relevant to the query.

Option 2:
"Use an LLM to summarize HR documentation. Provide summaries of documentation and user query into an LLM with a large context window to generate a response to the user."

Explanation:

This option first uses an LLM to summarize the HR documentation and then feeds the summary along with the query into the LLM to generate a response.
Why it's less effective: Summarization of large documents can sometimes result in loss of important details. HR documents might contain specific clauses, policies, or examples that are important for answering detailed employee questions. If the summaries lose these details, the LLM might not be able to provide accurate or complete answers. Additionally, summarizing documents dynamically adds an extra layer of processing that might impact efficiency.


Option 3:
"Create an interaction matrix of historical employee questions and HR documentation. Use ALS (Alternating Least Squares ) to factorize the matrix and create embeddings. Calculate the embeddings of new queries and use them to find the best HR documentation. Use an LLM to generate a response to the employee question based upon the documentation retrieved."

Explanation:

This option involves creating an interaction matrix based on historical questions and HR documents, using Alternating Least Squares (ALS) to factorize the matrix and create embeddings. The embeddings are then used to find relevant documentation.
Why it's less effective: This method is complex and typically used for collaborative filtering in recommendation systems, not direct query answering. It relies on historical interaction data (which may not be available for all questions) and adds unnecessary complexity by involving matrix factorization. While it might work in scenarios with sufficient interaction data, it's over-engineered for a simple HR query answering system. Moreover, this approach may not capture the latest documentation updates or specific employee queries that don’t follow a historical pattern.


Summary of Why Option 4 is the Best:
Efficiency: By using chunking and vector search, Option 4 allows the system to process large HR documents in an organized way, making it faster and more scalable.
Accuracy: It ensures that the LLM receives only the most relevant information for each query, improving the quality of responses.
Simplicity: The approach avoids complex processes like matrix factorization or summarization, making it easier to implement and maintain.


References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
22

Skill Section: 
 
Question:  
Generative AI Engineer at an electronics company just deployed a RAG application for customers to ask questions about products that the company carries. However, they received feedback that the RAG response often returns information about an irrelevant product.
What can the engineer do to improve the relevance of the RAG’s response?

Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):

Guideline Time in Seconds (30, 45, 60):

Option 1:
Assess the quality of the retrieved context

Option 2:
Implement caching for frequently asked questions


Option 3:
Use a different LLM to improve the generated response


Option 4:
Use a different semantic similarity search algorithm

Difficulty Level: Medium

Guideline Time in Seconds: 45



Option 1:
"Assess the quality of the retrieved context"

Explanation:

Option 1 suggests assessing the quality of the retrieved context. The key reason the RAG (Retrieval-Augmented Generation) model might provide irrelevant information is due to the quality of the documents or data retrieved in response to the customer’s query.
Why it works: If the retrieved context contains irrelevant product information, the LLM might generate a response based on that context. Therefore, evaluating and improving the search algorithm, or tuning how documents are retrieved, would directly address the issue of irrelevant information. This ensures the retrieval process prioritizes highly relevant documents or product details, resulting in more accurate responses.


Option 2:
"Implement caching for frequently asked questions."

Explanation:

Option 2 suggests implementing caching for frequently asked questions.
Why it's less effective: While caching can speed up responses for common queries, it doesn't directly address the issue of irrelevant product information being returned. In fact, it could potentially worsen the issue if cached answers are incorrect or outdated. Caching is more useful for performance improvements than solving relevance issues in the content retrieved.


Option 3:
"Use a different LLM to improve the generated response."

Explanation:

Option 3 suggests using a different LLM to generate the response.
Why it's less effective: Switching to a different LLM may alter the style or tone of the responses, but it will not address the core issue of irrelevant information being retrieved. The LLM’s role is to process the retrieved context; if the context is incorrect or irrelevant, the new LLM may still generate the same irrelevant responses. This option doesn't solve the problem at its root.


Option 4:
"Use a different semantic similarity search algorithm."

Explanation:

Option 4 suggests using a different semantic similarity search algorithm to retrieve more relevant documents.
Why it's less effective: While adjusting the search algorithm could potentially improve the relevance of the documents retrieved, assessing the quality of the retrieved context (Option 1) is a more holistic approach. Using a different algorithm can help, but it's only part of the solution. Evaluating the quality of the context as a whole ensures that any algorithmic adjustments will be applied effectively and that the entire retrieval process is optimized.



Summary of Why Option 1 is the Best:
Comprehensive Approach: Option 1 addresses the core issue by focusing on improving the relevance of the retrieved context, which directly impacts the quality of the final response.
Optimized Retrieval: Ensuring that the right context is retrieved for each query can drastically improve the accuracy and relevance of the RAG model's responses.
Actionable: Assessing and adjusting retrieval quality is a tangible, action-based step that directly resolves the underlying problem of irrelevant product information.

References:

Hands-On: 


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
23

Skill Section: 
 
Question:  
A Generative AI Engineer is developing a chatbot designed to assist users with insurance-related queries. The chatbot is built on a large language model (LLM) and is conversational. However, to maintain the chatbot’s focus and to comply with company policy, it must not provide responses to questions about politics. Instead, when presented with political inquiries, the chatbot should respond with a standard message:
“Sorry, I cannot answer that. I am a chatbot that can only answer questions around insurance.”
Which framework type should be implemented to solve this?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Safety Guardrail

Option 2:
Security Guardrail

Option 3:
Contextual Guardrail

Option 4:
Compliance Guardrail



Difficulty Level: Medium

Guideline Time in Seconds: 45

Explanation:

Option 4 involves implementing a Compliance Guardrail. This approach ensures that the chatbot adheres to specific company policies and regulations by restricting the scope of the responses it can provide. In this case, the chatbot should comply with the company's policy of not answering political questions, instead responding with a predefined standard message.
Why it works: A compliance guardrail specifically enforces adherence to rules or guidelines set by the organization or relevant legal frameworks. It ensures that the chatbot stays within the defined scope of insurance-related queries and avoids responding to political questions, thereby fulfilling the compliance requirement.


Option 1:
"Safety Guardrail"

Explanation:

Option 1 suggests a Safety Guardrail, which is typically used to protect against harmful or inappropriate content, such as offensive language, harmful advice, or inappropriate responses.
Why it's less effective: While a safety guardrail is important for preventing harmful content, it is not designed specifically for enforcing policy restrictions, like limiting the chatbot to insurance-related queries. A Compliance Guardrail is a more appropriate framework for ensuring the chatbot adheres to specific guidelines related to company policies.


Option 2:
"Security Guardrail"

Explanation:

Option 2 suggests a Security Guardrail, which is focused on protecting the system from security breaches, ensuring data privacy, and preventing malicious interactions.
Why it's less effective: While security guardrails are crucial for protecting user data and the chatbot’s system, they do not directly address the policy of restricting responses to certain types of queries (such as political questions). This framework is not relevant for enforcing the scope of the chatbot's answers.



Option 3:
"Contextual Guardrail"

Explanation:

Option 3 suggests a Contextual Guardrail, which could manage the context in which the chatbot responds, ensuring the chatbot understands the boundaries of the conversation.
Why it's less effective: While contextual guardrails could help the chatbot stay on topic, a Compliance Guardrail is more focused on enforcing explicit policy restrictions, such as not responding to political questions. A contextual guardrail might manage conversation flow but wouldn't enforce compliance with company policies as effectively as a compliance guardrail would.


Summary of Why Option 4 is the Best:
Policy Enforcement: Option 4 is specifically designed to ensure that the chatbot adheres to company policies and guidelines, making it the most appropriate choice for limiting responses to insurance-related queries and avoiding political discussions.
Clear Restrictions: A compliance guardrail directly addresses the need for restrictive behavior based on predefined company rules, ensuring the chatbot behaves within a legally and ethically defined scope.


References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
24

Skill Section: 
 
Question:  
A Generative AI Engineer is using the code below to test setting up a vector store:

from databricks.vector_search.client import VectorSearchClient
vsc = VectorSearchClient()
vsc.create_endpoint(name="vector_search_test", endpoint_type="STANDARD")

Assuming they intend to use Databricks managed embeddings with the default embedding model, what should be the next logical function call?


Answer: 
2


Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
vsc.get_index()

Option 2:
vsc.create_delta_sync_index()


Option 3:
vsc.create_direct_access_index()


Option 4:
vsc.similarity_search()




Difficulty Level: Medium

Guideline Time in Seconds: 45

Explanation:

Option 2, calling vsc.create_delta_sync_index(), is the next logical step when setting up a vector store in Databricks to use managed embeddings. 
After creating an endpoint, you need to create an index that will store the vectors for efficient retrieval. 
The create_delta_sync_index() function is used to create an index that is optimized for efficient searching, with delta sync capabilities that ensure the vector store is kept in sync with any updates to the data.
Why it works: This function is specifically designed to support the creation of an index for vector-based searches in Databricks and integrates well with managed embeddings. It's the step that follows creating the endpoint when setting up a vector store in Databricks.


Option 1:
"vsc.get_index()"

Explanation:

Option 1 retrieves information about an existing index.
Why it's less effective: This function is useful after an index has been created and you want to check or retrieve details about it. Since the task is focused on setting up a vector store, creating the index (create_delta_sync_index()) is the next logical step, not just fetching information about it.


Option 3:
"vsc.create_direct_access_index()"

Explanation:

Option 3 creates a direct access index, which may be relevant in some specific use cases where direct access to individual vector elements is required.
Why it's less effective: For standard vector search scenarios with managed embeddings, the delta sync index is a more appropriate and efficient choice, as it supports the typical use case of maintaining and querying a vector store.


Option 4:
"vsc.similarity_search()"

Explanation:

Option 4 performs a similarity search on the vectors in the store.
Why it's less effective: This function is used after the vector store and index are set up, and vectors are populated. It is not the next logical step immediately after creating the endpoint. You need to first create an index before you can perform a similarity search.


Summary of Why Option 2 is the Best:
Correct Next Step: create_delta_sync_index() is the proper next step to create an index after setting up the endpoint. It establishes the necessary infrastructure for storing and retrieving vectors.
Optimized for Databricks: This function is designed to optimize the process of storing embeddings and syncing updates, making it ideal for using managed embeddings in a Databricks vector store.


References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
25

Skill Section: 
 
Question:  
A Generative AI Engineer is tasked with deploying an application that takes advantage of a custom MLflow Pyfunc model to return some interim results.
How should they configure the endpoint to pass the secrets and credentials?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Use spark.conf.set ()

Option 2:
Pass variables using the Databricks Feature Store API


Option 3:
Add credentials using environment variables


Option 4:
Pass the secrets in plain text



Explanation:

Option 3, adding credentials using environment variables, is the most secure and appropriate method for passing secrets and credentials in a production deployment. Environment variables allow the system to securely access sensitive information like API keys, database credentials, or other secrets without hardcoding them into the application code.
Why it works: Using environment variables to store credentials ensures that sensitive information is kept secure and separate from the codebase. It also allows for flexibility in deployment environments, as credentials can be managed through environment configuration, making it easier to handle multiple deployment scenarios (e.g., local, staging, production) securely.


Option 1:
"Use spark.conf.set()"

Explanation:

Option 1 uses the spark.conf.set() function, which is used to set Spark configuration properties, but it's not the most appropriate way to securely handle secrets and credentials.
Why it's less effective: spark.conf.set() is typically used for Spark-related settings and configurations rather than for managing sensitive information. Storing credentials in this way would not be ideal for security, as they could be exposed in logs or configuration files.


Option 2:
"Pass variables using the Databricks Feature Store API"

Explanation:

Option 2 uses the Databricks Feature Store API to pass variables.
Why it's less effective: The Feature Store API is mainly designed for managing and accessing features for machine learning models, not for securely passing credentials or secrets. It wouldn't be the best approach for handling sensitive information related to API keys or other secrets.


Option 4:
"Pass the secrets in plain text"

Explanation:

Option 4 suggests passing the secrets in plain text, which is not secure.
Why it's less effective: Passing credentials in plain text is a major security risk, as it exposes sensitive information to anyone with access to the code or logs. It’s best practice to avoid this method entirely.


Summary of Why Option 3 is the Best:
Secure: Environment variables are a secure way to store and access sensitive information like credentials.
Scalable: This method is scalable and can easily be adapted across different environments, such as local, staging, and production, while keeping sensitive data safe.
Standard Practice: Storing credentials in environment variables is a widely accepted practice in the industry for securing secrets in a variety of applications, including machine learning deployment.


References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
26

Skill Section: 
 
Question:  
A Generative AI Engineer wants to build an LLM-based solution to help a restaurant improve its online customer experience with bookings by automatically handling common customer inquiries. The goal of the solution is to minimize escalations to human intervention and phone calls while maintaining a personalized interaction. To design the solution, the Generative AI Engineer needs to define the input data to the LLM and the task it should perform.
Which input/output pair will support their goal?


Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Input: Online chat logs; 
Output: Group the chat logs by users, followed by summarizing each user’s interactions

Option 2:
Input: Online chat logs; 
Output: Buttons that represent choices for booking details


Option 3:
Input: Customer reviews; 
Output: Classify review sentiment


Option 4:
Input: Online chat logs; 
Output: Cancellation options


Explanation:

Option 2 suggests using online chat logs as input and generating buttons that represent choices for booking details as output. This approach supports the goal of improving customer experience by making the booking process interactive and user-friendly. The LLM processes the chat logs to understand customer queries and provides structured, actionable options (like booking dates, times, or table preferences) that the customer can easily select from.
Why it works: This solution minimizes the need for human intervention by guiding users through the booking process in a personalized and interactive manner, which is more efficient than handling every query manually. It also keeps the interaction focused and smooth, reducing the chance of escalation to phone calls.


Option 1:
"Input: Online chat logs; Output: Group the chat logs by users, followed by summarizing each user’s interactions"

Explanation:

Option 1 involves grouping chat logs by users and summarizing each user's interactions.
Why it's less effective: While summarizing interactions might help in understanding customer behavior or improving backend analytics, it doesn't directly contribute to real-time customer interaction or reduce escalations. The goal is to handle common inquiries efficiently, and summarizing past interactions doesn’t provide an immediate, actionable solution for booking queries.


Option 3:
"Input: Customer reviews; Output: Classify review sentiment"

Explanation:

Option 3 focuses on classifying the sentiment of customer reviews.
Why it's less effective: Sentiment analysis on customer reviews is useful for overall business insights but does not directly address handling customer inquiries or improving the booking experience. This task is unrelated to the goal of minimizing escalations and providing a better booking process.


Option 4:
"Input: Online chat logs; Output: Cancellation options"

Explanation:

Option 4 provides cancellation options based on online chat logs.
Why it's less effective: While providing cancellation options could be a part of handling customer inquiries, it doesn’t cover the full spectrum of booking-related questions. The solution should handle a wider range of inquiries, including making new bookings, modifying existing ones, and providing information, not just cancellations.


Summary of Why Option 2 is the Best:
Interactive Booking: Option 2 focuses on improving the booking experience by providing customers with clear, actionable options, making it easier for them to complete the booking process.
Minimizes Escalations: By guiding customers through choices with buttons, it reduces the need for human intervention and ensures queries are resolved efficiently within the chat.
Personalized Experience: The approach supports personalized interaction, enhancing customer satisfaction while automating common inquiries.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
27

Skill Section: 
 
Question:  
What is an effective method to preprocess prompts using custom code before sending them to an LLM?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):

Guideline Time in Seconds (30, 45, 60):

Option 1:
Directly modify the LLM’s internal architecture to include preprocessing steps

Option 2:
It is better not to introduce custom code to preprocess prompts as the LLM has not been trained with examples of the preprocessed prompts

Option 3:
Rather than preprocessing prompts, it’s more effective to postprocess the LLM outputs to align the outputs to desired outcomes


Option 4:
Write a MLflow PyFunc model that has a separate function to process the prompts



Explanation:

Option 4 suggests writing an MLflow PyFunc model with a separate function to process the prompts. This is an effective method because it allows for custom preprocessing logic to be implemented independently of the LLM. The PyFunc model acts as a wrapper, where custom preprocessing can clean, transform, or enrich the input prompts before they are sent to the LLM.
Why it works: This approach maintains a clear separation between the preprocessing logic and the LLM, making the system modular and easier to maintain. It allows flexibility in adapting the preprocessing logic without altering the LLM itself.


Option 1:
"Directly modify the LLM’s internal architecture to include preprocessing steps"

Explanation:

Option 1 suggests modifying the LLM’s internal architecture.
Why it's less effective: Directly altering the internal architecture of the LLM is complex, risky, and not recommended for most use cases. It can lead to unintended consequences and makes the system harder to update or maintain. Preprocessing should be handled separately to avoid these issues.


Option 2:
"It is better not to introduce custom code to preprocess prompts as the LLM has not been trained with examples of the preprocessed prompts"

Explanation:

Option 2 advises against introducing custom preprocessing.
Why it's less effective: While it's true that the LLM has not been explicitly trained on preprocessed prompts, preprocessing is often necessary to ensure inputs are formatted correctly, relevant, and aligned with specific use cases. Properly designed preprocessing does not undermine the LLM’s capabilities; instead, it enhances the input quality.


Option 3:
"Rather than preprocessing prompts, it’s more effective to postprocess the LLM outputs to align the outputs to desired outcomes"

Explanation:

Option 3 focuses on postprocessing the outputs.
Why it's less effective: While postprocessing can be useful for refining outputs, it doesn’t address issues with the quality or relevance of the input prompts. Preprocessing ensures that the LLM receives well-structured and contextually appropriate inputs, which can significantly improve the quality of the generated outputs.



Summary of Why Option 4 is the Best:
Modular Approach: Using an MLflow PyFunc model to handle preprocessing allows for a clean separation of concerns, making the system more modular and maintainable.
Custom Logic: This method supports the implementation of custom preprocessing logic that can be tailored to the specific requirements of the application, ensuring that prompts are properly formatted and relevant before reaching the LLM.
Flexibility and Scalability: It enables flexible adaptation of preprocessing steps without needing to modify the underlying LLM, supporting scalability and easier updates.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
28

Skill Section: 
 
Question:  
A Generative AI Engineer is developing an LLM application that users can use to generate personalized birthday poems based on their names.
Which technique would be most effective in safeguarding the application, given the potential for malicious user inputs?


Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Implement a safety filter that detects any harmful inputs and ask the LLM to respond that it is unable to assist


Option 2:
Reduce the time that the users can interact with the LLM


Option 3:
Ask the LLM to remind the user that the input is malicious but continue the conversation with the user


Option 4:
Increase the amount of compute that powers the LLM to process input faster


Explanation:

Option 1 involves implementing a safety filter that detects harmful inputs and prompts the LLM to respond that it is unable to assist with such inputs. This approach helps in preventing the application from generating harmful or inappropriate content, thereby safeguarding both the system and the user experience.
Why it works: By filtering out harmful inputs before they reach the LLM, this technique ensures that the application maintains its intended use (generating personalized birthday poems) without being exploited for malicious purposes. It prevents the propagation of harmful content and keeps the interaction safe and appropriate.


Option 2:
"Reduce the time that the users can interact with the LLM"

Explanation:

Option 2 suggests limiting the interaction time between users and the LLM.
Why it's less effective: Limiting interaction time does not directly address the issue of malicious inputs. It might reduce the overall usage, but it doesn’t prevent harmful inputs from being processed or ensure that the outputs are safe.


Option 3:
"Ask the LLM to remind the user that the input is malicious but continue the conversation with the user"

Explanation:

Option 3 allows the conversation to continue after reminding the user about malicious input.
Why it's less effective: Continuing the conversation after detecting malicious input could lead to potential exploitation of the system. It’s better to halt the interaction or redirect it to prevent misuse entirely rather than allowing the conversation to proceed.


Option 4:
"Increase the amount of compute that powers the LLM to process input faster"

Explanation:

Option 4 suggests increasing computational resources to process inputs faster.
Why it's less effective: While increasing compute may improve performance, it doesn’t address the core issue of handling malicious inputs. Faster processing alone won’t safeguard the application against harmful user inputs.


Summary of Why Option 1 is the Best:
Proactive Safety: Implementing a safety filter ensures that harmful inputs are detected and prevented from being processed, maintaining the integrity and safety of the application.
Appropriate Response: By informing users that harmful inputs cannot be processed, the application sets clear boundaries and discourages malicious behavior.
Focus on Security: This approach directly targets the potential risks associated with malicious user inputs, ensuring the application remains secure and aligned with its intended purpose.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
29

Skill Section: 
 
Question:  
Which indicator should be considered to evaluate the safety of the LLM outputs when qualitatively assessing LLM responses for a translation use case?


Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
The ability to generate responses in code


Option 2:
The similarity to the previous language


Option 3:
The latency of the response and the length of text generated

Option 4:
The accuracy and relevance of the responses



Explanation:

Option 4 focuses on the accuracy and relevance of the responses, which are critical indicators when assessing the safety and quality of LLM outputs, especially in a translation use case. Ensuring that translations are accurate and relevant helps prevent miscommunications and potential harm caused by incorrect or inappropriate translations.
Why it works: Accurate translations maintain the original meaning and context of the text, while relevance ensures that the translated content is appropriate for the target audience. These factors directly impact the safety and effectiveness of the LLM's output, ensuring that it does not produce misleading or harmful content.


Option 1:
"The ability to generate responses in code"

Explanation:

Option 1 emphasizes generating responses in code.
Why it's less effective: While generating code is useful in programming-related tasks, it is not relevant for evaluating safety in a translation use case. The focus here should be on linguistic accuracy and context rather than coding abilities.


Option 2:
"The similarity to the previous language"

Explanation:

Option 2 suggests evaluating based on the similarity to the previous language.
Why it's less effective: Similarity to the original language doesn't necessarily indicate a safe or accurate translation. The goal in translation is to accurately convey the meaning in the target language, which might not always involve close similarity in phrasing or structure.


Option 3:
"The latency of the response and the length of text generated"

Explanation:

Option 3 focuses on response latency and text length.
Why it's less effective: While response time and text length are performance metrics, they don’t directly relate to the safety or quality of translations. Ensuring quick responses and optimal text length is useful for user experience, but it doesn't guarantee the correctness or appropriateness of the translation.


Summary of Why Option 4 is the Best:
Focus on Content Quality: Accuracy and relevance are key to ensuring that translations are correct and meaningful, minimizing the risk of misinterpretation or harm.
Ensures Safety: By focusing on these indicators, the evaluation ensures that the LLM produces translations that are safe, appropriate, and useful for the end user.
Aligned with Use Case: Accuracy and relevance are the most critical factors for assessing the output in a translation use case, making Option 4 the best choice.
References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
30

Skill Section: 
 
Question:  
A Generative AI Engineer is developing a patient-facing healthcare-focused chatbot. 
If the patient’s question is not a medical emergency, the chatbot should solicit more information from the patient to pass to the doctor’s office and suggest a few relevant pre-approved medical articles for reading. 
If the patient’s question is urgent, direct the patient to calling their local emergency services.

Given the following user input:
“I have been experiencing severe headaches and dizziness for the past two days.”
Which response is most appropriate for the chatbot to generate?


Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Here are a few relevant articles for your browsing. Let me know if you have questions after reading them.

Option 2:
Please call your local emergency services.


Option 3:
Headaches can be tough. Hope you feel better soon!


Option 4:
Please provide your age, recent activities, and any other symptoms you have noticed along with your headaches and dizziness.


Explanation:

Option 2 directs the patient to call their local emergency services. Given the severity of the symptoms described—severe headaches and dizziness over two days—this response is most appropriate. These symptoms could indicate a serious condition, and immediate medical attention is warranted.
Why it works: The chatbot's priority in potential emergencies is to ensure patient safety by advising them to seek immediate professional help. This response aligns with the best practices for handling potentially urgent medical scenarios.


Option 1:
"Here are a few relevant articles for your browsing. Let me know if you have questions after reading them."

Explanation:

Option 1 provides articles for browsing.
Why it's less effective: This response downplays the potential severity of the symptoms. For a case involving severe headaches and dizziness, the chatbot should prioritize safety by advising direct contact with emergency services rather than suggesting informational articles.

Option 3:
"Headaches can be tough. Hope you feel better soon!"

Explanation:

Option 3 offers a casual and non-specific response.
Why it's less effective: This response lacks the seriousness needed for the described symptoms. It does not provide any guidance or suggest taking necessary steps, making it inappropriate for potentially severe medical conditions.

Option 4:
"Please provide your age, recent activities, and any other symptoms you have noticed along with your headaches and dizziness."

Explanation:

Option 4 solicits more detailed information.
Why it's less effective: While gathering more information could be useful for non-emergencies, the symptoms described indicate a potential medical emergency. The appropriate action is to advise the patient to seek immediate help, rather than prolonging the interaction by collecting more details.



Summary of Why Option 2 is the Best:
Safety First: For symptoms that could indicate a serious condition, the chatbot should prioritize patient safety by recommending immediate contact with emergency services.
Urgent Response: This response ensures that the patient takes the necessary steps to receive timely medical attention, which is crucial in potential emergencies.
Compliance with Best Practices: The response adheres to best practices for healthcare chatbots, ensuring they do not delay or mislead patients in urgent situations.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
31


Skill Section: 
 
Question:  
After changing the response generating LLM in a RAG pipeline from GPT-4 to a model with a shorter context length that the company self-hosts, the Generative AI Engineer is getting the following error:

{"error_code": "BAD_REQUEST", "message": "Bad request: rpc error: code = InvalidArgument desc = prompt token count (4595) cannot exceed 4096..."}

What TWO solutions should the Generative AI Engineer implement without changing the response generating model? (Choose two.)

Answer: 
3 and 4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Use a smaller embedding model to generate embeddings

Option 2:
Reduce the maximum output tokens of the new model


Option 3:
Decrease the chunk size of embedded documents


Option 4:
Reduce the number of records retrieved from the vector database


Option 5:
Retrain the response generating model using ALiBi



Explanation:

Option 3: Decrease the chunk size of embedded documents.

Why it works: By reducing the chunk size of the embedded documents, the total number of tokens in each chunk will be smaller. This helps ensure that the combined token count of the prompt and context remains within the model's maximum token limit (4096 tokens in this case). Smaller chunks reduce the risk of exceeding the token limit when multiple chunks are retrieved and included in the prompt.

Option 4: Reduce the number of records retrieved from the vector database.

Why it works: Limiting the number of records retrieved from the vector database reduces the overall context size passed to the model. This directly helps in keeping the token count within the allowed limit, preventing errors caused by exceeding the maximum token count.


Option 1:
"Use a smaller embedding model to generate embeddings."

Explanation:

Why it's less effective: Using a smaller embedding model might improve efficiency or reduce embedding size, but it doesn’t address the core issue of exceeding the token limit in the response-generating model. The error pertains to the total token count, not the size of the embeddings.


Option 2:
"Reduce the maximum output tokens of the new model."

Explanation:

Why it's less effective: The error is related to the token count of the input prompt, not the output tokens. Reducing the maximum output tokens would not solve the issue of exceeding the input token limit.


Summary of Why Options 3 and 4 are Best:
Directly Address Token Limit Issue: Both solutions directly reduce the number of tokens in the input prompt, helping to stay within the model’s token limit.
Effective Context Management: Decreasing chunk size and limiting the number of retrieved records ensure that the context provided to the model is concise and manageable, preventing token overflow errors.


References:

Hands-On: 


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
32

Skill Section: 
 
Question:  
A Generative AI Engineer is building a system which will answer questions on latest stock news articles.
Which will NOT help with ensuring the outputs are relevant to financial news?


Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
E

Guideline Time in Seconds (30, 45, 60):
30

Option 1:
Implement a comprehensive guardrail framework that includes policies for content filters tailored to the finance sector.

Option 2:
Increase the compute to improve processing speed of questions to allow greater relevancy analysis

Option 3:
Implement a profanity filter to screen out offensive language.

Option 4:
Incorporate manual reviews to correct any problematic outputs prior to sending to the users


Explanation:

Option 3: Implement a profanity filter to screen out offensive language.
Why it doesn't help: A profanity filter is designed to screen out offensive language, but it doesn't directly contribute to ensuring the relevance of outputs to financial news. While it's important for maintaining appropriate language, it doesn't affect the accuracy or relevance of financial content.

Option 1:
"Implement a comprehensive guardrail framework that includes policies for content filters tailored to the finance sector."

Explanation:

Why it helps: Tailoring content filters specifically for the finance sector ensures that irrelevant or inappropriate content is excluded, helping maintain relevance to financial news.

Option 2:
"Increase the compute to improve processing speed of questions to allow greater relevancy analysis."

Explanation:

Why it helps: Improving processing speed allows for more complex relevancy analyses and better handling of real-time financial data, ensuring outputs are more aligned with the latest stock news.
Option 4:
"Incorporate manual reviews to correct any problematic outputs prior to sending to the users."

Explanation:

Why it helps: Manual reviews allow human experts to ensure the outputs are relevant and accurate, particularly important in the dynamic and detail-oriented context of financial news.


Summary of Why Option 3 is the Best Answer:
Irrelevant to Relevance: While important for maintaining appropriate language, a profanity filter doesn't impact the relevancy of the content to financial news, making it the least relevant option for ensuring output relevancy in this context.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
33

Skill Section: 
 
Question:  
A Generative AI Engineer is building a RAG application that answers questions about internal documents for the company SnoPen AI.
The source documents may contain a significant amount of irrelevant content, such as advertisements, sports news, or entertainment news, or content about other companies.
Which approach is advisable when building a RAG application to achieve this goal of filtering irrelevant information?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Keep all articles because the RAG application needs to understand non-company content to avoid answering questions about them.

Option 2:
Include in the system prompt that any information it sees will be about SnoPenAI, even if no data filtering is performed.


Option 3:
Include in the system prompt that the application is not supposed to answer any questions unrelated to SnoPen AI.


Option 4:
Consolidate all SnoPen AI related documents into a single chunk in the vector database.


Explanation:

Option 3: Include in the system prompt that the application is not supposed to answer any questions unrelated to SnoPen AI.
Why it works: By explicitly instructing the system to focus only on questions related to SnoPen AI, the application is guided to ignore irrelevant information. This approach helps filter out unrelated content and ensures responses are aligned with the company-specific context.


Option 1:
"Keep all articles because the RAG application needs to understand non-company content to avoid answering questions about them."

Explanation:

Why it's less effective: Keeping all articles, including irrelevant ones, increases the risk of the model retrieving and responding with non-relevant content. It's more efficient to filter out irrelevant content upfront rather than relying on the model to avoid answering based on understanding.


Option 2:
"Include in the system prompt that any information it sees will be about SnoPen AI, even if no data filtering is performed."

Explanation:

Why it's less effective: This approach assumes that the system will automatically disregard irrelevant content, which may not happen if the irrelevant content is still included in the data. Explicit filtering or instructive prompts are more reliable.


Option 4:
"Consolidate all SnoPen AI related documents into a single chunk in the vector database."

Explanation:

Why it's less effective: Consolidating all documents into a single chunk might simplify storage but does not effectively filter out irrelevant content. It could also lead to inefficiencies in retrieval, as the model would still have to process irrelevant information within that chunk.


Summary of Why Option 3 is the Best Answer:
Focused Filtering: By setting clear boundaries in the system prompt, the application is steered towards responding only to relevant queries, thereby minimizing distractions from irrelevant content.
Efficient Guidance: This method leverages the system prompt to dynamically filter out non-relevant topics, enhancing the relevance and accuracy of the RAG application’s outputs.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
34

Skill Section: 
 
Question:  
A Generative AI Engineer has successfully ingested unstructured documents and chunked them by document sections. 
They would like to store the chunks in a Vector Search index. 
The current format of the dataframe has two columns: 
(i) original document file name 
(ii) an array of text chunks for each document.
What is the most performant way to store this dataframe?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Split the data into train and test set, create a unique identifier for each document, then save to a Delta table

Option 2:
Flatten the dataframe to one chunk per row, create a unique identifier for each row, and save to a Delta table


Option 3:
First create a unique identifier for each document, then save to a Delta table


Option 4:
Store each chunk as an independent JSON file in Unity Catalog Volume. 
For each JSON file, the key is the document section name and the value is the array of text chunks for that section


Explanation:

Option 2: Flatten the dataframe to one chunk per row, create a unique identifier for each row, and save to a Delta table.
Why it works: Flattening the dataframe so that each text chunk is stored in its own row ensures efficient querying and retrieval from the vector search index. Storing each chunk with a unique identifier allows for better indexing and facilitates fast, granular searches. 
Using a Delta table provides scalable and performant storage, with built-in support for versioning and transactions.


Option 1:
"Split the data into train and test set, create a unique identifier for each document, then save to a Delta table."

Explanation:

Why it's less effective: Splitting the data into train and test sets is more relevant for training machine learning models rather than for storing data in a vector search index. This approach doesn't address the need for chunk-level indexing, which is critical for efficient vector search.


Option 3:
"First create a unique identifier for each document, then save to a Delta table."

Explanation:

Why it's less effective: While creating a unique identifier for each document helps in organizing the data, this method doesn't account for the need to store and index individual chunks separately. For efficient vector search, it's crucial to have each chunk as a separate entry.


Option 4:
"Store each chunk as an independent JSON file in Unity Catalog Volume. For each JSON file, the key is the document section name and the value is the array of text chunks for that section."

Explanation:

Why it's less effective: Storing chunks as independent JSON files can lead to storage inefficiencies and slower retrieval times, especially when dealing with large-scale data. Additionally, this approach may not leverage the full benefits of vector search indexing, which thrives on structured, tabular formats like Delta tables.


Summary of Why Option 2 is the Best Answer:
Optimal Structure: Flattening the dataframe ensures each chunk is treated as a separate entity, which is essential for efficient vector search operations.
Scalability and Performance: Saving the data in a Delta table allows for efficient storage, retrieval, and management of large datasets, making it the most performant choice for this use case.


References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
35

Skill Section: 
 
Question:  
A Generative AI Engineer has created a RAG application which can help employees retrieve answers from an internal knowledge base, such as Confluence pages or Google Drive. 
The prototype application is now working with some positive feedback from internal company testers. 
Now the Generative AI Engineer wants to formally evaluate the system’s performance and understand where to focus their efforts to further improve the system.
How should the Generative AI Engineer evaluate the system?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Use cosine similarity score to comprehensively evaluate the quality of the final generated answers.


Option 2:
Curate a dataset that can test the retrieval and generation components of the system separately. 
Use MLflow’s built in evaluation metrics to perform the evaluation on the retrieval and generation components.


Option 3:
Benchmark multiple LLMs with the same data and pick the best LLM for the job.


Option 4:
Use an LLM-as-a-judge to evaluate the quality of the final answers generated.



Explanation:

Option 2: Curate a dataset that can test the retrieval and generation components of the system separately. Use MLflow’s built-in evaluation metrics to perform the evaluation on the retrieval and generation components.
Why it works: This approach allows the engineer to independently evaluate the retrieval and generation aspects of the system, identifying specific areas for improvement. Using MLflow’s built-in evaluation metrics ensures a structured and thorough analysis, leading to a more targeted optimization of the system's performance.


Option 1:
"Use cosine similarity score to comprehensively evaluate the quality of the final generated answers."

Explanation:

Why it's less effective: Cosine similarity primarily measures the similarity between two vectors, which can be useful for retrieval but doesn't fully capture the nuanced quality of the final generated answers. It doesn't evaluate the content's correctness or relevance in the context of the user's query.


Option 3:
"Benchmark multiple LLMs with the same data and pick the best LLM for the job."

Explanation:

Why it's less effective: While benchmarking different LLMs can help in selecting a suitable model, it doesn't provide a comprehensive evaluation of the system's performance. This approach focuses more on model selection rather than on assessing and improving the retrieval and generation processes.


Option 4:
"Use an LLM-as-a-judge to evaluate the quality of the final answers generated."

Explanation:

Why it's less effective: While using an LLM-as-a-judge can provide insights into the quality of the generated answers, it lacks the depth of a structured evaluation framework like MLflow. This approach may not give detailed feedback on the separate components of the system.


Summary of Why Option 2 is the Best Answer:
Comprehensive Evaluation: By testing retrieval and generation components separately, the engineer can pinpoint specific areas for improvement.
Structured Metrics: Leveraging MLflow’s built-in metrics ensures a standardized evaluation, leading to more actionable insights for enhancing the system.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
36

Skill Section: 
 
Question:  
A Generative AI Engineer has already trained an LLM on Databricks and it is now ready to be deployed.
Which of the following steps correctly outlines the easiest process for deploying a model on Databricks?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
E

Guideline Time in Seconds (30, 45, 60):
30

Option 1:
Log the model as a pickle object, upload the object to Unity Catalog Volume, register it to Unity Catalog using MLflow, and start a serving endpoint


Option 2:
Log the model using MLflow during training, directly register the model to Unity Catalog using the MLflow API, and start a serving endpoint


Option 3:
Save the model along with its dependencies in a local directory, build the Docker image, and run the Docker container


Option 4:
Wrap the LLM’s prediction function into a Flask application and serve using Gunicorn


Explanation:

Option 2: Log the model using MLflow during training, directly register the model to Unity Catalog using the MLflow API, and start a serving endpoint.
Why it works: This is the easiest and most straightforward process on Databricks. MLflow integrates seamlessly with Databricks for logging models during training, and registering the model with Unity Catalog is streamlined using the MLflow API. Once the model is registered, deploying it by starting a serving endpoint is simple, leveraging Databricks' built-in capabilities for model deployment.


Option 1:
"Log the model as a pickle object, upload the object to Unity Catalog Volume, register it to Unity Catalog using MLflow, and start a serving endpoint."

Explanation:

Why it's less effective: While logging the model as a pickle object and using Unity Catalog for storage is an option, it’s not as efficient or standardized as directly using MLflow to log and register the model. MLflow provides a more integrated approach for managing models in Databricks.


Option 3:
"Save the model along with its dependencies in a local directory, build the Docker image, and run the Docker container."

Explanation:

Why it's less effective: While this approach is valid in some cases, it requires more steps, manual setup, and extra management of Docker containers. Databricks simplifies deployment with MLflow and Unity Catalog, so this approach is less optimal for Databricks-native model deployment.


Option 4:
"Wrap the LLM’s prediction function into a Flask application and serve using Gunicorn."

Explanation:

Why it's less effective: This approach is more suited for custom deployments outside of Databricks. In Databricks, leveraging MLflow and Unity Catalog provides a more integrated and efficient method for model deployment.


Summary of Why Option 2 is the Best Answer:
Streamlined Process: Option 2 directly utilizes Databricks' native tools (MLflow and Unity Catalog) to log, register, and deploy the model, making it the easiest and most efficient method for deployment.
Minimal Setup: This approach minimizes manual steps and leverages Databricks' built-in infrastructure for quick and smooth deployment.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
37

Skill Section: 
 
Question:  
A Generative AI Engineer developed an LLM application using the provisioned throughput Foundation Model API. 
Now that the application is ready to be deployed, they realize their volume of requests are not sufficiently high enough to create their own provisioned throughput endpoint. 
They want to choose a strategy that ensures the best cost-effectiveness for their application.
What strategy should the Generative AI Engineer use?


Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
E

Guideline Time in Seconds (30, 45, 60):
30

Option 1:
Switch to using External Models instead

Option 2:
Deploy the model using pay-per-token throughput as it comes with cost guarantees

Option 3:
Change to a model with a fewer number of parameters in order to reduce hardware constraint issues

Option 4:
Throttle the incoming batch of requests manually to avoid rate limiting issues



Explanation:

Option 2: Deploy the model using pay-per-token throughput as it comes with cost guarantees.
Why it works: The pay-per-token strategy ensures that you only pay for what you use, making it the most cost-effective approach for low-volume requests. 
It allows the application to scale dynamically without the need to commit to a provisioned throughput endpoint, which would be unnecessary and more expensive when traffic is low.


Option 1:
"Switch to using External Models instead."

Explanation:

Why it's less effective: Switching to external models could be an option, but it may not directly address the cost-effectiveness of the application. Additionally, external models might not offer the same level of integration and scalability as the provisioned throughput option or pay-per-token models in Databricks or similar platforms.


Option 3:
"Change to a model with fewer number of parameters in order to reduce hardware constraint issues."

Explanation:

Why it's less effective: Reducing the number of parameters could help with performance and resource requirements but might compromise the quality of the generated responses. It doesn't directly address the issue of cost-effectiveness for low request volumes.


Option 4:
"Throttle the incoming batch of requests manually to avoid rate limiting issues."

Explanation:

Why it's less effective: Throttling incoming requests manually may reduce the need for a provisioned throughput endpoint, but it introduces complexity in managing the system and doesn't provide a cost-efficient solution for variable request volumes. It also may not be a scalable long-term solution.


Summary of Why Option 2 is the Best Answer:
Cost Efficiency: The pay-per-token throughput strategy aligns costs with usage, ensuring that the application is cost-effective even with low volumes of requests.
Scalability: This model provides flexibility, allowing the application to scale up or down based on demand, without incurring unnecessary costs associated with provisioned throughput.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
38

Skill Section: 
 
Question:  
A Generative AI Engineer is building an LLM to generate article summaries in the form of a type of poem, such as a haiku, given the article content. 
However, the initial output from the LLM does not match the desired tone or style.
Which approach will NOT improve the LLM’s response to achieve the desired response?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Provide the LLM with a prompt that explicitly instructs it to generate text in the desired tone and style

Option 2:
Use a neutralizer to normalize the tone and style of the underlying documents


Option 3:
Include few-shot examples in the prompt to the LLM


Option 4:
Fine-tune the LLM on a dataset of desired tone and style


Explanation:

Option 2: Use a neutralizer to normalize the tone and style of the underlying documents.
Why it doesn't work: A neutralizer may attempt to standardize the tone and style of input text, but it is unlikely to help in achieving the specific creative output, like a poem in a particular style (e.g., haiku). The approach of normalizing tone and style may reduce the variation needed to match the specific artistic tone or style you're aiming for. Instead, explicit guidance through prompts, few-shot examples, or fine-tuning are more effective ways to train the LLM for the desired style.


Option 1:
"Provide the LLM with a prompt that explicitly instructs it to generate text in the desired tone and style."

Explanation:

Why it works: Providing clear and explicit instructions in the prompt, such as asking the model to generate a haiku or poetry in a particular tone, gives the model direct guidance on how to structure the output. This can effectively lead to more controlled and targeted outputs.


Option 3:
"Include few-shot examples in the prompt to the LLM."

Explanation:

Why it works: Including few-shot examples in the prompt provides the LLM with specific examples of the tone, style, or structure you are looking for. This helps the model understand the nuances of the desired output and improves the likelihood of generating text in that style.


Option 4:
"Fine-tune the LLM on a dataset of desired tone and style."

Explanation:

Why it works: Fine-tuning the model on a dataset that contains examples of the desired tone and style (e.g., haikus, poems, etc.) will help the LLM internalize those patterns and generate more accurate outputs over time. This is an effective method for training the model to consistently match the desired tone and style.

Summary of Why Option 2 is the Best Answer:
Neutralizing tone and style does not align with the goal of generating specific, creative outputs like haikus or poetry. The other options focus on providing explicit guidance, examples, or training to directly influence the style of the generated output, making them more effective in this case.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
39

Skill Section: 
 
Question:  
A Generative AI Engineer is creating an LLM-powered application that will need access to up-to-date news articles and stock prices.
The design requires the use of stock prices which are stored in Delta tables and finding the latest relevant news articles by searching the internet.
How should the Generative AI Engineer architect their LLM system?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45


Option 1:
Use an LLM to summarize the latest news articles and lookup stock tickers from the summaries to find stock prices.

Option 2:
Query the Delta table for volatile stock prices and use an LLM to generate a search query to investigate potential causes of the stock volatility.

Option 3:
Download and store news articles and stock price information in a vector store. Use a RAG architecture to retrieve and generate at runtime.

Option 4:
Create an agent with tools for SQL querying of Delta tables and web searching, provide retrieved values to an LLM for generation of response.


Explanation:

Option 4: Create an agent with tools for SQL querying of Delta tables and web searching, provide retrieved values to an LLM for generation of response.
Why it works: This option allows the system to use specific tools for accessing both the latest stock prices stored in Delta tables and relevant news articles via web searching. By designing an agent that can dynamically query these sources and provide the results to the LLM, the system is able to generate responses that are always up-to-date and relevant, ensuring access to both structured data (stock prices) and unstructured data (news articles) at runtime. This architecture is flexible and scalable, making it ideal for the dynamic requirements of the application.


Option 1:
"Use an LLM to summarize the latest news articles and lookup stock tickers from the summaries to find stock prices."

Explanation:

Why it's less effective: While summarizing news articles can be useful, relying on the LLM to extract stock tickers from those summaries could introduce errors or inaccuracies. The LLM might misinterpret the data, leading to incorrect stock ticker lookups. This approach doesn't ensure real-time access to the most accurate stock price information, making it less reliable compared to querying the data directly from Delta tables.


Option 2:
"Query the Delta table for volatile stock prices and use an LLM to generate a search query to investigate potential causes of the stock volatility."

Explanation:

Why it's less effective: This approach is focused on analyzing stock volatility rather than providing real-time, accurate responses to a broader set of questions (e.g., latest news and stock prices). Generating a search query based on volatility alone limits the scope of the LLM's functionality and may not address all user inquiries effectively. It also introduces an extra layer of complexity that may not be necessary for retrieving simple stock price data.


Option 3:
"Download and store news articles and stock price information in a vector store. Use a RAG architecture to retrieve and generate at runtime."

Explanation:

Why it's less effective: While RAG architectures can be useful for retrieving relevant documents, storing news articles and stock prices in a vector store might not guarantee access to the most up-to-date data. The application would rely on pre-processed information, which could become outdated or inaccurate as time passes. This makes it less dynamic compared to querying real-time sources like the Delta table for stock prices and using a web search for the latest news.


Summary of Why Option 4 is the Best Answer:
Real-time Data Access: Option 4 ensures access to both current stock prices and the latest news articles through dedicated tools (SQL querying for stock prices and web searching for news). This makes the system more dynamic and capable of providing relevant, up-to-date information.
Flexibility: It allows the LLM to generate responses using the latest and most accurate data, which is crucial for applications that require real-time information.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
40

Skill Section: 
 
Question:  
A Generative AI Engineer is designing a chatbot for a gaming company that aims to engage users on its platform while its users play online video games.
Which metric would help them increase user engagement and retention for their platform?


Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Randomness


Option 2:
Diversity of responses


Option 3:
Lack of relevance


Option 4:
Repetition of responses


Explanation:

Option 2: Diversity of responses
Why it works: For a gaming platform, increasing user engagement and retention depends on maintaining dynamic and engaging interactions. Offering a variety of responses helps keep the conversation fresh and interesting, which can make users more likely to return and continue interacting with the chatbot. Diverse responses ensure that users feel the chatbot is capable of handling different types of queries or discussions, thus enhancing their overall experience on the platform. This diversity can contribute to deeper engagement with the game and the platform.


Option 1:
"Randomness"

Why it's less effective: While some level of randomness can add an element of surprise, it alone doesn’t ensure meaningful or relevant engagement. Random responses may feel out of context or disjointed, which could frustrate users. It lacks the structured consistency that diversity of responses can offer.


Option 3:
"Lack of relevance"

Why it's not useful: Lack of relevance would lead to disengagement and frustration. If the chatbot provides responses that are irrelevant to the user’s queries or interests, it will negatively impact user experience, decreasing engagement and retention.


Option 4:
"Repetition of responses"

Why it's not helpful: Repetition of responses can quickly become monotonous for users, which will likely decrease engagement. Users may lose interest if they feel like they are receiving the same response repeatedly, which can lead to reduced retention.


Summary of Why Option 2 is the Best Answer:
Diversity of responses helps maintain user interest and ensures the chatbot remains engaging over multiple interactions. This approach provides users with a more dynamic and responsive experience, fostering greater engagement and retention on the gaming platform.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
41

Skill Section: 
 
Question:  
A company has a typical RAG-enabled, customer-facing chatbot on its website.

User Question -> 1 -> 2 -> 3 -> 4 -> Output
        |                  ^
        |                  |
        |                  |
        -------------------

Select the correct sequence of components a user's questions will go through before the final output is returned. Use the diagram above for reference.

Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
1.embedding model
2.vector search
3.context-augmented prompt
4.response-generating LLM


Option 2:
1.context-augmented prompt
2.vector search
3.embedding model
4.response-generating LLM


Option 3:
1.response-generating LLM
2.vector search
3.context-augmented prompt
4.embedding model


Option 4:
1.response-generating LLM
2.context-augmented prompt
3.vector search
4.embedding model



Explanation:
Explanation: The correct sequence follows the standard architecture of a Retrieval-Augmented Generation (RAG) system, which is used in customer-facing chatbots.

Embedding model: The user's question is first passed through the embedding model, which converts the text into embeddings. This is the first step because it transforms the input question into a numerical representation that can be used for semantic search.

Vector search: After embedding the user’s question, the next step is to perform a vector search in the index (a vector store) to retrieve the most relevant documents or chunks that match the question.

Context-augmented prompt: Once relevant documents are retrieved, they are added to the context-augmented prompt to provide additional context for the LLM. This prompt includes the retrieved information along with the original query to enhance the response generation.

Response-generating LLM: Finally, the response-generating LLM processes the context-augmented prompt and generates a response for the user.

Other Options Analysis:
Option 2:

The sequence starts with a context-augmented prompt, which is incorrect because the context comes after retrieving relevant documents. Hence, this order is not correct.
Option 3:

The response-generating LLM should not come before the embedding model, as the system needs embeddings for accurate retrieval first. This order is out of sequence.
Option 4:

Starting with the response-generating LLM is incorrect. The LLM requires embeddings, relevant context, and retrieved documents, which should come before generating the response.


Summary:
Option 1 correctly follows the standard RAG flow: embedding model → vector search → context-augmented prompt → response-generating LLM.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
42

Skill Section: 
 
Question:  
A team wants to serve a code generation model as an assistant for their software developers. 
It should support multiple programming languages. 
Quality is the primary objective.
Which of the Databricks Foundation Model APIs, or models available in the Marketplace, would be the best fit?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Llama2-70b

Option 2:
BGE-large

Option 3:
MPT-7b

Option 4:
CodeLlama-34B


Explanation: The best model for supporting multiple programming languages and focusing on quality for code generation would be CodeLlama-34B.

CodeLlama-34B is designed specifically for code generation and is a powerful model built for software development tasks. It excels in generating high-quality code, understanding programming languages, and handling complex development tasks across different programming languages. It is an ideal choice for a team looking to provide software developers with a high-quality assistant for code generation.
Other Options Analysis:


Option 1: Llama2-70b:

While Llama2-70b is a large, capable model, it is not specifically fine-tuned for code generation. It's a general-purpose language model that might not provide the same level of performance or quality as a model trained for coding tasks.


Option 2: BGE-large:

BGE-large is a model typically used for embedding-based tasks, like retrieval-based tasks or semantic search. It is not specialized for generating code or supporting multiple programming languages, so it would not be as effective in this scenario.


Option 3: MPT-7b:

MPT-7b is a versatile model, but it is not specifically designed for code generation. While it might perform well for general tasks, it is less optimized than CodeLlama-34B for the specific task of code assistance for developers.


Summary:
CodeLlama-34B (Option 4) is specifically designed and fine-tuned for code generation tasks and would be the best fit for assisting developers with high-quality, multi-language support.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
43

Skill Section: 
 
Question:  
A Generative AI Engineer is building a RAG application that will rely on context retrieved from source documents that are currently in PDF format. 
These PDFs can contain both text and images. They want to develop a solution using the least amount of lines of code.
Which Python package should be used to extract the text from the source documents?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
E

Guideline Time in Seconds (30, 45, 60):
30

Option 1:
flask

Option 2:
beautifulsoup


Option 3:
unstructured


Option 4:
numpy


Explanation: The most efficient Python package for extracting text from PDF documents, particularly when they contain both text and images, is unstructured.

unstructured is a Python package specifically designed to extract and process text from various document formats, including PDFs. It handles text extraction from structured and unstructured documents (like PDFs with images and text) and minimizes the amount of code needed for this task, making it ideal for the use case described in the question.
Other Options Analysis:


Option 1: flask:

Flask is a micro web framework used for building web applications. It is not suited for text extraction from PDFs and is irrelevant to the task.

Option 2: beautifulsoup:

BeautifulSoup is used for parsing HTML and XML documents, not PDFs. It is a great tool for web scraping, but not for extracting text from PDFs.

Option 4: numpy:

NumPy is a library for numerical operations and is not designed for text extraction or working with PDFs.


Summary:
unstructured (Option 3) is the best choice for extracting text from PDFs with minimal code. It is tailored for working with documents in various formats, including PDFs, and is optimized for this purpose.


References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
44

Skill Section: 
 
Question:  
A Generative AI Engineer received the following business requirements for an external chatbot.
The chatbot needs to know what types of questions the user asks and routes to appropriate models to answer the questions. 
For example, the user might ask about upcoming event details. Another user might ask about purchasing tickets for a particular event.
What is an ideal workflow for such a chatbot?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
The chatbot should only look at previous event information

Option 2:
There should be two different chatbots handling different types of user queries.


Option 3:
The chatbot should be implemented as a multi-step LLM workflow. First, identify the type of question asked, then route the question to the appropriate model. If it’s an upcoming event question, send the query to a text-to-SQL model. If it’s about ticket purchasing, the customer should be redirected to a payment platform.


Option 4:
The chatbot should only process payments


Explanation: The ideal solution for this scenario is to design the chatbot as a multi-step LLM workflow. The chatbot should first identify the type of user query, then route it to the appropriate model for answering the question or handling the task. This allows the chatbot to be dynamic and handle different types of questions efficiently.

Step 1: Identify the type of question (e.g., event details or ticket purchasing).
Step 2: Route the query to the appropriate model.
For questions about upcoming events, the chatbot could use a text-to-SQL model to query the database for relevant event details.
For questions about purchasing tickets, the chatbot could redirect the user to the payment platform to complete the transaction.
This approach ensures that the chatbot can manage multiple query types without needing separate systems for each, making it more scalable and efficient.

Other Options Analysis:
Option 1: The chatbot should only look at previous event information:

This option is too limited and doesn't account for the dynamic nature of user queries (e.g., ticket purchasing or other event-related inquiries).
Option 2: There should be two different chatbots handling different types of user queries:

Having separate chatbots for each query type is inefficient and not scalable. It would also introduce unnecessary complexity.
Option 4: The chatbot should only process payments:

This option would overly constrain the chatbot to just handling payments, missing out on other important tasks such as answering queries about events.

Summary:
The best approach is Option 3, where the chatbot is designed as a multi-step workflow that can intelligently route questions to the appropriate models or systems, ensuring efficient and scalable handling of various user queries.


References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
45

Skill Section: 
 
Question:  
A Generative Al Engineer is tasked with developing an application that is based on an open source large language model (LLM). 
They need a foundation LLM with a large context window.
Which model fits this need?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
DistilBERT

Option 2:
MPT-30B

Option 3:
Llama2-70B

Option 4:
DBRX



Explanation: The MPT-30B model is a large language model with a large context window that fits the requirement. It is designed to handle long-context processing, making it suitable for applications that require handling large amounts of input data. The "30B" in the model name indicates it has 30 billion parameters, which supports handling large inputs effectively.

Other Options Analysis:

Option 1: DistilBERT:
DistilBERT is a smaller and more efficient version of BERT, but it has a limited context window (typically around 512 tokens), making it less suitable for tasks requiring long-context handling.

Option 3: Llama2-70B:
While Llama2-70B is a large model with substantial capabilities, its context window may not be as large as MPT-30B's, depending on the specific variant and usage, so it may not be the most optimal choice for long-context tasks compared to MPT-30B.

Option 4: DBRX:
DBRX is a specialized model used for specific tasks within the Databricks ecosystem but is not known for its large context window, so it wouldn't be as 
fitting for applications requiring long-context handling.

Summary:
The MPT-30B (Option 2) is the most suitable model for applications that require a large context window, making it the best choice for this scenario.

References:

Hands-On: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
46

Skill Section: 
 
Question:  
Objective: Apply a chunking strategy for a given document structure and model constraints
A Generative AI Engineer is loading 150 million embeddings into a vector database that takes a
maximum of 100 million.
Which TWO actions can they take to reduce the record count?


Answer: 
1,2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Increase the document chunk size

Option 2:
Decrease the overlap between chunks
 
Option 3:
Decrease the document chunk size

 
Option 4:
Increase the overlap between chunks

Option 5:
Use a smaller embedding model


References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
47

Skill Section: 
 
Question:  
Objective: Identify needed source documents that provide necessary knowledge and quality for a
given RAG application.

A Generative AI Engineer is assessing the responses from a customer-facing GenAI application that they are developing to assist in selling automotive parts. 
The application requires the customer to explicitly input account_id and transaction_id to answer questions. 
After initial launch, the customer feedback was that the application did well on answering order and billing details, but failed to accurately answer shipping and expected arrival date questions.

Which of the following receivers would improve the application's ability to answer these questions?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Create a vector store that includes the company shipping policies and payment terms for all automotive parts

Option 2:
Create a feature store table with transaction_id as primary key that is populated with invoice data and expected delivery date

Option 3:
Provide examples data for expected arrival dates as a tuning dataset, then periodically fine-tune the model so that it has updated shipping information


Option 4:
Amend the chat prompt to input when the ordered was placed and instruct the model to add 14 days to that as no shipping method is expected to exceed 14 days

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
48

Skill Section: 
 
Question:  
Objective: Choose the appropriate Python package to extract document content from provided source data and format.
A Generative AI Engineer is building a RAG application that will rely on context retrieved from source documents that have been scanned and saved as image files in formats like .jpeg or .png.
They want to develop a solution using the least amount of lines of code.
Which Python package should be used to extract the text from the source documents?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
beautifulsoup

Option 2:
scrapy

Option 3:
pytesseract
 
Option 4:
pyquery

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
49

Skill Section: 
 
Question:  
Objective: Select the best LLM based on the attributes of the application to be developed
A Generative AI Engineer would like to build an application that can update a memo field that is about a paragraph long to just a single sentence gist that shows intent of the memo field, but fits into their application front end.
With which Natural Language Processing task category should they evaluate potential LLMs for this application?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
text2text Generation

Option 2:
Sentencizer

Option 3:
Text Classification

Option 4:
Summarization

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
50

Skill Section: 
Design Applications

Question:  
What is the primary goal of designing a prompt that elicits a specifically formatted response?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌Option 1:
To allow the model to generate random outputs

✅Option 2:
To ensure structured and predictable model responses

❌Option 3:
To maximize computation time

❌Option 4:
To bypass model constraints

🔹 Explanation: 
A well-structured prompt ensures that the model outputs follow a consistent format, making it easier to parse and use the responses.


References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
51

Skill Section: 
Design Applications

Question:  
Which of the following best describes chain components in a Gen AI pipeline?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Individual AI models that function independently

❌ Option 2:
Pre-built APIs for random data retrieval

✅ Option 3:
Modules that transform inputs and pass outputs in a sequence

❌ Option 4:
Static SQL queries

🔹 Explanation: Chain components define how the input moves through different transformations in an AI pipeline.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
52

Skill Section: 
Design Applications

Question:  
When designing an AI pipeline, how should business use case goals be translated?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Directly into SQL queries

✅ Option 2:
Into a description of the desired inputs and outputs for the model

❌ Option 3:
By selecting a random model and testing outputs

❌ Option 4:
By increasing API latency

🔹 Explanation: 
Business goals should be mapped to clear inputs and outputs to ensure the AI pipeline aligns with objectives.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
54

Skill Section: 
Design Applications

Question:  
Which of the following should be considered when selecting model tasks for a given business requirement?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Model name only

❌ Option 2:
Token cost and API latency

❌ Option 3:
The complexity of the model’s training data

✅ Option 4:
The model’s ability to generate relevant outputs based on task needs

🔹 Explanation: 
The model must align with the task requirements to produce useful results.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


Number: 
55

Skill Section: 
Design Applications

Question:  
What is the purpose of defining tools in a multi-stage reasoning Gen AI application?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
To reduce inference time

✅ Option 2:
To gather knowledge and take actions across steps

❌ Option 3:
To avoid using a model altogether

❌ Option 4:
To create unnecessary computational steps

🔹 Explanation: Multi-stage reasoning applications use tools to enhance knowledge retrieval and decision-making.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
56

Skill Section: 
Data Preparation

Question:  
What is the purpose of chunking in data preparation for Gen AI applications?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
To reduce the overall number of documents in the dataset

✅ Option 2:
To optimize input length while maintaining contextual relevance

❌ Option 3:
To discard unnecessary documents

❌ Option 4:
To increase token count per prompt

🔹 Explanation: Chunking helps fit content within model token limits while keeping it meaningful.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
57

Skill Section: 
Data Preparation

Question:  
What is a key consideration when filtering extraneous content in source documents for RAG applications?

Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

✅ Option 1:
Reducing hallucinations by removing irrelevant data

❌ Option 2:
Maximizing document size for embedding efficiency

❌ Option 3:
Ignoring text structure in documents

❌ Option 4:
Ensuring all content is included regardless of quality

🔹 Explanation: Irrelevant data can lead to misleading or incorrect AI responses.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
58

Skill Section: 
Data Preparation

Question:  
Which Python package is best suited for extracting document content from PDFs in a Gen AI pipeline?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
NumPy

✅ Option 2:
PyMuPDF

❌ Option 3:
Matplotlib

❌ Option 4:
Seaborn

🔹 Explanation: PyMuPDF is a robust library for extracting text from PDFs.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
59

Skill Section: 
Data Preparation

Question:  
What is a best practice when writing chunked text into Delta Lake tables in Unity Catalog?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Storing all text in a single row

✅ Option 2:
Using structured formats and metadata to enhance retrieval

❌ Option 3:
Removing text metadata for efficiency

❌ Option 4:
Avoiding partitioning for simplicity

🔹 Explanation: Metadata helps with indexing and retrieval accuracy.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
60

Skill Section: 
Data Preparation

Question:  
What tool can be used to evaluate retrieval performance in a RAG application?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Inference latency monitor

✅ Option 2:
Retrieval quality metrics like MRR (Mean Reciprocal Rank)

❌ Option 3:
Random sampling of documents

❌ Option 4:
Tokenization logs

🔹 Explanation: MRR helps measure how well a retrieval system ranks relevant documents.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
61

Skill Section: 
Application Development

Question:  
Which of the following tools can be used to extract data for retrieval in a Gen AI application?

Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

✅ Option 1:
LangChain retrievers

❌ Option 2:
Random document selection

❌ Option 3:
Hardcoded SQL queries

❌ Option 4:
Static JSON files

🔹 Explanation: LangChain provides efficient retrieval components.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
62

Skill Section: 
Application Development

Question:  
How do different prompt formats affect model output?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
They do not affect output in any way

✅ Option 2:
They can guide model behavior to produce more relevant responses

❌ Option 3:
They reduce the model's token limit

❌ Option 4:
They cause the model to ignore training data

🔹 Explanation: Prompt design influences how models interpret and respond to queries.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
63

Skill Section: 
Application Development

Question:  
What is a common issue in model responses that should be assessed qualitatively?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Token efficiency

✅ Option 2:
Response hallucinations or safety concerns

❌ Option 3:
API cost

❌ Option 4:
Response time variability

🔹 Explanation: AI-generated content should be evaluated for correctness and reliability.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
64

Skill Section: 
Application Development

Question:  
Why is metaprompting used in Generative AI applications?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
To make prompts longer

❌ Option 2:
To reduce model reasoning ability

✅ Option 3:
To minimize hallucinations and improve response quality

❌ Option 4:
To increase API cost

🔹 Explanation: Metaprompts refine AI outputs by structuring input prompts effectively.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
65

Skill Section: 
Application Development

Question:  
What is an essential step in selecting an embedding model for a RAG application?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Ignoring context length

✅ Option 2:
Aligning model selection with document structure and retrieval needs

❌ Option 3:
Prioritizing the cheapest model available

❌ Option 4:
Using any random model from the model hub

🔹 Explanation: The right model should fit the specific use case.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
66

Skill Section: 
Assembling and Deploying Applications

Question:  
What is the purpose of a pyfunc model with pre- and post-processing in an AI chain?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
To adjust input/output formatting and apply transformations

Option 2:
To replace the need for model inference

Option 3:
To reduce the number of API calls

Option 4:
To increase the model's complexity unnecessarily

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
67

Skill Section: 
Assembling and Deploying Applications

Question:  
What is the correct sequence for deploying a basic RAG application?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Data preparation → Indexing → Model selection → Deployment

Option 2:
Deployment → Model selection → Indexing → Data preparation

Option 3:
Model selection → Indexing → Deployment → Data preparation

Option 4:
Indexing → Deployment → Data preparation → Model selection

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
68

Skill Section: 
Assembling and Deploying Applications

Question:  
How can an LLM application be served efficiently?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
By directly exposing the model API with no restrictions

Option 2:
By leveraging Foundation Model APIs with controlled access

Option 3:
By running the model locally on consumer hardware

Option 4:
By storing responses as static JSON files

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
69

Skill Section: 
Assembling and Deploying Applications

Question:  
 What is the primary function of a Vector Search index in a RAG application?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
To randomly retrieve documents

Option 2:
To optimize search and retrieval efficiency

Option 3:
To replace embeddings with raw text

Option 4:
To store models

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
70

Skill Section: 
Assembling and Deploying Applications

Question:  
What should be done after registering a model in Unity Catalog using MLflow?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Skip evaluation and deploy immediately

Option 2:
Monitor model performance with tracking features

Option 3:
Store models in an unstructured format

Option 4:
Use only the default configurations

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
71

Skill Section: 
Governance

Question:  
 What is a key technique for preventing malicious user inputs in a Gen AI application?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Prompt filtering and input validation

Option 2:
Allowing all user inputs for flexibility

Option 3:
Disabling all API logging

Option 4:
Increasing token length

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
72

Skill Section: 
Governance

Question:  
How can masking techniques be used to meet performance objectives?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
By removing sensitive data from model inputs and outputs

Option 2:
By increasing token count

Option 3:
By disabling embeddings

Option 4:
By storing responses locally

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
73

Skill Section: 
Governance

Question:  
Why is it essential to follow licensing requirements for data sources?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
To avoid legal risks associated with data usage

Option 2:
To increase model performance

Option 3:
To reduce data storage requirements

Option 4:
To improve embedding efficiency

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
74

Skill Section: 
Governance

Question:  
What is a common method to mitigate problematic text in RAG applications?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Filtering out inappropriate or biased content

Option 2:
Increasing the number of tokens per request

Option 3:
Disabling logging

Option 4:
Using random sampling

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
75

Skill Section: 
Governance

Question:  
What is the primary reason for implementing guardrails in Gen AI applications?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
To ensure responsible AI usage and prevent harmful outputs

Option 2:
To increase API response time

Option 3:
To reduce cloud storage costs

Option 4:
To remove model interpretability

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
76

Skill Section: 
Evaluation and Monitoring

Question:  
What is a key metric to monitor in an LLM deployment scenario?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Model accuracy and latency

Option 2:
Cloud storage size

Option 3:
Random response sampling

Option 4:
API request cost only

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
77

Skill Section: 
Evaluation and Monitoring

Question:  
Why is inference logging important for monitoring a deployed RAG application?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
To analyze response quality and troubleshoot errors

Option 2:
To increase latency

Option 3:
To store random API responses

Option 4:
To reduce model explainability

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
78

Skill Section: 
Evaluation and Monitoring

Question:  
How does MLflow help in evaluating model performance?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
By tracking metrics and logging experiments

Option 2:
By increasing API rate limits

Option 3:
By restricting model usage

Option 4:
By disabling retrieval functions

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
79

Skill Section: 
Evaluation and Monitoring

Question:  
How can Databricks features help control LLM costs?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
By optimizing compute resources and inference settings

Option 2:
By disabling embeddings

Option 3:
By reducing token length randomly
 
Option 4:
By limiting API access

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
80

Skill Section: 
Evaluation and Monitoring

Question:  
What is the first step in evaluating an LLM model for a specific use case?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
Running quantitative and qualitative tests

Option 2:
Deploying without evaluation

Option 3:
Increasing token count

Option 4:
Skipping retrieval analysis

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
81

Skill Section: 
Assembling and Deploying Applications 

Question:  
What is the purpose of coding a chain using a pyfunc model with pre- and post-processing?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Pyfunc does not speed up processing directly.

✅ Option 2:
To customize input and output transformations for model inference

❌ Option 3:
It does not store model embeddings.

❌ Option 4:
Not limited to training workflows

🔹 Explanation: A pyfunc model allows adding pre-processing (formatting, cleaning) and post-processing (filtering, structuring) before and after model inference.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Number: 
82

Skill Section: 
Assembling and Deploying Applications

Question:  
What is a key factor when controlling access to resources from model serving endpoints?

Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

✅ Option 1:
 Implementing authentication and authorization controls

❌ Option 2:
Model accuracy

❌ Option 3:
data size,

❌ Option 4:
embedding context do not control access

🔹 Explanation: Role-based access control (RBAC) and API authentication secure model endpoints.


References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
83

Skill Section: 
Assembling and Deploying Applications

Question:  
How can LangChain be used in coding a simple chain?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
LangChain does not replace MLflow.

❌ Option 2:
It is not limited to data filtering.

✅ Option 3:
By defining a sequence of steps that process and retrieve data

❌ Option 4:
Not only used for visualization.

🔹 Explanation: LangChain enables structured processing of AI model inputs/outputs.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
84

Skill Section: 
Assembling and Deploying Applications

Question:  
What are the key elements needed to create a RAG application?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Metadata

❌ Option 2:
file type 

❌ Option 3:
 
✅ Option 4:
Model flavor, embedding model, retriever, dependencies, input examples, model signature

🔹 Explanation: RAG (Retrieval-Augmented Generation) applications require structured components for retrieval and response generation.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
85

Skill Section: 
Assembling and Deploying Applications

Question:  
What is required to deploy an endpoint for a basic RAG application

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
 
✅ Option 2:
Registering the model, creating an API, and setting up access

❌ Option 3:
Creating vector embeddings is part of preparation, not deploymen

❌ Option 4:

🔹 Explanation: Deploying involves model registration, API hosting, and secure access setup.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
86

Skill Section: 
Governance

Question:  
How can masking techniques be used in a Gen AI application?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
 
✅ Option 2:
 To redact sensitive information before generating responses

❌ Option 3:
Masking does not impact storage, token limits, or hallucinations.

❌ Option 4:


🔹 Explanation: Masking removes or replaces sensitive data like PII (Personally Identifiable Information).

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
87

Skill Section: 
Governance

Question:  
What is a common guardrail technique to prevent malicious inputs?

Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

✅ Option 1:
Input sanitization and validation

❌ Option 2:
Reducing API latency or encryption does not mitigate attacks.

❌ Option 3:
Reducing API latency or encryption does not mitigate attacks.

❌ Option 4:
Reducing API latency or encryption does not mitigate attacks.

🔹 Explanation: Sanitization filters out harmful queries to prevent prompt injection attacks.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
88

Skill Section: 
Governance

Question:  
 Why is legal compliance important in selecting data sources for a RAG application?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Avoiding legal risks does not mean rejecting all third-party data.

❌ Option 2:
Avoiding legal risks does not mean rejecting all third-party data.

❌ Option 3:
Avoiding legal risks does not mean rejecting all third-party data.

✅ Option 4:
To avoid copyright infringement and data privacy violations

🔹 Explanation: Legal compliance ensures licensed, ethical AI training and usage.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
89

Skill Section: 
Governance

Question:  
How can problematic text in a RAG application be mitigated?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
 
❌ Option 2:
 
✅ Option 3:
Implementing a filtering or re-ranking approach to improve response quality

❌ Option 4:

🔹 Explanation: Filtering removes misleading or biased AI outputs.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
90

Skill Section: 
Governance

Question:  
What is a best practice to handle user inputs that may lead to undesirable AI behavior?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Reducing token usage does not ensure safety.

✅ Option 2:
Using structured prompts and safety checks

❌ Option 3:
Reducing token usage does not ensure safety.

❌ Option 4:
Reducing token usage does not ensure safety.


🔹 Explanation: Structured prompts prevent offensive or misleading responses.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
91

Skill Section: 
Evaluation and Monitoring 

Question:  
Which metric is best for evaluating retrieval performance in a RAG application?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
 
❌ Option 2:
 
✅ Option 3:
Mean Reciprocal Rank (MRR)

❌ Option 4:

🔹 Explanation: MRR measures how well the retrieval system ranks relevant documents.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
92

Skill Section: 
Evaluation and Monitoring

Question:  
Why is inference logging important for deployed RAG applications?

Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

✅ Option 1:
To track model response quality and detect anomalies

❌ Option 2:
It is not primarily for debugging or cost tracking.

❌ Option 3:
It is not primarily for debugging or cost tracking.

❌ Option 4:
It is not primarily for debugging or cost tracking.


🔹 Explanation: Inference logging records AI outputs to identify errors or drifts.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
93

Skill Section: 
Evaluation and Monitoring

Question:  
What is an effective way to control LLM costs in Databricks?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Running multiple small models or avoiding retrievers increases costs.

✅ Option 2:
Optimize prompt length and batch processing

❌ Option 3:
Running multiple small models or avoiding retrievers increases costs.

❌ Option 4:
Running multiple small models or avoiding retrievers increases costs.

🔹 Explanation: Reducing prompt size and batching reduce token consumption.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
94

Skill Section: 
Evaluation and Monitoring

Question:  
What should be monitored to ensure stable model performance over time?

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Cost and speed do not indicate performance quality.

❌ Option 2:
Cost and speed do not indicate performance quality.

✅ Option 3:
Model drift and response consistency

❌ Option 4:
Cost and speed do not indicate performance quality.


🔹 Explanation: Drift occurs when model outputs become less relevant over time.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
95

Skill Section: 
Evaluation and Monitoring

Question:  
How can MLflow be used to evaluate a RAG model?

Answer: 
4

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
MLflow does not modify LLM settings or token usage directly

❌ Option 2:
MLflow does not modify LLM settings or token usage directly

❌ Option 3:
MLflow does not modify LLM settings or token usage directly

✅ Option 4:
Tracking retrieval metrics and response quality over multiple runs

🔹 Explanation: MLflow logs retrieval success rates and AI output quality.

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
96

Skill Section: 
Design Applications

Question:  
How can you ensure a language model returns responses in JSON format?

prompt = """
Provide the response in JSON format:
{{
    "answer": "<your answer>",
    "confidence": <your confidence score>
}}
Question: What is the capital of France?
"""


Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Let the model decide the response format

✅ Option 2:
Use explicit formatting instructions within curly braces {}

❌ Option 3:
Use XML instead of JSON 
 

❌ Option 4:
Only mention "Answer in JSON"

🔹 Explanation: Providing an explicit structured format ensures the model returns JSON.
Wrong Options:
a) Let the model decide the response format (Leads to inconsistent outputs).
c) Use XML instead of JSON (Most AI models prefer JSON).
d) Only mention "Answer in JSON" (Lacks structure).

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
97

Skill Section: 
Design Applications

Question:  
Which LangChain component allows chaining multiple LLM calls?

from langchain.chains import SequentialChain


Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
SimpleChain

❌ Option 2:
Calling models separately

✅ Option 3:
SequentialChain

❌ Option 4:
Storing intermediate outputs in files 

🔹 Explanation: This class chains multiple LLM calls, passing outputs forward.
Wrong Options:

a) SimpleChain (Not designed for multi-step tasks).
b) Calling models separately (Lacks automation).
d) Storing intermediate outputs in files (Inefficient for AI pipelines).

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
98

Skill Section: 
Design Applications

Question:  
How do you integrate an LLM with external tools in LangChain?

from langchain.agents import initialize_agent


Answer: 
1

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

✅ Option 1:
Use initialize_agent and pass tools

❌ Option 2:
 
❌ Option 3:
 
❌ Option 4:

🔹 Explanation: initialize_agent allows the model to use external tools dynamically.
❌ Wrong Options:

b) Define tools but don’t pass them (Tools won’t be used).
c) Use a basic LLM without tools (Reduces AI capability).
d) Manually execute tool calls (Not dynamic).

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
99

Skill Section: 
Data Preparation

Question:  
How do you extract text from a PDF file using Python?

Answer: 
2

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
PIL

✅ Option 2:
Use pdfplumber

❌ Option 3:
openpyxl

❌ Option 4:
requests 

🔹 Explanation: pdfplumber extracts structured text from PDFs.
Wrong Options:

a) PIL (Used for images).
c) openpyxl (For Excel files).
d) requests (For web data).


References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
100

Skill Section: 
Application Development

Question:  
How do you secure model endpoints?

Answer: 
3

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

❌ Option 1:
Allow open access 

❌ Option 2:
Use API keys only

✅ Option 3:
Use OAuth authentication

❌ Option 4:
Disable authentication


🔹 Explanation: OAuth ensures secure access control.
Wrong Options:

a) Allow open access (Security risk).
b) Use API keys only (Less secure than OAuth).
d) Disable authentication (Unsafe).

References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Number: 
81

Skill Section: 
 
Question:  

Answer: 

Difficulty Level ( Easy, Medium, Intense ):
M

Guideline Time in Seconds (30, 45, 60):
45

Option 1:
 
Option 2:
 
Option 3:
 
Option 4:


References:

Hands-On: 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
1. Foundations of Generative AI
Q1: What is the main benefit of using a Generative AI model over traditional machine learning models?

A: It can generate new, unseen data based on training.
B: It can only classify data.
C: It requires less data to train.
D: It only predicts based on historical data.
Answer: A
Explanation: Generative AI models are capable of creating new data (text, images, etc.) by learning from the input data, unlike traditional models that focus on classification or regression tasks.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q2: Which of the following is NOT a primary task of a Generative AI model?

A: Text generation
B: Data augmentation
C: Image classification
D: Image generation
Answer: C
Explanation: Image classification is a task for discriminative models, while generative models focus on generating new content (e.g., text, images).

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q3: Which of the following is a popular architecture used in Generative AI for text generation?

A: RNN
B: GAN
C: Transformer
D: CNN
Answer: C
Explanation: The Transformer architecture is commonly used in Generative AI, especially for tasks like text generation, due to its parallel processing capabilities.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q4: In the context of Generative AI, what does the term "latent space" refer to?

A: A hidden, intermediate feature representation
B: A data preprocessing technique
C: A model's output layer
D: A loss function in the model
Answer: A
Explanation: "Latent space" is a representation of input data in a compressed form that the model uses to learn to generate new data.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q5: Which Generative AI method is used to generate new content by learning from the statistical properties of the input data?

A: Variational Autoencoders (VAE)
B: Support Vector Machines (SVM)
C: K-Nearest Neighbors (KNN)
D: Decision Trees
Answer: A
Explanation: VAEs are used in Generative AI to learn the distribution of the input data and generate new content based on that learned distribution.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q6: Which of the following is an example of a Generative AI model?

A: BERT
B: GPT-3
C: SVM
D: Random Forest
Answer: B
Explanation: GPT-3 is a generative model designed for text generation tasks, while BERT is a transformer-based model focused on understanding text.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q7: What is a key advantage of transformer models in Generative AI?

A: They excel at sequential data tasks.
B: They are efficient for large-scale parallel processing.
C: They are simpler to train compared to RNNs.
D: They require minimal data for training.
Answer: B
Explanation: Transformers can process data in parallel, making them more efficient for large datasets and tasks like text generation.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q8: What kind of task would a GAN (Generative Adversarial Network) be used for?

A: Predicting future trends
B: Generating new images or videos
C: Classifying email spam
D: Clustering customer data
Answer: B
Explanation: GANs are used in generative tasks like creating new images or videos based on training data.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q9: Which of the following is an essential component of a GAN?

A: Generator and Discriminator
B: Encoder and Decoder
C: Input layer and Output layer
D: Convolutional and Pooling layers
Answer: A
Explanation: A GAN consists of two components: a generator, which creates new data, and a discriminator, which evaluates the authenticity of the generated data.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q10: What is the key difference between unsupervised and supervised learning in the context of Generative AI?

A: Unsupervised learning generates new data while supervised learning only predicts based on existing data.
B: Unsupervised learning requires labeled data while supervised does not.
C: Unsupervised learning generates new data by learning from labeled data.
D: There is no difference.
Answer: A
Explanation: In unsupervised learning, the model learns from unlabeled data and can generate new data, while supervised learning typically focuses on predicting outcomes based on labeled data.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2. Databricks for AI and Machine Learning
Q1: Which of the following is NOT a feature of Databricks?

A: Managed Spark clusters
B: Easy integration with machine learning libraries like TensorFlow and PyTorch
C: Seamless scalability of AI models
D: Image generation using pre-built models
Answer: D
Explanation: While Databricks provides tools for machine learning and AI, it doesn't specifically focus on pre-built image generation models.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q2: What is the main advantage of using Databricks over other platforms for AI/ML?

A: It supports only Python.
B: It offers integrated Spark clusters for big data processing.
C: It doesn't require cloud infrastructure.
D: It is only for data engineers.
Answer: B
Explanation: Databricks offers managed Spark clusters, which is a big advantage for handling large-scale data and running distributed machine learning tasks.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q3: Which tool in Databricks is used to create and manage machine learning models?

A: MLflow
B: Databricks SQL Analytics
C: Delta Lake
D: Databricks Notebooks
Answer: A
Explanation: MLflow is a machine learning lifecycle management tool in Databricks used for model tracking, experimentation, and deployment.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q4: How does Databricks help scale AI models for production environments?

A: By allowing models to run in serverless environments
B: By using GPU acceleration only for data processing
C: By limiting the number of concurrent jobs
D: By providing an easy-to-use API for machine learning only
Answer: A
Explanation: Databricks allows you to scale AI models by providing serverless clusters and leveraging GPU acceleration for efficient model training and deployment.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q5: In Databricks, what is the primary purpose of Delta Lake?

A: It stores models for reuse.
B: It handles data engineering pipelines.
C: It manages scalable, reliable data lakes.
D: It performs real-time inference.
Answer: C
Explanation: Delta Lake provides a reliable, scalable data lake solution with ACID transactions, which is critical for AI/ML workflows.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q6: Which of the following is true about the integration of Databricks with MLflow?

A: MLflow is only used for experimentation and tracking models.
B: Databricks provides direct access to MLflow for managing models in a production environment.
C: MLflow is used for orchestrating data pipelines.
D: Databricks cannot be integrated with MLflow.
Answer: B
Explanation: MLflow is deeply integrated with Databricks, allowing for model tracking, versioning, and managing machine learning experiments.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q7: How does Databricks ensure security for machine learning models?

A: By offering encryption options for data in transit and at rest.
B: By restricting the usage of machine learning libraries.
C: By monitoring AI models' decision-making behavior.
D: By providing only pre-built models.
Answer: A
Explanation: Databricks provides robust security features, including data encryption and access control to ensure that machine learning models are securely managed.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q8: What does Databricks' integration with Apache Spark allow users to do?

A: Only handle batch processing.
B: Process and analyze large-scale datasets for machine learning.
C: Build models using only SQL.
D: Automatically tune AI models for optimization.
Answer: B
Explanation: Apache Spark in Databricks allows users to process and analyze large-scale datasets, enabling efficient model training for machine learning tasks.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q9: What is the primary feature of Databricks that enables collaboration on AI projects?

A: Integrated Notebooks for collaborative development.
B: Automatic model deployment pipelines.
C: Integrated GPU support for training.
D: Data pipeline automation.
Answer: A
Explanation: Databricks Notebooks allow data scientists, engineers, and business analysts to collaborate on projects, track experiments, and visualize results.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q10: Which of the following best describes the role of the Databricks Unified Analytics Platform?

A: It provides an integrated environment for all stages of AI model development, including data ingestion, processing, training, and deployment.
B: It provides a specific focus on data engineering only.
C: It only supports machine learning tasks.
D: It is mainly focused on business analytics.
Answer: A
Explanation: The Databricks Unified Analytics Platform supports the entire machine learning lifecycle, from data preparation to model deployment and monitoring.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
3. Building RAG (Retrieve and Generate) Applications
Q1: What is the main benefit of using a RAG architecture in a chatbot application?

A: It speeds up the model's inference time.
B: It combines retrieval and generation for more accurate responses.
C: It generates all possible answers to any question.
D: It focuses only on retrieval without generating new content.
Answer: B
Explanation: The RAG architecture combines the retrieval of relevant documents with the generation of responses, improving the accuracy and relevance of the output.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q2: In the RAG model, which component is responsible for fetching relevant information from a knowledge base?

A: The generator
B: The retriever
C: The evaluator
D: The classifier
Answer: B
Explanation: In a RAG model, the retriever component searches the knowledge base for relevant documents or information to pass to the generator.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q3: Which of the following best describes a "context-augmented prompt" in a RAG architecture?

A: A prompt that is modified based on the retrieved information.
B: A prompt that requires no additional data to generate a response.
C: A prompt used for fine-tuning models.
D: A prompt used to summarize long documents.
Answer: A
Explanation: A context-augmented prompt incorporates additional information from the retrieved documents to help the generator produce a more relevant response.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q4: What is the role of the retrieval model in a RAG application?

A: To answer questions directly without external input.
B: To query a database or document store and retrieve relevant context for the question.
C: To generate new content from scratch based on the query.
D: To classify and label data based on questions.
Answer: B
Explanation: The retrieval model queries a document store to retrieve relevant context, which is then used by the generation model to formulate a response.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q5: What is the best way to ensure that a RAG-based chatbot only returns relevant responses?

A: Increase the model's response randomness.
B: Ensure high-quality and contextually accurate documents are retrieved.
C: Limit the model's training data.
D: Use a simple response-generating model.
Answer: B
Explanation: Ensuring that the retrieved documents are of high quality and contextually accurate is essential for generating relevant responses.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q6: Which model would most likely be used in the generation step of a RAG architecture?

A: GPT-3
B: BERT
C: Decision Trees
D: KMeans
Answer: A
Explanation: GPT-3 is a generative model that can generate natural language responses based on the retrieved context in a RAG architecture.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q7: Which of the following is a potential challenge in implementing a RAG application?

A: Difficulty in generating diverse responses.
B: Ensuring that the retrieval model finds accurate and relevant information.
C: Limiting the model's understanding of the question.
D: Training models without using any data.
Answer: B
Explanation: Ensuring that the retrieval model accurately retrieves relevant and up-to-date information is a key challenge in RAG applications.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q8: How can a RAG model improve the accuracy of its generated responses?

A: By using a larger context window during the retrieval phase.
B: By training the model with more data.
C: By reducing the amount of generated content.
D: By always using the same data for both retrieval and generation.
Answer: A
Explanation: Using a larger context window during retrieval allows the model to gather more relevant information, improving the accuracy of the generated responses.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Q9: How does a RAG model deal with new, unseen information that isn't stored in the knowledge base?

A: By ignoring such queries.
B: By generating answers based on the general knowledge embedded in the model.
C: By sending the query to a human operator.
D: By retrieving historical data only.
Answer: B
Explanation: If a RAG model encounters new information, it may generate an answer based on the knowledge embedded in its training data, even if that information is not in the knowledge base.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Q10: What are the primary benefits of using a RAG architecture in conversational AI systems?

A: Enhanced speed and accuracy due to real-time retrieval.
B: Limited training data required.
C: The ability to generate creative content only.
D: Simplified implementation.
Answer: A
Explanation: RAG architecture improves the speed and accuracy of conversational AI systems by combining retrieval of relevant context with generation of responses in real time.
