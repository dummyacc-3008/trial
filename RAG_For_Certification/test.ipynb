{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a2724c",
   "metadata": {},
   "source": [
    "RAG for Semantic kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed138ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c363f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LakshyaBansal\\Desktop\\Python vs code\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#For extraction\n",
    "import fitz  # PyMuPDF\n",
    "from typing import List\n",
    "import pickle\n",
    "#For Embedding \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "#Vector Indexing\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d39abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell us about your PDF experience.\n",
      "Introduction to Semantic Kernel\n",
      "Article ‚Ä¢ 06/24/2024\n",
      "Semantic Kernel is a lightweight, open-source development kit that lets you easily build\n",
      "AI agents and integrate the latest AI models into your C#, Python, or Java codebase. It\n",
      "serves as an efficient middleware that enables rapid delivery of enterprise-grade\n",
      "solutions.\n",
      "Microsoft and other Fortune 500 companies are already leveraging Semantic Kernel\n",
      "because it‚Äôs flexible, modular, and observable. Backed with security enhancing\n",
      "capabilities like telemetry support, and hooks and filters so you‚Äôll feel confident you‚Äôre\n",
      "delivering responsible AI solutions at scale.\n",
      "Version 1.0+ support across C#, Python, and Java means it‚Äôs reliable, committed to non\n",
      "breaking changes. Any existing chat-based APIs are easily expanded to support\n",
      "additional modalities like voice and video.\n",
      "Semantic Kernel was designed to be future proof, easily connecting your code to the\n",
      "latest AI models evolving with the technology as it advances. When new models are\n",
      "released, you‚Äôll simply swap them out without needing to rewrite your entire codebase.\n",
      "Enterprise ready\n",
      "Semantic Kernel combines prompts with existing APIs to perform actions. By describing\n",
      "your existing code to AI models, they‚Äôll be called to address requests. When a request is\n",
      "made the model calls a function, and Semantic Kernel is the middleware translating the\n",
      "model's request to a function call and passes the results back to the model.\n",
      "By adding your existing code as a plugin, you‚Äôll maximize your investment by flexibly\n",
      "integrating AI services through a set of out-of-the-box connectors. Semantic Kernel uses\n",
      "OpenAPI specifications (like Microsoft 365 Copilot) so you can share any extensions with\n",
      "other pro or low-code developers in your company.\n",
      "Automating business processes\n",
      "Modular and extensible\n",
      "Now that you know what Semantic Kernel is, get started with the quick start guide. You‚Äôll\n",
      "build agents that automatically call functions to perform actions faster than any other\n",
      "SDK out there.\n",
      "Get started\n",
      "Quickly get started\n",
      "Getting started with Semantic Kernel\n",
      "Article ‚Ä¢ 11/08/2024\n",
      "In just a few steps, you can build your first AI agent with Semantic Kernel in either\n",
      "Python, .NET, or Java. This guide will show you how to...\n",
      "Install the necessary packages\n",
      "Create a back-and-forth conversation with an AI\n",
      "Give an AI agent the ability to run your code\n",
      "Watch the AI create plans on the fly\n",
      "Semantic Kernel has several NuGet packages available. For most scenarios, however, you\n",
      "typically only need Microsoft.SemanticKernel .\n",
      "You can install it using the following command:\n",
      "Bash\n",
      "For the full list of Nuget packages, please refer to the supported languages article.\n",
      "If you're a Python or C# developer, you can quickly get started with our notebooks.\n",
      "These notebooks provide step-by-step guides on how to use Semantic Kernel to build\n",
      "AI agents.\n",
      "Installing the SDK\n",
      "dotnet add package Microsoft.SemanticKernel\n",
      "Quickly get started with notebooks\n",
      "To get started, follow these steps:\n",
      "1. Clone the Semantic Kernel repo\n",
      "2. Open the repo in Visual Studio Code\n",
      "3. Navigate to _/dotnet/notebooks\n",
      "4. Open 00-getting-started.ipynb to get started setting your environment and\n",
      "creating your first AI agent!\n",
      "1. Create a new .NET Console project using this command:\n",
      "Bash\n",
      "2. Install the following .NET dependencies:\n",
      "Bash\n",
      "3. Replace the content of the Program.cs  file with this code:\n",
      "Writing your first console app\n",
      "dotnet new console\n",
      "dotnet add package Microsoft.SemanticKernel\n",
      "dotnet add package Microsoft.Extensions.Logging\n",
      "dotnet add package Microsoft.Extensions.Logging.Console\n",
      "C#\n",
      "// Import packages\n",
      "using Microsoft.Extensions.DependencyInjection;\n",
      "using Microsoft.Extensions.Logging;\n",
      "using Microsoft.SemanticKernel;\n",
      "using Microsoft.SemanticKernel.ChatCompletion;\n",
      "using Microsoft.SemanticKernel.Connectors.OpenAI;\n",
      "// Populate values from your OpenAI deployment\n",
      "var modelId = \"\";\n",
      "var endpoint = \"\";\n",
      "var apiKey = \"\";\n",
      "// Create a kernel with Azure OpenAI chat completion\n",
      "var builder = Kernel.CreateBuilder().AddAzureOpenAIChatCompletion(modelId, \n",
      "endpoint, apiKey);\n",
      "// Add enterprise components\n",
      "builder.Services.AddLogging(services => \n",
      "services.AddConsole().SetMinimumLevel(LogLevel.Trace));\n",
      "// Build the kernel\n",
      "Kernel kernel = builder.Build();\n",
      "var chatCompletionService = \n",
      "kernel.GetRequiredService<IChatCompletionService>();\n",
      "// Add a plugin (the LightsPlugin class is defined below)\n",
      "kernel.Plugins.AddFromType<LightsPlugin>(\"Lights\");\n",
      "// Enable planning\n",
      "OpenAIPromptExecutionSettings openAIPromptExecutionSettings = new()¬†\n",
      "{\n",
      "    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()\n",
      "};\n",
      "// Create a history store the conversation\n",
      "var history = new ChatHistory();\n",
      "// Initiate a back-and-forth chat\n",
      "string? userInput;\n",
      "do {\n",
      "    // Collect user input\n",
      "    Console.Write(\"User > \");\n",
      "    userInput = Console.ReadLine();\n",
      "    // Add user input\n",
      "    history.AddUserMessage(userInput);\n",
      "    // Get the response from the AI\n",
      "    var result = await chatCompletionService.GetChatMessageContentAsync(\n",
      "        history,\n",
      "        executionSettings: openAIPromptExecutionSettings,\n",
      "        kernel: kernel);\n",
      "The following back-and-forth chat should be similar to what you see in the console. The\n",
      "function calls have been added below to demonstrate how the AI leverages the plugin\n",
      "behind the scenes.\n",
      "Role\n",
      "Message\n",
      "üîµ¬†User\n",
      "Please toggle the light\n",
      "üî¥¬†Assistant¬†(function¬†call)\n",
      "LightsPlugin.GetState()\n",
      "üü¢¬†Tool\n",
      "off\n",
      "üî¥¬†Assistant¬†(function¬†call)\n",
      "LightsPlugin.ChangeState(true)\n",
      "üü¢¬†Tool\n",
      "on\n",
      "üî¥¬†Assistant\n",
      "The light is now on\n",
      "If you're interested in understanding more about the code above, we'll break it down in\n",
      "the next section.\n",
      "To make it easier to get started building enterprise apps with Semantic Kernel, we've\n",
      "created a step-by-step that guides you through the process of creating a kernel and\n",
      "using it to interact with AI services.\n",
      "    // Print the results\n",
      "    Console.WriteLine(\"Assistant > \" + result);\n",
      "    // Add the message from the agent to the chat history\n",
      "    history.AddMessage(result.Role, result.Content ?? string.Empty);\n",
      "} while (userInput is not null);\n",
      "Ôæâ\n",
      "Expand table\n",
      "Understanding the code\n",
      "In the following sections, we'll unpack the above sample by walking through steps 1, 2,\n",
      "3, 4, 6, 9, and 10. Everything you need to build a simple agent that is powered by an AI\n",
      "service and can run your code.\n",
      "Import packages\n",
      "Add AI services\n",
      "Enterprise components ::: zone-end\n",
      "Build the kernel\n",
      "Add memory (skipped)\n",
      "Add plugins\n",
      "Create kernel arguments (skipped)\n",
      "Create prompts (skipped)\n",
      "Planning\n",
      "Invoke\n",
      "For this sample, we first started by importing the following packages:\n",
      "C#\n",
      "Afterwards, we add the most important part of a kernel: the AI services that you want to\n",
      "use. In this example, we added an Azure OpenAI chat completion service to the kernel\n",
      "builder.\n",
      "C#\n",
      "1) Import packages\n",
      "using Microsoft.SemanticKernel;\n",
      "using Microsoft.SemanticKernel.ChatCompletion;\n",
      "using Microsoft.SemanticKernel.Connectors.OpenAI;\n",
      "2) Add AI services\n",
      "Ôºó Note\n",
      "In this example, we used Azure OpenAI, but you can use any other chat completion\n",
      "service. To see the full list of supported services, refer to the supported languages\n",
      "article. If you need help creating a different service, refer to the AI services article.\n",
      "There, you'll find guidance on how to use OpenAI or Azure OpenAI models as\n",
      "services.\n",
      "One of the main benefits of using Semantic Kernel is that it supports enterprise-grade\n",
      "services. In this sample, we added the logging service to the kernel to help debug the AI\n",
      "agent.\n",
      "C#\n",
      "Once the services have been added, we then build the kernel and retrieve the chat\n",
      "completion service for later use.\n",
      "C#\n",
      "With plugins, can give your AI agent the ability to run your code to retrieve information\n",
      "from external sources or to perform actions. In the above example, we added a plugin\n",
      "that allows the AI agent to interact with a light bulb. Below, we'll show you how to\n",
      "create this plugin.\n",
      "Below, you can see that creating a native plugin is as simple as creating a new class.\n",
      "In this example, we've created a plugin that can manipulate a light bulb. While this is a\n",
      "simple example, this plugin quickly demonstrates how you can support both...\n",
      "// Create kernel\n",
      "var builder = Kernel.CreateBuilder()\n",
      "builder.AddAzureOpenAIChatCompletion(modelId, endpoint, apiKey);\n",
      "3) Add enterprise services\n",
      "builder.Services.AddLogging(services => \n",
      "services.AddConsole().SetMinimumLevel(LogLevel.Trace));\n",
      "4) Build the kernel and retrieve services\n",
      "Kernel kernel = builder.Build();\n",
      "// Retrieve the chat completion service\n",
      "var chatCompletionService = \n",
      "kernel.Services.GetRequiredService<IChatCompletionService>();\n",
      "6) Add plugins\n",
      "Create a native plugin\n",
      "1. Retrieval Augmented Generation (RAG) by providing the AI agent with the state of\n",
      "the light bulb\n",
      "2. And task automation by allowing the AI agent to turn the light bulb on or off.\n",
      "In your own code, you can create a plugin that interacts with any external service or API\n",
      "to achieve similar results.\n",
      "C#\n",
      "using System.ComponentModel;\n",
      "using System.Text.Json.Serialization;\n",
      "using Microsoft.SemanticKernel;\n",
      "public class LightsPlugin\n",
      "{\n",
      "   // Mock data for the lights\n",
      "   private readonly List<LightModel> lights = new()\n",
      "   {\n",
      "      new LightModel { Id = 1, Name = \"Table Lamp\", IsOn = false },\n",
      "      new LightModel { Id = 2, Name = \"Porch light\", IsOn = false },\n",
      "      new LightModel { Id = 3, Name = \"Chandelier\", IsOn = true }\n",
      "   };\n",
      "   [KernelFunction(\"get_lights\")]\n",
      "   [Description(\"Gets a list of lights and their current state\")]\n",
      "   [return: Description(\"An array of lights\")]\n",
      "   public async Task<List<LightModel>> GetLightsAsync()\n",
      "   {\n",
      "      return lights;\n",
      "   }\n",
      "   [KernelFunction(\"change_state\")]\n",
      "   [Description(\"Changes the state of the light\")]\n",
      "   [return: Description(\"The updated state of the light; will return null if \n",
      "the light does not exist\")]\n",
      "   public async Task<LightModel?> ChangeStateAsync(int id, bool isOn)\n",
      "   {\n",
      "      var light = lights.FirstOrDefault(light => light.Id == id);\n",
      "      if (light == null)\n",
      "      {\n",
      "         return null;\n",
      "      }\n",
      "      // Update the light with the new state\n",
      "      light.IsOn = isOn;\n",
      "      return light;\n",
      "   }\n",
      "}\n",
      "public class LightModel\n",
      "{\n",
      "Once you've created your plugin, you can add it to the kernel so the AI agent can access\n",
      "it. In the sample, we added the LightsPlugin  class to the kernel.\n",
      "C#\n",
      "Semantic Kernel leverages function calling‚Äìa native feature of most LLMs‚Äìto provide\n",
      "planning. With function calling, LLMs can request (or call) a particular function to satisfy\n",
      "a user's request. Semantic Kernel then marshals the request to the appropriate function\n",
      "in your codebase and returns the results back to the LLM so the AI agent can generate a\n",
      "final response.\n",
      "To enable automatic function calling, we first need to create the appropriate execution\n",
      "settings so that Semantic Kernel knows to automatically invoke the functions in the\n",
      "kernel when the AI agent requests them.\n",
      "C#\n",
      "Finally, we invoke the AI agent with the plugin. The sample code demonstrates how to\n",
      "generate a non-streaming response, but you can also generate a streaming response by\n",
      "   [JsonPropertyName(\"id\")]\n",
      "   public int Id { get; set; }\n",
      "   [JsonPropertyName(\"name\")]\n",
      "   public string Name { get; set; }\n",
      "   [JsonPropertyName(\"is_on\")]\n",
      "   public bool? IsOn { get; set; }\n",
      "}\n",
      "Add the plugin to the kernel\n",
      "// Add the plugin to the kernel\n",
      "kernel.Plugins.AddFromType<LightsPlugin>(\"Lights\");\n",
      "9) Planning\n",
      "OpenAIPromptExecutionSettings openAIPromptExecutionSettings = new()\n",
      "{\n",
      "    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()\n",
      "};\n",
      "10) Invoke\n",
      "using the GetStreamingChatMessageContentAsync  method.\n",
      "C#\n",
      "Run the program using this command:\n",
      "Bash\n",
      "In this guide, you learned how to quickly get started with Semantic Kernel by building a\n",
      "simple AI agent that can interact with an AI service and run your code. To see more\n",
      "examples and learn how to build more complex AI agents, check out our in-depth\n",
      "samples.\n",
      "// Create chat history\n",
      "var history = new ChatHistory();\n",
      "// Get the response from the AI\n",
      "var result = await chatCompletionService.GetChatMessageContentAsync(\n",
      "    history,\n",
      "    executionSettings: openAIPromptExecutionSettings,\n",
      "    kernel: kernel\n",
      ");\n",
      "dotnet run\n",
      "Next steps\n",
      "Deep dive into Semantic Kernel\n",
      "Article ‚Ä¢ 10/03/2024\n",
      "If you want to dive into deeper into Semantic Kernel and learn how to use more\n",
      "advanced functionality not explicitly covered in our Learn documentation, we\n",
      "recommend that you check out our concepts samples that individually demonstrate how\n",
      "to use specific features within the SDK.\n",
      "Each of the SDKs (Python, C#, and Java) have their own set of samples that walk through\n",
      "the SDK. Each sample is modelled as a test case within our main repo, so you're always\n",
      "guaranteed that the sample will work with the latest nightly version of the SDK! Below\n",
      "are most of the samples you'll find in our concepts project.\n",
      "View all C# concept samples on GitHub\n",
      "Supported Semantic Kernel languages\n",
      "Article ‚Ä¢ 11/11/2024\n",
      "Semantic Kernel plans on providing support to the following languages:\n",
      "While the overall architecture of the kernel is consistent across all languages, we made\n",
      "sure the SDK for each language follows common paradigms and styles in each language\n",
      "to make it feel native and easy to use.\n",
      "In C#, there are several packages to help ensure that you only need to import the\n",
      "functionality that you need for your project. The following table shows the available\n",
      "packages in C#.\n",
      "Package name\n",
      "Description\n",
      "Microsoft.SemanticKernel\n",
      "The main package that includes\n",
      "everything to get started\n",
      "Microsoft.SemanticKernel.Core\n",
      "The core package that provides\n",
      "implementations for\n",
      "Microsoft.SemanticKernel.Abstractions\n",
      "Microsoft.SemanticKernel.Abstractions\n",
      "The base abstractions for Semantic\n",
      "Kernel\n",
      "Microsoft.SemanticKernel.Connectors.Amazon\n",
      "The AI connector for Amazon AI\n",
      "Microsoft.SemanticKernel.Connectors.AzureAIInference\n",
      "The AI connector for Azure AI Inference\n",
      "Microsoft.SemanticKernel.Connectors.AzureOpenAI\n",
      "The AI connector for Azure OpenAI\n",
      "Microsoft.SemanticKernel.Connectors.Google\n",
      "The AI connector for Google models\n",
      "(e.g., Gemini)\n",
      "C#\n",
      "ÔºÇ\n",
      "Python\n",
      "ÔºÇ\n",
      "Java\n",
      "ÔºÇ\n",
      "Available SDK packages\n",
      "C# packages\n",
      "Ôæâ\n",
      "Expand table\n",
      "Package name\n",
      "Description\n",
      "Microsoft.SemanticKernel.Connectors.HuggingFace\n",
      "The AI connector for Hugging Face\n",
      "models\n",
      "Microsoft.SemanticKernel.Connectors.MistralAI\n",
      "The AI connector for Mistral AI models\n",
      "Microsoft.SemanticKernel.Connectors.Ollama\n",
      "The AI connector for Ollama\n",
      "Microsoft.SemanticKernel.Connectors.Onnx\n",
      "The AI connector for Onnx\n",
      "Microsoft.SemanticKernel.Connectors.OpenAI\n",
      "The AI connector for OpenAI\n",
      "Microsoft.SemanticKernel.Connectors.AzureAISearch\n",
      "The vector store connector for\n",
      "AzureAISearch\n",
      "Microsoft.SemanticKernel.Connectors.AzureCosmosDBMongoDB\n",
      "The vector store connector for\n",
      "AzureCosmosDBMongoDB\n",
      "Microsoft.SemanticKernel.Connectors.AzureCosmosDBNoSQL\n",
      "The vector store connector for\n",
      "AzureAISearch\n",
      "Microsoft.SemanticKernel.Connectors.MongoDB\n",
      "The vector store connector for\n",
      "MongoDB\n",
      "Microsoft.SemanticKernel.Connectors.Pinecone\n",
      "The vector store connector for\n",
      "Pinecone\n",
      "Microsoft.SemanticKernel.Connectors.Qdrant\n",
      "The vector store connector for Qdrant\n",
      "Microsoft.SemanticKernel.Connectors.Redis\n",
      "The vector store connector for Redis\n",
      "Microsoft.SemanticKernel.Connectors.Sqlite\n",
      "The vector store connector for Sqlite\n",
      "Microsoft.SemanticKernel.Connectors.Weaviate\n",
      "The vector store connector for\n",
      "Weaviate\n",
      "Microsoft.SemanticKernel.Plugins.OpenApi  (Experimental)\n",
      "Enables loading plugins from OpenAPI\n",
      "specifications\n",
      "Microsoft.SemanticKernel.PromptTemplates.Handlebars\n",
      "Enables the use of Handlebars\n",
      "templates for prompts\n",
      "Microsoft.SemanticKernel.Yaml\n",
      "Provides support for serializing\n",
      "prompts using YAML files\n",
      "Microsoft.SemanticKernel.Prompty\n",
      "Provides support for serializing\n",
      "prompts using Prompty files\n",
      "Microsoft.SemanticKernel.Agents.Abstractions\n",
      "Provides abstractions for creating\n",
      "agents\n",
      "Package name\n",
      "Description\n",
      "Microsoft.SemanticKernel.Agents.OpenAI\n",
      "Provides support for Assistant API\n",
      "agents\n",
      "To install any of these packages, you can use the following command:\n",
      "Bash\n",
      "In Python, there's a single package that includes everything you need to get started with\n",
      "Semantic Kernel. To install the package, you can use the following command:\n",
      "Bash\n",
      "On PyPI\n",
      " under Provides-Extra  the additional extras you can install are also listed and\n",
      "when used that will install the packages needed for using SK with that specific connector\n",
      "or service, you can install those with the square bracket syntax for instance:\n",
      "Bash\n",
      "This will install Semantic Kernel, as well as specific tested versions of: azure-ai-\n",
      "inference , azure-search-documents , azure-core , azure-identity , azure-cosmos  and\n",
      "msgraph-sdk  (and any dependencies of those packages). Similarly the extra\n",
      "hugging_face  will install transformers  and sentence-transformers .\n",
      "For Java, Semantic Kernel has the following packages; all are under the group Id\n",
      "com.microsoft.semantic-kernel , and can be imported from maven.\n",
      "XML\n",
      "dotnet add package <package-name>\n",
      "Python packages\n",
      "pip install semantic-kernel\n",
      "pip install semantic-kernel[azure]\n",
      "Java packages\n",
      "    <dependency>\n",
      "        <groupId>com.microsoft.semantic-kernel</groupId>\n",
      "A BOM is provided that can be used to define the versions of all Semantic Kernel\n",
      "packages.\n",
      "XML\n",
      "semantickernel-bom  ‚Äì A Maven project BOM that can be used to define the\n",
      "versions of all Semantic Kernel packages.\n",
      "semantickernel-api  ‚Äì Package that defines the core public API for the Semantic\n",
      "Kernel for a Maven project.\n",
      "semantickernel-aiservices-openai  ‚ÄìProvides a connector that can be used to\n",
      "interact with the OpenAI API.\n",
      "Below is an example POM XML for a simple project that uses OpenAI.\n",
      "XML\n",
      "        <artifactId>semantickernel-api</artifactId>\n",
      "    </dependency>\n",
      "    <dependencyManagement>\n",
      "        <dependencies>\n",
      "            <dependency>\n",
      "                <groupId>com.microsoft.semantic-kernel</groupId>\n",
      "                <artifactId>semantickernel-bom</artifactId>\n",
      "                <version>${semantickernel.version}</version>\n",
      "                <scope>import</scope>\n",
      "                <type>pom</type>\n",
      "            </dependency>\n",
      "        </dependencies>\n",
      "    </dependencyManagement>\n",
      "<project>\n",
      "    <dependencyManagement>\n",
      "        <dependencies>\n",
      "            <dependency>\n",
      "                <groupId>com.microsoft.semantic-kernel</groupId>\n",
      "                <artifactId>semantickernel-bom</artifactId>\n",
      "                <version>${semantickernel.version}</version>\n",
      "                <scope>import</scope>\n",
      "                <type>pom</type>\n",
      "            </dependency>\n",
      "        </dependencies>\n",
      "    </dependencyManagement>\n",
      "    <dependencies>\n",
      "        <dependency>\n",
      "            <groupId>com.microsoft.semantic-kernel</groupId>\n",
      "            <artifactId>semantickernel-api</artifactId>\n",
      "        </dependency>\n",
      "        <dependency>\n",
      "The following tables show which features are available in each language. The üîÑ symbol\n",
      "indicates that the feature is partially implemented, please see the associated note\n",
      "column for more details. The ‚ùå symbol indicates that the feature is not yet available in\n",
      "that language; if you would like to see a feature implemented in a language, please\n",
      "consider contributing to the project or opening an issue.\n",
      "Services\n",
      "C#\n",
      "Python\n",
      "Java\n",
      "Notes\n",
      "Prompts\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "To see the full list of supported template and\n",
      "serialization formats, refer to the tables below\n",
      "Native functions\n",
      "and plugins\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "OpenAPI plugins\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Java has a sample demonstrating how to load\n",
      "OpenAPI plugins\n",
      "Automatic function\n",
      "calling\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Open Telemetry\n",
      "logs\n",
      "‚úÖ\n",
      "üîÑ\n",
      "‚ùå\n",
      "Hooks and filters\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "When authoring prompts, Semantic Kernel provides a variety of template languages that\n",
      "allow you to embed variables and invoke functions. The following table shows which\n",
      "template languages are supported in each language.\n",
      "            <groupId>com.microsoft.semantic-kernel</groupId>\n",
      "            <artifactId>semantickernel-connectors-ai-openai</artifactId>\n",
      "        </dependency>\n",
      "    </dependencies>\n",
      "</project>\n",
      "Available features in each SDK\n",
      "Core capabilities\n",
      "Ôæâ\n",
      "Expand table\n",
      "Prompt template formats\n",
      "Ôæâ\n",
      "Expand table\n",
      "Formats\n",
      "C#\n",
      "Python\n",
      "Java\n",
      "Notes\n",
      "Semantic Kernel template language\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Handlebars\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Liquid\n",
      "‚úÖ\n",
      "‚ùå\n",
      "‚ùå\n",
      "Jinja2\n",
      "‚ùå\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Once you've created a prompt, you can serialize it so that it can be stored or shared\n",
      "across teams. The following table shows which serialization formats are supported in\n",
      "each language.\n",
      "Formats\n",
      "C#\n",
      "Python\n",
      "Java\n",
      "Notes\n",
      "YAML\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Prompty\n",
      "‚ùå\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Services\n",
      "C#\n",
      "Python\n",
      "Java\n",
      "Notes\n",
      "Text Generation\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Example: Text-Davinci-003\n",
      "Chat Completion\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Example: GPT4, Chat-GPT\n",
      "Text Embeddings (Experimental)\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Example: Text-Embeddings-Ada-002\n",
      "Text to Image (Experimental)\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Example: Dall-E\n",
      "Image to Text (Experimental)\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Example: Pix2Struct\n",
      "Text to Audio (Experimental)\n",
      "‚úÖ\n",
      "‚ùå\n",
      "‚ùå\n",
      "Example: Text-to-speech\n",
      "Audio to Text (Experimental)\n",
      "‚úÖ\n",
      "‚ùå\n",
      "‚ùå\n",
      "Example: Whisper\n",
      "Prompt serialization formats\n",
      "Ôæâ\n",
      "Expand table\n",
      "AI Services Modalities\n",
      "Ôæâ\n",
      "Expand table\n",
      "AI Service Connectors\n",
      "Endpoints\n",
      "C#\n",
      "Python\n",
      "Java\n",
      "Notes\n",
      "Amazon Bedrock\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Anthropic\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Azure AI Inference\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Azure OpenAI\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Google\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Hugging Face Inference API\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Mistral\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Ollama\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "ONNX\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "OpenAI\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Other endpoints that suppoprt\n",
      "OpenAI APIs\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Includes LLM Studio, Azure Model-as-\n",
      "a-service, etc.\n",
      "For the list of out of the box vector store connectors and the language support for each,\n",
      "refer to out of the box connectors.\n",
      "Ôæâ\n",
      "Expand table\n",
      "Vector Store Connectors (Experimental)\n",
      "Ôºí Warning\n",
      "The Semantic Kernel Vector Store functionality is in preview, and improvements that\n",
      "require breaking changes may still occur in limited circumstances before release.\n",
      "Memory Store Connectors (Legacy)\n",
      "Ôºâ Important\n",
      "Memory Store connectors are legacy and have been replaced by Vector Store\n",
      "connectors. For more information see Legacy Memory Stores.\n",
      "Memory Connectors\n",
      "C#\n",
      "Python\n",
      "Java\n",
      "Notes\n",
      "Azure AI Search\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Chroma\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "DuckDB\n",
      "‚úÖ\n",
      "‚ùå\n",
      "‚ùå\n",
      "Milvus\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Pinecone\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Postgres\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Qdrant\n",
      "‚úÖ\n",
      "üîÑ\n",
      "‚ùå\n",
      "Redis\n",
      "‚úÖ\n",
      "üîÑ\n",
      "‚ùå\n",
      "Sqlite\n",
      "‚úÖ\n",
      "‚ùå\n",
      "üîÑ\n",
      "Weaviate\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Ôæâ\n",
      "Expand table\n",
      "Understanding the kernel\n",
      "Article ‚Ä¢ 04/16/2025\n",
      "The kernel is the central component of Semantic Kernel. At its simplest, the kernel is a\n",
      "Dependency Injection container that manages all of the services and plugins necessary to run\n",
      "your AI application. If you provide all of your services and plugins to the kernel, they will then\n",
      "be seamlessly used by the AI as needed.\n",
      "Because the kernel has all of the services and plugins necessary to run both native code and AI\n",
      "services, it is used by nearly every component within the Semantic Kernel SDK to power your\n",
      "agents. This means that if you run any prompt or code in Semantic Kernel, the kernel will\n",
      "always be available to retrieve the necessary services and plugins.\n",
      "This is extremely powerful, because it means you as a developer have a single place where you\n",
      "can configure, and most importantly monitor, your AI agents. Take for example, when you\n",
      "invoke a prompt from the kernel. When you do so, the kernel will...\n",
      "1. Select the best AI service to run the prompt.\n",
      "2. Build the prompt using the provided prompt template.\n",
      "3. Send the prompt to the AI service.\n",
      "4. Receive and parse the response.\n",
      "5. And finally return the response from the LLM to your application.\n",
      "Throughout this entire process, you can create events and middleware that are triggered at\n",
      "each of these steps. This means you can perform actions like logging, provide status updates\n",
      "to users, and most importantly responsible AI. All from a single place.\n",
      "The kernel is at the center\n",
      "Before building a kernel, you should first understand the two types of components that exist:\n",
      "Component\n",
      "Description\n",
      "Services\n",
      "These consist of both AI services (e.g., chat completion) and other services (e.g., logging\n",
      "and HTTP clients) that are necessary to run your application. This was modelled after the\n",
      "Service Provider pattern in .NET so that we could support dependency injection across all\n",
      "languages.\n",
      "Plugins\n",
      "These are the components that are used by your AI services and prompt templates to\n",
      "perform work. AI services, for example, can use plugins to retrieve data from a database or\n",
      "call an external API to perform actions.\n",
      "To start creating a kernel, import the necessary packages at the top of your file:\n",
      "C#\n",
      "Next, you can add services and plugins. Below is an example of how you can add an Azure\n",
      "OpenAI chat completion, a logger, and a time plugin.\n",
      "C#\n",
      "In C#, you can use Dependency Injection to create a kernel. This is done by creating a\n",
      "ServiceCollection  and adding services and plugins to it. Below is an example of how you can\n",
      "create a kernel using Dependency Injection.\n",
      "Build a kernel with services and plugins\n",
      "Ôæâ\n",
      "Expand table\n",
      "using Microsoft.Extensions.DependencyInjection;\n",
      "using Microsoft.Extensions.Logging;\n",
      "using Microsoft.SemanticKernel;\n",
      "using Microsoft.SemanticKernel.Plugins.Core;\n",
      "// Create a kernel with a logger and Azure OpenAI chat completion service\n",
      "var builder = Kernel.CreateBuilder();\n",
      "builder.AddAzureOpenAIChatCompletion(modelId, endpoint, apiKey);\n",
      "builder.Services.AddLogging(c => c.AddDebug().SetMinimumLevel(LogLevel.Trace));\n",
      "builder.Plugins.AddFromType<TimePlugin>();\n",
      "Kernel kernel = builder.Build();\n",
      "Using Dependency Injection\n",
      "C#\n",
      "Ó™Ä Tip\n",
      "We recommend that you create a kernel as a transient service so that it is disposed of\n",
      "after each use because the plugin collection is mutable. The kernel is extremely\n",
      "lightweight (since it's just a container for services and plugins), so creating a new kernel\n",
      "for each use is not a performance concern.\n",
      "using Microsoft.SemanticKernel;\n",
      "var builder = Host.CreateApplicationBuilder(args);\n",
      "// Add the OpenAI chat completion service as a singleton\n",
      "builder.Services.AddOpenAIChatCompletion(\n",
      "    modelId: \"gpt-4\",\n",
      "    apiKey: \"YOUR_API_KEY\",\n",
      "    orgId: \"YOUR_ORG_ID\", // Optional; for OpenAI deployment\n",
      "    serviceId: \"YOUR_SERVICE_ID\" // Optional; for targeting specific services \n",
      "within Semantic Kernel\n",
      ");\n",
      "// Create singletons of your plugins\n",
      "builder.Services.AddSingleton(() => new LightsPlugin());\n",
      "builder.Services.AddSingleton(() => new SpeakerPlugin());\n",
      "// Create the plugin collection (using the KernelPluginFactory to create plugins \n",
      "from objects)\n",
      "builder.Services.AddSingleton<KernelPluginCollection>((serviceProvider) => \n",
      "    [\n",
      "        \n",
      "KernelPluginFactory.CreateFromObject(serviceProvider.GetRequiredService<LightsPlug\n",
      "in>()),\n",
      "        \n",
      "KernelPluginFactory.CreateFromObject(serviceProvider.GetRequiredService<SpeakerPlu\n",
      "gin>())\n",
      "    ]\n",
      ");\n",
      "// Finally, create the Kernel service with the service provider and plugin \n",
      "collection\n",
      "builder.Services.AddTransient((serviceProvider)=> {\n",
      "    KernelPluginCollection pluginCollection = \n",
      "serviceProvider.GetRequiredService<KernelPluginCollection>();\n",
      "    return new Kernel(serviceProvider, pluginCollection);\n",
      "});\n",
      "Ó™Ä Tip\n",
      "Now that you understand the kernel, you can learn about all the different AI services that you\n",
      "can add to it.\n",
      "For more samples on how to use dependency injection in C#, refer to the concept\n",
      "samples.\n",
      "Next steps\n",
      "Learn about AI services\n",
      "Semantic Kernel Components\n",
      "Article ‚Ä¢ 12/06/2024\n",
      "Semantic Kernel provides many different components, that can be used individually or\n",
      "together. This article gives an overview of the different components and explains the\n",
      "relationship between them.\n",
      "The Semantic Kernel AI service connectors provide an abstraction layer that exposes\n",
      "multiple AI service types from different providers via a common interface. Supported\n",
      "services include Chat Completion, Text Generation, Embedding Generation, Text to\n",
      "Image, Image to Text, Text to Audio and Audio to Text.\n",
      "When an implementation is registered with the Kernel, Chat Completion or Text\n",
      "Generation services will be used by default, by any method calls to the kernel. None of\n",
      "the other supported services will be used automatically.\n",
      "The Semantic Kernel Vector Store connectors provide an abstraction layer that exposes\n",
      "vector stores from different providers via a common interface. The Kernel does not use\n",
      "any registered vector store automatically, but Vector Search can easily be exposed as a\n",
      "plugin to the Kernel in which case the plugin is made available to Prompt Templates and\n",
      "the Chat Completion AI Model.\n",
      "AI Service Connectors\n",
      "Ó™Ä Tip\n",
      "For more information on using AI services see Adding AI services to Semantic\n",
      "Kernel.\n",
      "Vector Store (Memory) Connectors\n",
      "Ó™Ä Tip\n",
      "For more information on using memory connectors see Adding AI services to\n",
      "Semantic Kernel.\n",
      "Functions and Plugins\n",
      "Plugins are named function containers. Each can contain one or more functions. Plugins\n",
      "can be registered with the kernel, which allows the kernel to use them in two ways:\n",
      "1. Advertise them to the chat completion AI, so that the AI can choose them for\n",
      "invocation.\n",
      "2. Make them available to be called from a template during template rendering.\n",
      "Functions can easily be created from many sources, including from native code,\n",
      "OpenAPI specs, ITextSearch  implementations for RAG scenarios, but also from prompt\n",
      "templates.\n",
      "Ó™Ä Tip\n",
      "For more information on different plugin sources see What is a Plugin?.\n",
      "Ó™Ä Tip\n",
      "For more information on advertising plugins to the chat completion AI see\n",
      "Function calling with chat completion.\n",
      "Prompt Templates\n",
      "Prompt templates allow a developer or prompt engineer to create a template that mixes\n",
      "context and instructions for the AI with user input and function output. E.g. the template\n",
      "may contain instructions for the Chat Completion AI model, and placeholders for user\n",
      "input, plus hardcoded calls to plugins that always need to be executed before invoking\n",
      "the Chat Completion AI model.\n",
      "Prompt templates can be used in two ways:\n",
      "1. As the starting point of a Chat Completion flow by asking the kernel to render the\n",
      "template and invoke the Chat Completion AI model with the rendered result.\n",
      "2. As a plugin function, so that it can be invoked in the same way as any other\n",
      "function can be.\n",
      "When a prompt template is used, it will first be rendered, plus any hardcoded function\n",
      "references that it contains will be executed. The rendered prompt will then be passed to\n",
      "the Chat Completion AI model. The result generated by the AI will be returned to the\n",
      "caller. If the prompt template had been registered as a plugin function, the function may\n",
      "have been chosen for execution by the Chat Completion AI model and in this case the\n",
      "caller is Semantic Kernel, on behalf of the AI model.\n",
      "Using prompt templates as plugin functions in this way can result in rather complex\n",
      "flows. E.g. consider the scenario where a prompt template A  is registered as a plugin. At\n",
      "the same time a different prompt template B  may be passed to the kernel to start the\n",
      "chat completion flow. B  could have a hardcoded call to A . This would result in the\n",
      "following steps:\n",
      "1. B  rendering starts and the prompt execution finds a reference to A\n",
      "2. A  is rendered.\n",
      "3. The rendered output of A  is passed to the Chat Completion AI model.\n",
      "4. The result of the Chat Completion AI model is returned to B .\n",
      "5. Rendering of B  completes.\n",
      "6. The rendered output of B  is passed to the Chat Completion AI model.\n",
      "7. The result of the Chat Completion AI model is returned to to the caller.\n",
      "Also consider the scenario where there is no hardcoded call from B  to A . If function\n",
      "calling is enabled, the Chat Completion AI model may still decide that A  should be\n",
      "invoked since it requires data or functionality that A  can provide.\n",
      "Registering prompt templates as plugin functions allows for the possibility of creating\n",
      "functionality that is described using human language instead of actual code. Separating\n",
      "the functionality into a plugin like this allows the AI model to reason about this\n",
      "separately to the main execution flow, and can lead to higher success rates by the AI\n",
      "model, since it can focus on a single problem at a time.\n",
      "See the following diagram for a simple flow that is started from a prompt template.\n",
      "Filters provide a way to take custom action before and after specific events during the\n",
      "chat completion flow. These events include:\n",
      "1. Before and after function invocation.\n",
      "2. Before and after prompt rendering.\n",
      "Filters need to be registered with the kernel to get invoked during the chat completion\n",
      "flow.\n",
      "Note that since prompt templates are always converted to KernelFunctions before\n",
      "execution, both function and prompt filters will be invoked for a prompt template. Since\n",
      "filters are nested when more than one is available, function filters are the outer filters\n",
      "and prompt filters are the inner filters.\n",
      "Ó™Ä Tip\n",
      "For more information on prompt templates see What are prompts?.\n",
      "Filters\n",
      "Ó™Ä Tip\n",
      "For more information on filters see What are Filters?.\n",
      "Adding AI services to Semantic Kernel\n",
      "Article ‚Ä¢ 03/06/2025\n",
      "One of the main features of Semantic Kernel is its ability to add different AI services to\n",
      "the kernel. This allows you to easily swap out different AI services to compare their\n",
      "performance and to leverage the best model for your needs. In this section, we will\n",
      "provide sample code for adding different AI services to the kernel.\n",
      "Within Semantic Kernel, there are interfaces for the most popular AI tasks. In the table\n",
      "below, you can see the services that are supported by each of the SDKs.\n",
      "Services\n",
      "C#\n",
      "Python\n",
      "Java\n",
      "Notes\n",
      "Chat completion\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Text generation\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Embedding generation (Experimental)\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "Text-to-image (Experimental)\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Image-to-text (Experimental)\n",
      "‚úÖ\n",
      "‚ùå\n",
      "‚ùå\n",
      "Text-to-audio (Experimental)\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Audio-to-text (Experimental)\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Realtime (Experimental)\n",
      "‚ùå\n",
      "‚úÖ\n",
      "‚ùå\n",
      "To learn more about each of the services, please refer to the specific articles for each\n",
      "service type. In each of the articles we provide sample code for adding the service to the\n",
      "kernel across multiple AI service providers.\n",
      "Ôæâ\n",
      "Expand table\n",
      "Ó™Ä Tip\n",
      "In most scenarios, you will only need to add chat completion to your kernel, but to\n",
      "support multi-modal AI, you can add any of the above services to your kernel.\n",
      "Next steps\n",
      "Learn about chat completion\n",
      "Chat completion\n",
      "Article ‚Ä¢ 05/28/2025\n",
      "With chat completion, you can simulate a back-and-forth conversation with an AI agent. This is\n",
      "of course useful for creating chat bots, but it can also be used for creating autonomous agents\n",
      "that can complete business processes, generate code, and more. As the primary model type\n",
      "provided by OpenAI, Google, Mistral, Facebook, and others, chat completion is the most\n",
      "common AI service that you will add to your Semantic Kernel project.\n",
      "When picking out a chat completion model, you will need to consider the following:\n",
      "What modalities does the model support (e.g., text, image, audio, etc.)?\n",
      "Does it support function calling?\n",
      "How fast does it receive and generate tokens?\n",
      "How much does each token cost?\n",
      "Some of the AI Services can be hosted locally and may require some setup. Below are\n",
      "instructions for those that support this.\n",
      "No local setup.\n",
      "Before adding chat completion to your kernel, you will need to install the necessary packages.\n",
      "Below are the packages you will need to install for each AI service provider.\n",
      "Ôºâ Important\n",
      "Of all the above questions, the most important is whether the model supports function\n",
      "calling. If it does not, you will not be able to use the model to call your existing code. Most\n",
      "of the latest models from OpenAI, Google, Mistral, and Amazon all support function\n",
      "calling. Support from small language models, however, is still limited.\n",
      "Setting up your local environment\n",
      "Azure OpenAI\n",
      "Installing the necessary packages\n",
      "Azure OpenAI\n",
      "Bash\n",
      "Now that you've installed the necessary packages, you can create chat completion services.\n",
      "Below are the several ways you can create chat completion services using Semantic Kernel.\n",
      "To add a chat completion service, you can use the following code to add it to the kernel's inner\n",
      "service provider.\n",
      "C#\n",
      "If you're using dependency injection, you'll likely want to add your AI services directly to the\n",
      "service provider. This is helpful if you want to create singletons of your AI services and reuse\n",
      "them in transient kernels.\n",
      "dotnet add package Microsoft.SemanticKernel.Connectors.AzureOpenAI\n",
      "Creating chat completion services\n",
      "Adding directly to the kernel\n",
      "Azure OpenAI\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder kernelBuilder = Kernel.CreateBuilder();\n",
      "kernelBuilder.AddAzureOpenAIChatCompletion(\n",
      "    deploymentName: \"NAME_OF_YOUR_DEPLOYMENT\",\n",
      "    apiKey: \"YOUR_API_KEY\",\n",
      "    endpoint: \"YOUR_AZURE_ENDPOINT\",\n",
      "    modelId: \"gpt-4\", // Optional name of the underlying model if the \n",
      "deployment name doesn't match the model name\n",
      "    serviceId: \"YOUR_SERVICE_ID\", // Optional; for targeting specific services \n",
      "within Semantic Kernel\n",
      "    httpClient: new HttpClient() // Optional; if not provided, the HttpClient \n",
      "from the kernel will be used\n",
      ");\n",
      "Kernel kernel = kernelBuilder.Build();\n",
      "Using dependency injection\n",
      "Azure OpenAI\n",
      "C#\n",
      "Lastly, you can create instances of the service directly so that you can either add them to a\n",
      "kernel later or use them directly in your code without ever injecting them into the kernel or in a\n",
      "service provider.\n",
      "C#\n",
      "using Microsoft.SemanticKernel;\n",
      "var builder = Host.CreateApplicationBuilder(args);\n",
      "builder.Services.AddAzureOpenAIChatCompletion(\n",
      "    deploymentName: \"NAME_OF_YOUR_DEPLOYMENT\",\n",
      "    apiKey: \"YOUR_API_KEY\",\n",
      "    endpoint: \"YOUR_AZURE_ENDPOINT\",\n",
      "    modelId: \"gpt-4\", // Optional name of the underlying model if the \n",
      "deployment name doesn't match the model name\n",
      "    serviceId: \"YOUR_SERVICE_ID\" // Optional; for targeting specific services \n",
      "within Semantic Kernel\n",
      ");\n",
      "builder.Services.AddTransient((serviceProvider)=> {\n",
      "    return new Kernel(serviceProvider);\n",
      "});\n",
      "Creating standalone instances\n",
      "Azure OpenAI\n",
      "using Microsoft.SemanticKernel.Connectors.AzureOpenAI;\n",
      "AzureOpenAIChatCompletionService chatCompletionService = new (\n",
      "    deploymentName: \"NAME_OF_YOUR_DEPLOYMENT\",\n",
      "    apiKey: \"YOUR_API_KEY\",\n",
      "    endpoint: \"YOUR_AZURE_ENDPOINT\",\n",
      "    modelId: \"gpt-4\", // Optional name of the underlying model if the \n",
      "deployment name doesn't match the model name\n",
      "    httpClient: new HttpClient() // Optional; if not provided, the HttpClient \n",
      "from the kernel will be used\n",
      ");\n",
      "Retrieving chat completion services\n",
      "Once you've added chat completion services to your kernel, you can retrieve them using the\n",
      "get service method. Below is an example of how you can retrieve a chat completion service\n",
      "from the kernel.\n",
      "C#\n",
      "Now that you have a chat completion service, you can use it to generate responses from an AI\n",
      "agent. There are two main ways to use a chat completion service:\n",
      "Non-streaming: You wait for the service to generate an entire response before returning\n",
      "it to the user.\n",
      "Streaming: Individual chunks of the response are generated and returned to the user as\n",
      "they are created.\n",
      "Below are the two ways you can use a chat completion service to generate responses.\n",
      "To use non-streaming chat completion, you can use the following code to generate a response\n",
      "from the AI agent.\n",
      "C#\n",
      "var chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();\n",
      "Ó™Ä Tip\n",
      "Adding the chat completion service to the kernel is not required if you don't need to use\n",
      "other services in the kernel. You can use the chat completion service directly in your code.\n",
      "Using chat completion services\n",
      "Non-streaming chat completion\n",
      "ChatHistory history = [];\n",
      "history.AddUserMessage(\"Hello, how are you?\");\n",
      "var response = await chatCompletionService.GetChatMessageContentAsync(\n",
      "    history,\n",
      "    kernel: kernel\n",
      ");\n",
      "Streaming chat completion\n",
      "To use streaming chat completion, you can use the following code to generate a response from\n",
      "the AI agent.\n",
      "C#\n",
      "Now that you've added chat completion services to your Semantic Kernel project, you can start\n",
      "creating conversations with your AI agent. To learn more about using a chat completion\n",
      "service, check out the following articles:\n",
      "ChatHistory history = [];\n",
      "history.AddUserMessage(\"Hello, how are you?\");\n",
      "var response = chatCompletionService.GetStreamingChatMessageContentsAsync(\n",
      "    chatHistory: history,\n",
      "    kernel: kernel\n",
      ");\n",
      "await foreach (var chunk in response)\n",
      "{\n",
      "    Console.Write(chunk);\n",
      "}\n",
      "Next steps\n",
      "Using the chat history object\n",
      "Optimizing function calling with chat completion\n",
      "Chat history\n",
      "Article ‚Ä¢ 01/31/2025\n",
      "The chat history object is used to maintain a record of messages in a chat session. It is\n",
      "used to store messages from different authors, such as users, assistants, tools, or the\n",
      "system. As the primary mechanism for sending and receiving messages, the chat history\n",
      "object is essential for maintaining context and continuity in a conversation.\n",
      "A chat history object is a list under the hood, making it easy to create and add messages\n",
      "to.\n",
      "C#\n",
      "The easiest way to add messages to a chat history object is to use the methods above.\n",
      "However, you can also add messages manually by creating a new ChatMessage  object.\n",
      "This allows you to provide additional information, like names and images content.\n",
      "C#\n",
      "Creating a chat history object\n",
      "using Microsoft.SemanticKernel.ChatCompletion;\n",
      "// Create a chat history object\n",
      "ChatHistory chatHistory = [];\n",
      "chatHistory.AddSystemMessage(\"You are a helpful assistant.\");\n",
      "chatHistory.AddUserMessage(\"What's available to order?\");\n",
      "chatHistory.AddAssistantMessage(\"We have pizza, pasta, and salad available \n",
      "to order. What would you like to order?\");\n",
      "chatHistory.AddUserMessage(\"I'd like to have the first option, please.\");\n",
      "Adding richer messages to a chat history\n",
      "using Microsoft.SemanticKernel.ChatCompletion;\n",
      "// Add system message\n",
      "chatHistory.Add(\n",
      "    new() {\n",
      "        Role = AuthorRole.System,\n",
      "        Content = \"You are a helpful assistant\"\n",
      "    }\n",
      ");\n",
      "In addition to user, assistant, and system roles, you can also add messages from the tool\n",
      "role to simulate function calls. This is useful for teaching the AI how to use plugins and\n",
      "to provide additional context to the conversation.\n",
      "For example, to inject information about the current user in the chat history without\n",
      "requiring the user to provide the information or having the LLM waste time asking for it,\n",
      "you can use the tool role to provide the information directly.\n",
      "Below is an example of how we're able to provide user allergies to the assistant by\n",
      "simulating a function call to the User  plugin.\n",
      "// Add user message with an image\n",
      "chatHistory.Add(\n",
      "    new() {\n",
      "        Role = AuthorRole.User,\n",
      "        AuthorName = \"Laimonis Dumins\",\n",
      "        Items = [\n",
      "            new TextContent { Text = \"What available on this menu\" },\n",
      "            new ImageContent { Uri = new Uri(\"https://example.com/menu.jpg\") \n",
      "}\n",
      "        ]\n",
      "    }\n",
      ");\n",
      "// Add assistant message\n",
      "chatHistory.Add(\n",
      "    new() {\n",
      "        Role = AuthorRole.Assistant,\n",
      "        AuthorName = \"Restaurant Assistant\",\n",
      "        Content = \"We have pizza, pasta, and salad available to order. What \n",
      "would you like to order?\"\n",
      "    }\n",
      ");\n",
      "// Add additional message from a different user\n",
      "chatHistory.Add(\n",
      "    new() {\n",
      "        Role = AuthorRole.User,\n",
      "        AuthorName = \"Ema Vargova\",\n",
      "        Content = \"I'd like to have the first option, please.\"\n",
      "    }\n",
      ");\n",
      "Simulating function calls\n",
      "Ó™Ä Tip\n",
      "C#\n",
      "Simulated function calls is particularly helpful for providing details about the\n",
      "current user(s). Today's LLMs have been trained to be particularly sensitive to user\n",
      "information. Even if you provide user details in a system message, the LLM may still\n",
      "choose to ignore it. If you provide it via a user message, or tool message, the LLM\n",
      "is more likely to use it.\n",
      "// Add a simulated function call from the assistant\n",
      "chatHistory.Add(\n",
      "    new() {\n",
      "        Role = AuthorRole.Assistant,\n",
      "        Items = [\n",
      "            new FunctionCallContent(\n",
      "                functionName: \"get_user_allergies\",\n",
      "                pluginName: \"User\",\n",
      "                id: \"0001\",\n",
      "                arguments: new () { {\"username\", \"laimonisdumins\"} }\n",
      "            ),\n",
      "            new FunctionCallContent(\n",
      "                functionName: \"get_user_allergies\",\n",
      "                pluginName: \"User\",\n",
      "                id: \"0002\",\n",
      "                arguments: new () { {\"username\", \"emavargova\"} }\n",
      "            )\n",
      "        ]\n",
      "    }\n",
      ");\n",
      "// Add a simulated function results from the tool role\n",
      "chatHistory.Add(\n",
      "    new() {\n",
      "        Role = AuthorRole.Tool,\n",
      "        Items = [\n",
      "            new FunctionResultContent(\n",
      "                functionName: \"get_user_allergies\",\n",
      "                pluginName: \"User\",\n",
      "                id: \"0001\",\n",
      "                result: \"{ \\\"allergies\\\": [\\\"peanuts\\\", \\\"gluten\\\"] }\"\n",
      "            )\n",
      "        ]\n",
      "    }\n",
      ");\n",
      "chatHistory.Add(\n",
      "    new() {\n",
      "        Role = AuthorRole.Tool,\n",
      "        Items = [\n",
      "            new FunctionResultContent(\n",
      "                functionName: \"get_user_allergies\",\n",
      "                pluginName: \"User\",\n",
      "                id: \"0002\",\n",
      "                result: \"{ \\\"allergies\\\": [\\\"dairy\\\", \\\"soy\\\"] }\"\n",
      "Whenever you pass a chat history object to a chat completion service with auto function\n",
      "calling enabled, the chat history object will be manipulated so that it includes the\n",
      "function calls and results. This allows you to avoid having to manually add these\n",
      "messages to the chat history object and also allows you to inspect the chat history\n",
      "object to see the function calls and results.\n",
      "You must still, however, add the final messages to the chat history object. Below is an\n",
      "example of how you can inspect the chat history object to see the function calls and\n",
      "results.\n",
      "C#\n",
      "            )\n",
      "        ]\n",
      "    }\n",
      ");\n",
      "Ôºâ Important\n",
      "When simulating tool results, you must always provide the id  of the function call\n",
      "that the result corresponds to. This is important for the AI to understand the\n",
      "context of the result. Some LLMs, like OpenAI, will throw an error if the id  is\n",
      "missing or if the id  does not correspond to a function call.\n",
      "Inspecting a chat history object\n",
      "using Microsoft.SemanticKernel.ChatCompletion;\n",
      "ChatHistory chatHistory = [\n",
      "    new() {\n",
      "        Role = AuthorRole.User,\n",
      "        Content = \"Please order me a pizza\"\n",
      "    }\n",
      "];\n",
      "// Get the current length of the chat history object\n",
      "int currentChatHistoryLength = chatHistory.Count;\n",
      "// Get the chat message content\n",
      "ChatMessageContent results = await \n",
      "chatCompletionService.GetChatMessageContentAsync(\n",
      "    chatHistory,\n",
      "    kernel: kernel\n",
      ");\n",
      "// Get the new messages added to the chat history object\n",
      "Managing chat history is essential for maintaining context-aware conversations while\n",
      "ensuring efficient performance. As a conversation progresses, the history object can\n",
      "grow beyond the limits of a model‚Äôs context window, affecting response quality and\n",
      "slowing down processing. A structured approach to reducing chat history ensures that\n",
      "the most relevant information remains available without unnecessary overhead.\n",
      "Performance Optimization: Large chat histories increase processing time. Reducing\n",
      "their size helps maintain fast and efficient interactions.\n",
      "Context Window Management: Language models have a fixed context window.\n",
      "When the history exceeds this limit, older messages are lost. Managing chat history\n",
      "ensures that the most important context remains accessible.\n",
      "Memory Efficiency: In resource-constrained environments such as mobile\n",
      "applications or embedded systems, unbounded chat history can lead to excessive\n",
      "memory usage and slow performance.\n",
      "Privacy and Security: Retaining unnecessary conversation history increases the risk\n",
      "of exposing sensitive information. A structured reduction process minimizes data\n",
      "retention while maintaining relevant context.\n",
      "Several approaches can be used to keep chat history manageable while preserving\n",
      "essential information:\n",
      "Truncation: The oldest messages are removed when the history exceeds a\n",
      "predefined limit, ensuring only recent interactions are retained.\n",
      "Summarization: Older messages are condensed into a summary, preserving key\n",
      "details while reducing the number of stored messages.\n",
      "for (int i = currentChatHistoryLength; i < chatHistory.Count; i++)\n",
      "{\n",
      "    Console.WriteLine(chatHistory[i]);\n",
      "}\n",
      "// Print the final message\n",
      "Console.WriteLine(results);\n",
      "// Add the final message to the chat history object\n",
      "chatHistory.Add(results);\n",
      "Chat History Reduction\n",
      "Why Reduce Chat History?\n",
      "Strategies for Reducing Chat History\n",
      "Token-Based: Token-based reduction ensures chat history stays within a model‚Äôs\n",
      "token limit by measuring total token count and removing or summarizing older\n",
      "messages when the limit is exceeded.\n",
      "A Chat History Reducer automates these strategies by evaluating the history‚Äôs size and\n",
      "reducing it based on configurable parameters such as target count (the desired number\n",
      "of messages to retain) and threshold count (the point at which reduction is triggered).\n",
      "By integrating these reduction techniques, chat applications can remain responsive and\n",
      "performant without compromising conversational context.\n",
      "In the .NET version of Semantic Kernel, the Chat History Reducer abstraction is defined\n",
      "by the IChatHistoryReducer  interface:\n",
      "C#\n",
      "This interface allows custom implementations for chat history reduction.\n",
      "Additionally, Semantic Kernel provides built-in reducers:\n",
      "ChatHistoryTruncationReducer  - truncates chat history to a specified size and\n",
      "discards the removed messages. The reduction is triggered when the chat history\n",
      "length exceeds the limit.\n",
      "ChatHistorySummarizationReducer  - truncates chat history, summarizes the\n",
      "removed messages and adds the summary back into the chat history as a single\n",
      "message.\n",
      "Both reducers always preserve system messages to retain essential context for the\n",
      "model.\n",
      "The following example demonstrates how to retain only the last two user messages\n",
      "while maintaining conversation flow:\n",
      "C#\n",
      "namespace Microsoft.SemanticKernel.ChatCompletion;\n",
      "[Experimental(\"SKEXP0001\")]\n",
      "public interface IChatHistoryReducer\n",
      "{\n",
      "    Task<IEnumerable<ChatMessageContent>?> \n",
      "ReduceAsync(IReadOnlyList<ChatMessageContent> chatHistory, CancellationToken \n",
      "cancellationToken = default);\n",
      "}\n",
      "using Microsoft.SemanticKernel.ChatCompletion;\n",
      "using Microsoft.SemanticKernel.Connectors.OpenAI;\n",
      "More examples can be found in the Semantic Kernel repository\n",
      ".\n",
      "var chatService = new OpenAIChatCompletionService(\n",
      "    modelId: \"<model-id>\",\n",
      "    apiKey: \"<api-key>\");\n",
      "var reducer = new ChatHistoryTruncationReducer(targetCount: 2); // Keep \n",
      "system message and last user message\n",
      "var chatHistory = new ChatHistory(\"You are a librarian and expert on books \n",
      "about cities\");\n",
      "string[] userMessages = [\n",
      "    \"Recommend a list of books about Seattle\",\n",
      "    \"Recommend a list of books about Dublin\",\n",
      "    \"Recommend a list of books about Amsterdam\",\n",
      "    \"Recommend a list of books about Paris\",\n",
      "    \"Recommend a list of books about London\"\n",
      "];\n",
      "int totalTokenCount = 0;\n",
      "foreach (var userMessage in userMessages)\n",
      "{\n",
      "    chatHistory.AddUserMessage(userMessage);\n",
      "    Console.WriteLine($\"\\n>>> User:\\n{userMessage}\");\n",
      "    var reducedMessages = await reducer.ReduceAsync(chatHistory);\n",
      "    if (reducedMessages is not null)\n",
      "    {\n",
      "        chatHistory = new ChatHistory(reducedMessages);\n",
      "    }\n",
      "    var response = await \n",
      "chatService.GetChatMessageContentAsync(chatHistory);\n",
      "    chatHistory.AddAssistantMessage(response.Content!);\n",
      "    Console.WriteLine($\"\\n>>> Assistant:\\n{response.Content!}\");\n",
      "    if (response.InnerContent is OpenAI.Chat.ChatCompletion chatCompletion)\n",
      "    {\n",
      "        totalTokenCount += chatCompletion.Usage?.TotalTokenCount ?? 0;\n",
      "    }\n",
      "}\n",
      "Console.WriteLine($\"Total Token Count: {totalTokenCount}\");\n",
      "Next steps\n",
      "Now that you know how to create and manage a chat history object, you can learn more\n",
      "about function calling in the Function calling topic.\n",
      "Learn how function calling works\n",
      "Multi-modal chat completion\n",
      "Article ‚Ä¢ 11/21/2024\n",
      "Many AI services support input using images, text and potentially more at the same\n",
      "time, allowing developers to blend together these different inputs. This allows for\n",
      "scenarios such as passing an image and asking the AI model a specific question about\n",
      "the image.\n",
      "The Semantic Kernel chat completion connectors support passing both images and text\n",
      "at the same time to a chat completion AI model. Note that not all AI models or AI\n",
      "services support this behavior.\n",
      "After you have constructed a chat completion service using the steps outlined in the\n",
      "Chat completion article, you can provide images and text in the following way.\n",
      "Using images with chat completion\n",
      "// Load an image from disk.\n",
      "byte[] bytes = File.ReadAllBytes(\"path/to/image.jpg\");\n",
      "// Create a chat history with a system message instructing\n",
      "// the LLM on its required role.\n",
      "var chatHistory = new ChatHistory(\"Your job is describing images.\");\n",
      "// Add a user message with both the image and a question\n",
      "// about the image.\n",
      "chatHistory.AddUserMessage(\n",
      "[\n",
      "    new TextContent(\"What‚Äôs in this image?\"),\n",
      "    new ImageContent(bytes, \"image/jpeg\"),\n",
      "]);\n",
      "// Invoke the chat completion model.\n",
      "var reply = await \n",
      "chatCompletionService.GetChatMessageContentAsync(chatHistory);\n",
      "Console.WriteLine(reply.Content);\n",
      "Function calling with chat completion\n",
      "Article ‚Ä¢ 04/16/2025\n",
      "The most powerful feature of chat completion is the ability to call functions from the model.\n",
      "This allows you to create a chat bot that can interact with your existing code, making it possible\n",
      "to automate business processes, create code snippets, and more.\n",
      "With Semantic Kernel, we simplify the process of using function calling by automatically\n",
      "describing your functions and their parameters to the model and then handling the back-and-\n",
      "forth communication between the model and your code.\n",
      "When using function calling, however, it's good to understand what's actually happening\n",
      "behind the scenes so that you can optimize your code and make the most of this feature.\n",
      "When you make a request to a model with function calling enabled, Semantic Kernel performs\n",
      "the following steps:\n",
      "#\n",
      "Step\n",
      "Description\n",
      "1\n",
      "Serialize functions\n",
      "All of the available functions (and its input parameters) in the kernel are\n",
      "serialized using JSON schema.\n",
      "2\n",
      "Send the messages\n",
      "and functions to the\n",
      "model\n",
      "The serialized functions (and the current chat history) are sent to the model\n",
      "as part of the input.\n",
      "3\n",
      "Model processes the\n",
      "input\n",
      "The model processes the input and generates a response. The response can\n",
      "either be a chat message or one or more function calls.\n",
      "4\n",
      "Handle the response\n",
      "If the response is a chat message, it is returned to the caller. If the response\n",
      "is a function call, however, Semantic Kernel extracts the function name and\n",
      "its parameters.\n",
      "How auto function calling works\n",
      "Ôºó Note\n",
      "The following section describes how auto function calling works in Semantic Kernel. Auto\n",
      "function calling is the default behavior in Semantic Kernel, but you can also manually\n",
      "invoke functions if you prefer. For more information on manual function invocation, please\n",
      "refer to the function invocation article.\n",
      "Ôæâ\n",
      "Expand table\n",
      "#\n",
      "Step\n",
      "Description\n",
      "5\n",
      "Invoke the function\n",
      "The extracted function name and parameters are used to invoke the\n",
      "function in the kernel.\n",
      "6\n",
      "Return the function\n",
      "result\n",
      "The result of the function is then sent back to the model as part of the chat\n",
      "history. Steps 2-6 are then repeated until the model returns a chat message\n",
      "or the max iteration number has been reached.\n",
      "The following diagram illustrates the process of function calling:\n",
      "The following section will use a concrete example to illustrate how function calling works in\n",
      "practice.\n",
      "Let's assume you have a plugin that allows a user to order a pizza. The plugin has the following\n",
      "functions:\n",
      "1. get_pizza_menu : Returns a list of available pizzas\n",
      "2. add_pizza_to_cart : Adds a pizza to the user's cart\n",
      "3. remove_pizza_from_cart : Removes a pizza from the user's cart\n",
      "4. get_pizza_from_cart : Returns the specific details of a pizza in the user's cart\n",
      "Example: Ordering a pizza\n",
      "5. get_cart : Returns the user's current cart\n",
      "6. checkout : Checks out the user's cart\n",
      "In C#, the plugin might look like this:\n",
      "C#\n",
      "public class OrderPizzaPlugin(\n",
      "    IPizzaService pizzaService,\n",
      "    IUserContext userContext,\n",
      "    IPaymentService paymentService)\n",
      "{\n",
      "    [KernelFunction(\"get_pizza_menu\")]\n",
      "    public async Task<Menu> GetPizzaMenuAsync()\n",
      "    {\n",
      "        return await pizzaService.GetMenu();\n",
      "    }\n",
      "    [KernelFunction(\"add_pizza_to_cart\")]\n",
      "    [Description(\"Add a pizza to the user's cart; returns the new item and updated \n",
      "cart\")]\n",
      "    public async Task<CartDelta> AddPizzaToCart(\n",
      "        PizzaSize size,\n",
      "        List<PizzaToppings> toppings,\n",
      "        int quantity = 1,\n",
      "        string specialInstructions = \"\"\n",
      "    )\n",
      "    {\n",
      "        Guid cartId = userContext.GetCartId();\n",
      "        return await pizzaService.AddPizzaToCart(\n",
      "            cartId: cartId,\n",
      "            size: size,\n",
      "            toppings: toppings,\n",
      "            quantity: quantity,\n",
      "            specialInstructions: specialInstructions);\n",
      "    }\n",
      "    [KernelFunction(\"remove_pizza_from_cart\")]\n",
      "    public async Task<RemovePizzaResponse> RemovePizzaFromCart(int pizzaId)\n",
      "    {\n",
      "        Guid cartId = userContext.GetCartId();\n",
      "        return await pizzaService.RemovePizzaFromCart(cartId, pizzaId);\n",
      "    }\n",
      "    [KernelFunction(\"get_pizza_from_cart\")]\n",
      "    [Description(\"Returns the specific details of a pizza in the user's cart; use \n",
      "this instead of relying on previous messages since the cart may have changed since \n",
      "then.\")]\n",
      "    public async Task<Pizza> GetPizzaFromCart(int pizzaId)\n",
      "    {\n",
      "        Guid cartId = await userContext.GetCartIdAsync();\n",
      "        return await pizzaService.GetPizzaFromCart(cartId, pizzaId);\n",
      "    }\n",
      "You would then add this plugin to the kernel like so:\n",
      "C#\n",
      "When you create a kernel with the OrderPizzaPlugin , the kernel will automatically serialize the\n",
      "functions and their parameters. This is necessary so that the model can understand the\n",
      "functions and their inputs.\n",
      "For the above plugin, the serialized functions would look like this:\n",
      "    [KernelFunction(\"get_cart\")]\n",
      "    [Description(\"Returns the user's current cart, including the total price and \n",
      "items in the cart.\")]\n",
      "    public async Task<Cart> GetCart()\n",
      "    {\n",
      "        Guid cartId = await userContext.GetCartIdAsync();\n",
      "        return await pizzaService.GetCart(cartId);\n",
      "    }\n",
      "    [KernelFunction(\"checkout\")]\n",
      "    [Description(\"Checkouts the user's cart; this function will retrieve the \n",
      "payment from the user and complete the order.\")]\n",
      "    public async Task<CheckoutResponse> Checkout()\n",
      "    {\n",
      "        Guid cartId = await userContext.GetCartIdAsync();\n",
      "        Guid paymentId = await paymentService.RequestPaymentFromUserAsync(cartId);\n",
      "        return await pizzaService.Checkout(cartId, paymentId);\n",
      "    }\n",
      "}\n",
      "IKernelBuilder kernelBuilder = new KernelBuilder();\n",
      "kernelBuilder..AddAzureOpenAIChatCompletion(\n",
      "    deploymentName: \"NAME_OF_YOUR_DEPLOYMENT\",\n",
      "    apiKey: \"YOUR_API_KEY\",\n",
      "    endpoint: \"YOUR_AZURE_ENDPOINT\"\n",
      ");\n",
      "kernelBuilder.Plugins.AddFromType<OrderPizzaPlugin>(\"OrderPizza\");\n",
      "Kernel kernel = kernelBuilder.Build();\n",
      "Ôºó Note\n",
      "Only functions with the KernelFunction  attribute will be serialized and sent to the model.\n",
      "This allows you to have helper functions that are not exposed to the model.\n",
      "1) Serializing the functions\n",
      "JSON\n",
      "[\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"OrderPizza-get_pizza_menu\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"OrderPizza-add_pizza_to_cart\",\n",
      "      \"description\": \"Add a pizza to the user's cart; returns the new item and \n",
      "updated cart\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"size\": {\n",
      "            \"type\": \"string\",\n",
      "            \"enum\": [\"Small\", \"Medium\", \"Large\"]\n",
      "          },\n",
      "          \"toppings\": {\n",
      "            \"type\": \"array\",\n",
      "            \"items\": {\n",
      "              \"type\": \"string\",\n",
      "              \"enum\": [\"Cheese\", \"Pepperoni\", \"Mushrooms\"]\n",
      "            }\n",
      "          },\n",
      "          \"quantity\": {\n",
      "            \"type\": \"integer\",\n",
      "            \"default\": 1,\n",
      "            \"description\": \"Quantity of pizzas\"\n",
      "          },\n",
      "          \"specialInstructions\": {\n",
      "            \"type\": \"string\",\n",
      "            \"default\": \"\",\n",
      "            \"description\": \"Special instructions for the pizza\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\"size\", \"toppings\"]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"OrderPizza-remove_pizza_from_cart\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"pizzaId\": {\n",
      "            \"type\": \"integer\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\"pizzaId\"]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"OrderPizza-get_pizza_from_cart\",\n",
      "      \"description\": \"Returns the specific details of a pizza in the user's cart; \n",
      "use this instead of relying on previous messages since the cart may have changed \n",
      "since then.\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"pizzaId\": {\n",
      "            \"type\": \"integer\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\"pizzaId\"]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"OrderPizza-get_cart\",\n",
      "      \"description\": \"Returns the user's current cart, including the total price \n",
      "and items in the cart.\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"OrderPizza-checkout\",\n",
      "      \"description\": \"Checkouts the user's cart; this function will retrieve the \n",
      "payment from the user and complete the order.\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n",
      "There's a few things to note here which can impact both the performance and the quality of\n",
      "the chat completion:\n",
      "1. Verbosity of function schema ‚Äì Serializing functions for the model to use doesn't come\n",
      "for free. The more verbose the schema, the more tokens the model has to process, which\n",
      "can slow down the response time and increase costs.\n",
      "2. Parameter types ‚Äì With the schema, you can specify the type of each parameter. This is\n",
      "important for the model to understand the expected input. In the above example, the\n",
      "size  parameter is an enum, and the toppings  parameter is an array of enums. This helps\n",
      "the model generate more accurate responses.\n",
      "3. Required parameters - You can also specify which parameters are required. This is\n",
      "important for the model to understand which parameters are actually necessary for the\n",
      "function to work. Later on in Step 3, the model will use this information to provide as\n",
      "minimal information as necessary to call the function.\n",
      "Ó™Ä Tip\n",
      "Keep your functions as simple as possible. In the above example, you'll notice that\n",
      "not all functions have descriptions where the function name is self-explanatory. This\n",
      "is intentional to reduce the number of tokens. The parameters are also kept simple;\n",
      "anything the model shouldn't need to know (like the cartId  or paymentId ) are kept\n",
      "hidden. This information is instead provided by internal services.\n",
      "Ôºó Note\n",
      "The one thing you don't need to worry about is the complexity of the return types.\n",
      "You'll notice that the return types are not serialized in the schema. This is because\n",
      "the model doesn't need to know the return type to generate a response. In Step 6,\n",
      "however, we'll see how overly verbose return types can impact the quality of the chat\n",
      "completion.\n",
      "Ó™Ä Tip\n",
      "Avoid, where possible, using string  as a parameter type. The model can't infer the\n",
      "type of string, which can lead to ambiguous responses. Instead, use enums or other\n",
      "types (e.g., int , float , and complex types) where possible.\n",
      "4. Function descriptions ‚Äì Function descriptions are optional but can help the model\n",
      "generate more accurate responses. In particular, descriptions can tell the model what to\n",
      "expect from the response since the return type is not serialized in the schema. If the\n",
      "model is using functions improperly, you can also add descriptions to provide examples\n",
      "and guidance.\n",
      "For example, in the get_pizza_from_cart  function, the description tells the user to use this\n",
      "function instead of relying on previous messages. This is important because the cart may\n",
      "have changed since the last message.\n",
      "5. Plugin name ‚Äì As you can see in the serialized functions, each function has a name\n",
      "property. Semantic Kernel uses the plugin name to namespace the functions. This is\n",
      "important because it allows you to have multiple plugins with functions of the same\n",
      "name. For example, you may have plugins for multiple search services, each with their\n",
      "own search  function. By namespacing the functions, you can avoid conflicts and make it\n",
      "easier for the model to understand which function to call.\n",
      "Knowing this, you should choose a plugin name that is unique and descriptive. In the\n",
      "above example, the plugin name is OrderPizza . This makes it clear that the functions are\n",
      "related to ordering pizza.\n",
      "Ó™Ä Tip\n",
      "Only mark parameters as required if they are actually required. This helps the model\n",
      "call functions more quickly and accurately.\n",
      "Ó™Ä Tip\n",
      "Before adding a description, ask yourself if the model needs this information to\n",
      "generate a response. If not, consider leaving it out to reduce verbosity. You can\n",
      "always add descriptions later if the model is struggling to use the function properly.\n",
      "Ó™Ä Tip\n",
      "When choosing a plugin name, we recommend removing superfluous words like\n",
      "\"plugin\" or \"service\". This helps reduce verbosity and makes the plugin name easier\n",
      "to understand for the model.\n",
      "Ôºó Note\n",
      "Once the functions are serialized, they are sent to the model along with the current chat\n",
      "history. This allows the model to understand the context of the conversation and the available\n",
      "functions.\n",
      "In this scenario, we can imagine the user asking the assistant to add a pizza to their cart:\n",
      "C#\n",
      "We can then send this chat history and the serialized functions to the model. The model will\n",
      "use this information to determine the best way to respond.\n",
      "C#\n",
      "By default, the delimiter for the function name is - . While this works for most\n",
      "models, some of them may have different requirements, such as Gemini\n",
      ". This is\n",
      "taken care of by the kernel automatically however you may see slightly different\n",
      "function names in the serialized functions.\n",
      "2) Sending the messages and functions to the model\n",
      "ChatHistory chatHistory = [];\n",
      "chatHistory.AddUserMessage(\"I'd like to order a pizza!\");\n",
      "IChatCompletionService chatCompletion = \n",
      "kernel.GetRequiredService<IChatCompletionService>();\n",
      "OpenAIPromptExecutionSettings openAIPromptExecutionSettings = new()¬†\n",
      "{\n",
      "    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()\n",
      "};\n",
      "ChatResponse response = await chatCompletion.GetChatMessageContentAsync(\n",
      "    chatHistory,\n",
      "    executionSettings: openAIPromptExecutionSettings,\n",
      "    kernel: kernel)\n",
      "Ôºó Note\n",
      "This example uses the FunctionChoiceBehavior.Auto()  behavior, one of the few available\n",
      "ones. For more information about other function choice behaviors, check out the function\n",
      "choice behaviors article.\n",
      "Ôºâ Important\n",
      "With both the chat history and the serialized functions, the model can determine the best way\n",
      "to respond. In this case, the model recognizes that the user wants to order a pizza. The model\n",
      "would likely want to call the add_pizza_to_cart  function, but because we specified the size and\n",
      "toppings as required parameters, the model will ask the user for this information:\n",
      "C#\n",
      "Since the model wants the user to respond next, Semantic Kernel will stop automatic function\n",
      "calling and return control to the user. At this point, the user can respond with the size and\n",
      "toppings of the pizza they want to order:\n",
      "C#\n",
      "Now that the model has the necessary information, it can now call the add_pizza_to_cart\n",
      "function with the user's input. Behind the scenes, it adds a new message to the chat history\n",
      "that looks like this:\n",
      "C#\n",
      "The kernel must be passed to the service in order to use function calling. This is because\n",
      "the plugins are registered with the kernel, and the service needs to know which plugins\n",
      "are available.\n",
      "3) Model processes the input\n",
      "Console.WriteLine(response);\n",
      "chatHistory.AddAssistantMessage(response);\n",
      "// \"Before I can add a pizza to your cart, I need to\n",
      "// know the size and toppings. What size pizza would\n",
      "// you like? Small, medium, or large?\"\n",
      "chatHistory.AddUserMessage(\"I'd like a medium pizza with cheese and pepperoni, \n",
      "please.\");\n",
      "response = await chatCompletion.GetChatMessageContentAsync(\n",
      "    chatHistory,\n",
      "    kernel: kernel)\n",
      "\"tool_calls\": [\n",
      "    {\n",
      "        \"id\": \"call_abc123\",\n",
      "        \"type\": \"function\",\n",
      "        \"function\": {\n",
      "            \"name\": \"OrderPizzaPlugin-add_pizza_to_cart\",\n",
      "When Semantic Kernel receives the response from the model, it checks if the response is a\n",
      "function call. If it is, Semantic Kernel extracts the function name and its parameters. In this case,\n",
      "the function name is OrderPizzaPlugin-add_pizza_to_cart , and the arguments are the size and\n",
      "toppings of the pizza.\n",
      "With this information, Semantic Kernel can marshal the inputs into the appropriate types and\n",
      "pass them to the add_pizza_to_cart  function in the OrderPizzaPlugin . In this example, the\n",
      "arguments originate as a JSON string but are deserialized by Semantic Kernel into a PizzaSize\n",
      "enum and a List<PizzaToppings> .\n",
      "After marshalling the inputs, Semantic Kernel will also add the function call to the chat history:\n",
      "C#\n",
      "            \"arguments\": \"{\\n\\\"size\\\": \\\"Medium\\\",\\n\\\"toppings\\\": [\\\"Cheese\\\", \n",
      "\\\"Pepperoni\\\"]\\n}\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "Ó™Ä Tip\n",
      "It's good to remember that every argument you require must be generated by the model.\n",
      "This means spending tokens to generate the response. Avoid arguments that require\n",
      "many tokens (like a GUID). For example, notice that we use an int  for the pizzaId . Asking\n",
      "the model to send a one to two digit number is much easier than asking for a GUID.\n",
      "Ôºâ Important\n",
      "This step is what makes function calling so powerful. Previously, AI app developers had to\n",
      "create separate processes to extract intent and slot fill functions. With function calling, the\n",
      "model can decide when to call a function and what information to provide.\n",
      "4) Handle the response\n",
      "Ôºó Note\n",
      "Marshaling the inputs into the correct types is one of the key benefits of using Semantic\n",
      "Kernel. Everything from the model comes in as a JSON object, but Semantic Kernel can\n",
      "automatically deserialize these objects into the correct types for your functions.\n",
      "Once Semantic Kernel has the correct types, it can finally invoke the add_pizza_to_cart\n",
      "function. Because the plugin uses dependency injection, the function can interact with external\n",
      "services like pizzaService  and userContext  to add the pizza to the user's cart.\n",
      "Not all functions will succeed, however. If the function fails, Semantic Kernel can handle the\n",
      "error and provide a default response to the model. This allows the model to understand what\n",
      "went wrong and decide to retry or generate a response to the user.\n",
      "After the function has been invoked, the function result is sent back to the model as part of the\n",
      "chat history. This allows the model to understand the context of the conversation and generate\n",
      "a subsequent response.\n",
      "chatHistory.Add(\n",
      "    new() {\n",
      "        Role = AuthorRole.Assistant,\n",
      "        Items = [\n",
      "            new FunctionCallContent(\n",
      "                functionName: \"add_pizza_to_cart\",\n",
      "                pluginName: \"OrderPizza\",\n",
      "                id: \"call_abc123\",\n",
      "                arguments: new () { {\"size\", \"Medium\"}, {\"toppings\", [\"Cheese\", \n",
      "\"Pepperoni\"]} }\n",
      "            )\n",
      "        ]\n",
      "    }\n",
      ");\n",
      "5) Invoke the function\n",
      "Ó™Ä Tip\n",
      "To ensure a model can self-correct, it's important to provide error messages that clearly\n",
      "communicate what went wrong and how to fix it. This can help the model retry the\n",
      "function call with the correct information.\n",
      "Ôºó Note\n",
      "Semantic Kernel automatically invokes functions by default. However, if you prefer to\n",
      "manage function invocation manually, you can enable manual function invocation mode.\n",
      "For more details on how to do this, please refer to the function invocation article.\n",
      "6) Return the function result\n",
      "Behind the scenes, Semantic Kernel adds a new message to the chat history from the tool role\n",
      "that looks like this:\n",
      "C#\n",
      "Notice that the result is a JSON string that the model then needs to process. As before, the\n",
      "model will need to spend tokens consuming this information. This is why it's important to keep\n",
      "the return types as simple as possible. In this case, the return only includes the new items\n",
      "added to the cart, not the entire cart.\n",
      "After the result is returned to the model, the process repeats. The model processes the latest\n",
      "chat history and generates a response. In this case, the model might ask the user if they want\n",
      "to add another pizza to their cart or if they want to check out.\n",
      "In the above example, we demonstrated how an LLM can call a single function. Often this can\n",
      "be slow if you need to call multiple functions in sequence. To speed up the process, several\n",
      "LLMs support parallel function calls. This allows the LLM to call multiple functions at once,\n",
      "speeding up the process.\n",
      "chatHistory.Add(\n",
      "    new() {\n",
      "        Role = AuthorRole.Tool,\n",
      "        Items = [\n",
      "            new FunctionResultContent(\n",
      "                functionName: \"add_pizza_to_cart\",\n",
      "                pluginName: \"OrderPizza\",\n",
      "                id: \"0001\",\n",
      "                result: \"{ \\\"new_items\\\": [ { \\\"id\\\": 1, \\\"size\\\": \\\"Medium\\\", \n",
      "\\\"toppings\\\": [\\\"Cheese\\\",\\\"Pepperoni\\\"] } ] }\"\n",
      "            )\n",
      "        ]\n",
      "    }\n",
      ");\n",
      "Ó™Ä Tip\n",
      "Be as succinct as possible with your returns. Where possible, only return the information\n",
      "the model needs or summarize the information using another LLM prompt before\n",
      "returning it.\n",
      "Repeat steps 2-6\n",
      "Parallel function calls\n",
      "For example, if a user wants to order multiple pizzas, the LLM can call the add_pizza_to_cart\n",
      "function for each pizza at the same time. This can significantly reduce the number of round\n",
      "trips to the LLM and speed up the ordering process.\n",
      "Now that you understand how function calling works, you can proceed to learn how to\n",
      "configure various aspects of function calling that better correspond to your specific scenarios\n",
      "by going to the next step:\n",
      "Next steps\n",
      "Function Choice Behavior\n",
      "Function Choice Behaviors\n",
      "Article ‚Ä¢ 05/06/2025\n",
      "Function choice behaviors are bits of configuration that allows a developer to configure:\n",
      "1. Which functions are advertised to AI models.\n",
      "2. How the models should choose them for invocation.\n",
      "3. How Semantic Kernel might invoke those functions.\n",
      "As of today, the function choice behaviors are represented by three static methods of the\n",
      "FunctionChoiceBehavior  class:\n",
      "Auto: Allows the AI model to choose from zero or more function(s) from the provided\n",
      "function(s) for invocation.\n",
      "Required: Forces the AI model to choose one or more function(s) from the provided\n",
      "function(s) for invocation.\n",
      "None: Instructs the AI model not to choose any function(s).\n",
      "Function advertising is the process of providing functions to AI models for further calling and\n",
      "invocation. All three function choice behaviors accept a list of functions to advertise as a\n",
      "functions  parameter. By default, it is null, which means all functions from plugins registered\n",
      "on the Kernel are provided to the AI model.\n",
      "C#\n",
      "Ôºó Note\n",
      "If your code uses the function-calling capabilities represented by the ToolCallBehavior\n",
      "class, please refer to the migration guide to update the code to the latest function-calling\n",
      "model.\n",
      "Ôºó Note\n",
      "The function-calling capabilities is only supported by a few AI connectors so far, see the\n",
      "Supported AI Connectors section below for more details.\n",
      "Function Advertising\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "If a list of functions is provided, only those functions are sent to the AI model:\n",
      "C#\n",
      "An empty list of functions means no functions are provided to the AI model, which is\n",
      "equivalent to disabling function calling.\n",
      "C#\n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "Kernel kernel = builder.Build();\n",
      "// All functions from the DateTimeUtils and WeatherForecastUtils plugins will be \n",
      "sent to AI model together with the prompt.\n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "FunctionChoiceBehavior.Auto() }; \n",
      "await kernel.InvokePromptAsync(\"Given the current time of day and weather, what is \n",
      "the likely color of the sky in Boston?\", new(settings));\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "Kernel kernel = builder.Build();\n",
      "KernelFunction getWeatherForCity = \n",
      "kernel.Plugins.GetFunction(\"WeatherForecastUtils\", \"GetWeatherForCity\");\n",
      "KernelFunction getCurrentTime = kernel.Plugins.GetFunction(\"DateTimeUtils\", \n",
      "\"GetCurrentUtcDateTime\");\n",
      "// Only the specified getWeatherForCity and getCurrentTime functions will be sent \n",
      "to AI model alongside the prompt.\n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "FunctionChoiceBehavior.Auto(functions: [getWeatherForCity, getCurrentTime]) }; \n",
      "await kernel.InvokePromptAsync(\"Given the current time of day and weather, what is \n",
      "the likely color of the sky in Boston?\", new(settings));\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "The Auto  function choice behavior instructs the AI model to choose from zero or more\n",
      "function(s) from the provided function(s) for invocation.\n",
      "In this example, all functions from the DateTimeUtils  and WeatherForecastUtils  plugins will be\n",
      "provided to the AI model alongside the prompt. The model will first choose GetCurrentTime\n",
      "function for invocation to obtain the current date and time, as this information is needed as\n",
      "input for the GetWeatherForCity  function. Next, it will choose GetWeatherForCity  function for\n",
      "invocation to get the weather forecast for the city of Boston using the obtained date and time.\n",
      "With this information, the model will be able to determine the likely color of the sky in Boston.\n",
      "C#\n",
      "The same example can be easily modeled in a YAML prompt template configuration:\n",
      "C#\n",
      "Kernel kernel = builder.Build();\n",
      "// Disables function calling. Equivalent to var settings = new() { \n",
      "FunctionChoiceBehavior = null } or var settings = new() { }.\n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "FunctionChoiceBehavior.Auto(functions: []) }; \n",
      "await kernel.InvokePromptAsync(\"Given the current time of day and weather, what is \n",
      "the likely color of the sky in Boston?\", new(settings));\n",
      "Using Auto Function Choice Behavior\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "Kernel kernel = builder.Build();\n",
      "// All functions from the DateTimeUtils and WeatherForecastUtils plugins will be \n",
      "provided to AI model alongside the prompt.\n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "FunctionChoiceBehavior.Auto() }; \n",
      "await kernel.InvokePromptAsync(\"Given the current time of day and weather, what is \n",
      "the likely color of the sky in Boston?\", new(settings));\n",
      "The Required  behavior forces the model to choose one or more function(s) from the provided\n",
      "function(s) for invocation. This is useful for scenarios when the AI model must obtain required\n",
      "information from the specified functions rather than from it's own knowledge.\n",
      "Here, we specify that the AI model must choose the GetWeatherForCity  function for invocation\n",
      "to obtain the weather forecast for the city of Boston, rather than guessing it based on its own\n",
      "knowledge. The model will first choose the GetWeatherForCity  function for invocation to\n",
      "retrieve the weather forecast. With this information, the model can then determine the likely\n",
      "color of the sky in Boston using the response from the call to GetWeatherForCity .\n",
      "C#\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "Kernel kernel = builder.Build();\n",
      "string promptTemplateConfig = \"\"\"\n",
      "    template_format: semantic-kernel\n",
      "    template: Given the current time of day and weather, what is the likely color \n",
      "of the sky in Boston?\n",
      "    execution_settings:\n",
      "      default:\n",
      "        function_choice_behavior:\n",
      "          type: auto\n",
      "    \"\"\";\n",
      "KernelFunction promptFunction = \n",
      "KernelFunctionYaml.FromPromptYaml(promptTemplateConfig);\n",
      "Console.WriteLine(await kernel.InvokeAsync(promptFunction));\n",
      "Using Required Function Choice Behavior\n",
      "Ôºó Note\n",
      "The behavior advertises functions in the first request to the AI model only and stops\n",
      "sending them in subsequent requests to prevent an infinite loop where the model keeps\n",
      "choosing the same functions for invocation repeatedly.\n",
      "An identical example in a YAML template configuration:\n",
      "C#\n",
      "Alternatively, all functions registered in the kernel can be provided to the AI model as required.\n",
      "However, only the ones chosen by the AI model as a result of the first request will be invoked\n",
      "by the Semantic Kernel. The functions will not be sent to the AI model in subsequent requests\n",
      "to prevent an infinite loop, as mentioned above.\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "Kernel kernel = builder.Build();\n",
      "KernelFunction getWeatherForCity = \n",
      "kernel.Plugins.GetFunction(\"WeatherForecastUtils\", \"GetWeatherForCity\");\n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "FunctionChoiceBehavior.Required(functions: [getWeatherFunction]) };\n",
      "await kernel.InvokePromptAsync(\"Given that it is now the 10th of September 2024, \n",
      "11:29 AM, what is the likely color of the sky in Boston?\", new(settings));\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "Kernel kernel = builder.Build();\n",
      "string promptTemplateConfig = \"\"\"\n",
      "    template_format: semantic-kernel\n",
      "    template: Given that it is now the 10th of September 2024, 11:29 AM, what is \n",
      "the likely color of the sky in Boston?\n",
      "    execution_settings:\n",
      "      default:\n",
      "        function_choice_behavior:\n",
      "          type: required\n",
      "          functions:\n",
      "            - WeatherForecastUtils.GetWeatherForCity\n",
      "    \"\"\";\n",
      "KernelFunction promptFunction = \n",
      "KernelFunctionYaml.FromPromptYaml(promptTemplateConfig);\n",
      "Console.WriteLine(await kernel.InvokeAsync(promptFunction));\n",
      "C#\n",
      "The None  behavior instructs the AI model to use the provided function(s) without choosing any\n",
      "of them for invocation and instead generate a message response. This is useful for dry runs\n",
      "when the caller may want to see which functions the model would choose without actually\n",
      "invoking them. For instance in the sample below the AI model correctly lists the functions it\n",
      "would choose to determine the color of the sky in Boston.\n",
      "C#\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "Kernel kernel = builder.Build();\n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "FunctionChoiceBehavior.Required() };\n",
      "await kernel.InvokePromptAsync(\"Given that it is now the 10th of September 2024, \n",
      "11:29 AM, what is the likely color of the sky in Boston?\", new(settings));\n",
      "Using None Function Choice Behavior\n",
      "Here, we advertise all functions from the `DateTimeUtils` and \n",
      "`WeatherForecastUtils` plugins to the AI model but instruct it not to choose any \n",
      "of them.\n",
      "Instead, the model will provide a response describing which functions it would \n",
      "choose to determine the color of the sky in Boston on a specified date.\n",
      "```csharp\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "Kernel kernel = builder.Build();\n",
      "KernelFunction getWeatherForCity = \n",
      "kernel.Plugins.GetFunction(\"WeatherForecastUtils\", \"GetWeatherForCity\");\n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "FunctionChoiceBehavior.None() };\n",
      "await kernel.InvokePromptAsync(\"Specify which provided functions are needed to \n",
      "A corresponding example in a YAML prompt template configuration:\n",
      "C#\n",
      "Certain aspects of the function choice behaviors can be configured through options that each\n",
      "function choice behavior class accepts via the options  constructor parameter of the\n",
      "FunctionChoiceBehaviorOptions  type. The following options are available:\n",
      "AllowConcurrentInvocation: This option enables the concurrent invocation of functions\n",
      "by the Semantic Kernel. By default, it is set to false, meaning that functions are invoked\n",
      "sequentially. Concurrent invocation is only possible if the AI model can choose multiple\n",
      "determine the color of the sky in Boston on a specified date.\", new(settings))\n",
      "// Sample response: To determine the color of the sky in Boston on a specified \n",
      "date, first call the DateTimeUtils-GetCurrentUtcDateTime function to obtain the \n",
      "// current date and time in UTC. Next, use the WeatherForecastUtils-\n",
      "GetWeatherForCity function, providing 'Boston' as the city name and the retrieved \n",
      "UTC date and time. \n",
      "// These functions do not directly provide the sky's color, but the \n",
      "GetWeatherForCity function offers weather data, which can be used to infer the \n",
      "general sky condition (e.g., clear, cloudy, rainy).\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "Kernel kernel = builder.Build();\n",
      "string promptTemplateConfig = \"\"\"\n",
      "    template_format: semantic-kernel\n",
      "    template: Specify which provided functions are needed to determine the color \n",
      "of the sky in Boston on a specified date.\n",
      "    execution_settings:\n",
      "      default:\n",
      "        function_choice_behavior:\n",
      "          type: none\n",
      "    \"\"\";\n",
      "KernelFunction promptFunction = \n",
      "KernelFunctionYaml.FromPromptYaml(promptTemplateConfig);\n",
      "Console.WriteLine(await kernel.InvokeAsync(promptFunction));\n",
      "Function Choice Behavior Options\n",
      "functions for invocation in a single request; otherwise, there is no distinction between\n",
      "sequential and concurrent invocation\n",
      "AllowParallelCalls: This option allows the AI model to choose multiple functions in one\n",
      "request. Some AI models may not support this feature; in such cases, the option will have\n",
      "no effect. By default, this option is set to null, indicating that the AI model's default\n",
      "behavior will be used.\n",
      "Function invocation is the process whereby Semantic Kernel invokes functions chosen by the AI\n",
      "model. For more details on function invocation see function invocation article.\n",
      "As of today, the following AI connectors in Semantic Kernel support the function calling model:\n",
      "AI Connector\n",
      "FunctionChoiceBehavior\n",
      "ToolCallBehavior\n",
      "Anthropic\n",
      "Planned\n",
      "‚ùå\n",
      "AzureAIInference\n",
      "Coming soon\n",
      "‚ùå\n",
      "AzureOpenAI\n",
      "‚úîÔ∏è\n",
      "‚úîÔ∏è\n",
      "The following table summarizes the effects of various combinations of the \n",
      "AllowParallelCalls and AllowConcurrentInvocation options:\n",
      "| AllowParallelCalls  | AllowConcurrentInvocation | # of functions chosen per \n",
      "AI roundtrip  | Concurrent Invocation by SK |\n",
      "|---------------------|---------------------------|--------------------------\n",
      "---------------|-----------------------|\n",
      "| false               | false                     | one                         \n",
      "| false                 |\n",
      "| false               | true                      | one                         \n",
      "| false*                |\n",
      "| true                | false                     | multiple                    \n",
      "| false                 |\n",
      "| true                | true                      | multiple                    \n",
      "| true                  |\n",
      "`*` There's only one function to invoke\n",
      "Function Invocation\n",
      "Supported AI Connectors\n",
      "Ôæâ\n",
      "Expand table\n",
      "AI Connector\n",
      "FunctionChoiceBehavior\n",
      "ToolCallBehavior\n",
      "Gemini\n",
      "Planned\n",
      "‚úîÔ∏è\n",
      "HuggingFace\n",
      "Planned\n",
      "‚ùå\n",
      "Mistral\n",
      "Planned\n",
      "‚úîÔ∏è\n",
      "Ollama\n",
      "Coming soon\n",
      "‚ùå\n",
      "Onnx\n",
      "Coming soon\n",
      "‚ùå\n",
      "OpenAI\n",
      "‚úîÔ∏è\n",
      "‚úîÔ∏è\n",
      "Function Invocation Modes\n",
      "Article ‚Ä¢ 11/23/2024\n",
      "When the AI model receives a prompt containing a list of functions, it may choose one\n",
      "or more of them for invocation to complete the prompt. When a function is chosen by\n",
      "the model, it needs be invoked by Semantic Kernel.\n",
      "The function calling subsystem in Semantic Kernel has two modes of function\n",
      "invocation: auto and manual.\n",
      "Depending on the invocation mode, Semantic Kernel either does end-to-end function\n",
      "invocation or gives the caller control over the function invocation process.\n",
      "Auto function invocation is the default mode of the Semantic Kernel function-calling\n",
      "subsystem. When the AI model chooses one or more functions, Semantic Kernel\n",
      "automatically invokes the chosen functions. The results of these function invocations are\n",
      "added to the chat history and sent to the model automatically in subsequent requests.\n",
      "The model then reasons about the chat history, chooses additional functions if needed,\n",
      "or generates the final response. This approach is fully automated and requires no\n",
      "manual intervention from the caller.\n",
      "This example demonstrates how to use the auto function invocation in Semantic Kernel.\n",
      "AI model decides which functions to call to complete the prompt and Semantic Kernel\n",
      "does the rest and invokes them automatically.\n",
      "C#\n",
      "Auto Function Invocation\n",
      "Ó™Ä Tip\n",
      "Auto function invocation is different from the auto function choice behavior. The\n",
      "former dictates if functions should be invoked automatically by Semantic Kernel,\n",
      "while the latter determines if functions should be chosen automatically by the AI\n",
      "model.\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "Some AI models support parallel function calling, where the model chooses multiple\n",
      "functions for invocation. This can be useful in cases when invoking chosen functions\n",
      "takes a long time. For example, the AI may choose to retrieve the latest news and the\n",
      "current time simultaneously, rather than making a round trip per function.\n",
      "Semantic Kernel can invoke these functions in two different ways:\n",
      "Sequentially: The functions are invoked one after another. This is the default\n",
      "behavior.\n",
      "Concurrently: The functions are invoked at the same time. This can be enabled by\n",
      "setting the FunctionChoiceBehaviorOptions.AllowConcurrentInvocation  property to\n",
      "true , as shown in the example below.\n",
      "C#\n",
      "Kernel kernel = builder.Build();\n",
      "// By default, functions are set to be automatically invoked.  \n",
      "// If you want to explicitly enable this behavior, you can do so with the \n",
      "following code:  \n",
      "// PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "FunctionChoiceBehavior.Auto(autoInvoke: true) };  \n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "FunctionChoiceBehavior.Auto() }; \n",
      "await kernel.InvokePromptAsync(\"Given the current time of day and weather, \n",
      "what is the likely color of the sky in Boston?\", new(settings));\n",
      "using Microsoft.SemanticKernel;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<NewsUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "Kernel kernel = builder.Build();\n",
      "// Enable concurrent invocation of functions to get the latest news and the \n",
      "current time.\n",
      "FunctionChoiceBehaviorOptions options = new() { AllowConcurrentInvocation = \n",
      "true };\n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "FunctionChoiceBehavior.Auto(options: options) }; \n",
      "await kernel.InvokePromptAsync(\"Good morning! What is the current time and \n",
      "latest news headlines?\", new(settings));\n",
      "In cases when the caller wants to have more control over the function invocation\n",
      "process, manual function invocation can be used.\n",
      "When manual function invocation is enabled, Semantic Kernel does not automatically\n",
      "invoke the functions chosen by the AI model. Instead, it returns a list of chosen\n",
      "functions to the caller, who can then decide which functions to invoke, invoke them\n",
      "sequentially or in parallel, handle exceptions, and so on. The function invocation results\n",
      "need to be added to the chat history and returned to the model, which will reason\n",
      "about them and decide whether to choose additional functions or generate a final\n",
      "response.\n",
      "The example below demonstrates how to use manual function invocation.\n",
      "C#\n",
      "Manual Function Invocation\n",
      "using Microsoft.SemanticKernel;\n",
      "using Microsoft.SemanticKernel.ChatCompletion;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "Kernel kernel = builder.Build();\n",
      "IChatCompletionService chatCompletionService = \n",
      "kernel.GetRequiredService<IChatCompletionService>();\n",
      "// Manual function invocation needs to be enabled explicitly by setting \n",
      "autoInvoke to false.\n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "Microsoft.SemanticKernel.FunctionChoiceBehavior.Auto(autoInvoke: false) };\n",
      "ChatHistory chatHistory = [];\n",
      "chatHistory.AddUserMessage(\"Given the current time of day and weather, what \n",
      "is the likely color of the sky in Boston?\");\n",
      "while (true)\n",
      "{\n",
      "    ChatMessageContent result = await \n",
      "chatCompletionService.GetChatMessageContentAsync(chatHistory, settings, \n",
      "kernel);\n",
      "    // Check if the AI model has generated a response.\n",
      "    if (result.Content is not null)\n",
      "    {\n",
      "        Console.Write(result.Content);\n",
      "        // Sample output: \"Considering the current weather conditions in \n",
      "Boston with a tornado watch in effect resulting in potential severe \n",
      "thunderstorms,\n",
      "        // the sky color is likely unusual such as green, yellow, or dark \n",
      "gray. Please stay safe and follow instructions from local authorities.\"\n",
      "        break;\n",
      "    }\n",
      "    // Adding AI model response containing chosen functions to chat history \n",
      "as it's required by the models to preserve the context.\n",
      "    chatHistory.Add(result); \n",
      "    // Check if the AI model has chosen any function for invocation.\n",
      "    IEnumerable<FunctionCallContent> functionCalls = \n",
      "FunctionCallContent.GetFunctionCalls(result);\n",
      "    if (!functionCalls.Any())\n",
      "    {\n",
      "        break;\n",
      "    }\n",
      "    // Sequentially iterating over each chosen function, invoke it, and add \n",
      "the result to the chat history.\n",
      "    foreach (FunctionCallContent functionCall in functionCalls)\n",
      "    {\n",
      "        try\n",
      "        {\n",
      "            // Invoking the function\n",
      "            FunctionResultContent resultContent = await \n",
      "functionCall.InvokeAsync(kernel);\n",
      "            // Adding the function result to the chat history\n",
      "            chatHistory.Add(resultContent.ToChatMessage());\n",
      "        }\n",
      "        catch (Exception ex)\n",
      "        {\n",
      "            // Adding function exception to the chat history.\n",
      "            chatHistory.Add(new FunctionResultContent(functionCall, \n",
      "ex).ToChatMessage());\n",
      "            // or\n",
      "            //chatHistory.Add(new FunctionResultContent(functionCall, \"Error \n",
      "details that the AI model can reason about.\").ToChatMessage());\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Ôºó Note\n",
      "The FunctionCallContent and FunctionResultContent classes are used to represent\n",
      "AI model function calls and Semantic Kernel function invocation results,\n",
      "respectively. They contain information about chosen function, such as the function\n",
      "The following example demonstrates how to use manual function invocation with the\n",
      "streaming chat completion API. Note the usage of the FunctionCallContentBuilder  class\n",
      "to build function calls from the streaming content. Due to the streaming nature of the\n",
      "API, function calls are also streamed. This means that the caller must build the function\n",
      "calls from the streaming content before invoking them.\n",
      "C#\n",
      "ID, name, and arguments, and function invocation results, such as function call ID\n",
      "and result.\n",
      "using Microsoft.SemanticKernel;\n",
      "using Microsoft.SemanticKernel.ChatCompletion;\n",
      "IKernelBuilder builder = Kernel.CreateBuilder(); \n",
      "builder.AddOpenAIChatCompletion(\"<model-id>\", \"<api-key>\");\n",
      "builder.Plugins.AddFromType<WeatherForecastUtils>();\n",
      "builder.Plugins.AddFromType<DateTimeUtils>(); \n",
      "Kernel kernel = builder.Build();\n",
      "IChatCompletionService chatCompletionService = \n",
      "kernel.GetRequiredService<IChatCompletionService>();\n",
      "// Manual function invocation needs to be enabled explicitly by setting \n",
      "autoInvoke to false.\n",
      "PromptExecutionSettings settings = new() { FunctionChoiceBehavior = \n",
      "Microsoft.SemanticKernel.FunctionChoiceBehavior.Auto(autoInvoke: false) };\n",
      "ChatHistory chatHistory = [];\n",
      "chatHistory.AddUserMessage(\"Given the current time of day and weather, what \n",
      "is the likely color of the sky in Boston?\");\n",
      "while (true)\n",
      "{\n",
      "    AuthorRole? authorRole = null;\n",
      "    FunctionCallContentBuilder fccBuilder = new ();\n",
      "    // Start or continue streaming chat based on the chat history\n",
      "    await foreach (StreamingChatMessageContent streamingContent in \n",
      "chatCompletionService.GetStreamingChatMessageContentsAsync(chatHistory, \n",
      "settings, kernel))\n",
      "    {\n",
      "        // Check if the AI model has generated a response.\n",
      "        if (streamingContent.Content is not null)\n",
      "        {\n",
      "            Console.Write(streamingContent.Content);\n",
      "            // Sample streamed output: \"The color of the sky in Boston is \n",
      "likely to be gray due to the rainy weather.\"\n",
      "        }\n",
      "        authorRole ??= streamingContent.Role;\n",
      "        // Collect function calls details from the streaming content\n",
      "        fccBuilder.Append(streamingContent);\n",
      "    }\n",
      "    // Build the function calls from the streaming content and quit the chat \n",
      "loop if no function calls are found\n",
      "    IReadOnlyList<FunctionCallContent> functionCalls = fccBuilder.Build();\n",
      "    if (!functionCalls.Any())\n",
      "    {\n",
      "        break;\n",
      "    }\n",
      "    // Creating and adding chat message content to preserve the original \n",
      "function calls in the chat history.\n",
      "    // The function calls are added to the chat message a few lines below.\n",
      "    ChatMessageContent fcContent = new ChatMessageContent(role: authorRole \n",
      "?? default, content: null);\n",
      "    chatHistory.Add(fcContent);\n",
      "    // Iterating over the requested function calls and invoking them.\n",
      "    // The code can easily be modified to invoke functions concurrently if \n",
      "needed.\n",
      "    foreach (FunctionCallContent functionCall in functionCalls)\n",
      "    {\n",
      "        // Adding the original function call to the chat message content\n",
      "        fcContent.Items.Add(functionCall);\n",
      "        // Invoking the function\n",
      "        FunctionResultContent functionResult = await \n",
      "functionCall.InvokeAsync(kernel);\n",
      "        // Adding the function result to the chat history\n",
      "        chatHistory.Add(functionResult.ToChatMessage());\n",
      "    }\n",
      "}\n",
      "Text Embedding generation in Semantic\n",
      "Kernel\n",
      "Article ‚Ä¢ 11/13/2024\n",
      "With text embedding generation, you can use an AI model to generate vectors (aka\n",
      "embeddings). These vectors encode the semantic meaning of the text in such a way that\n",
      "mathematical equations can be used on two vectors to compare the similarity of the\n",
      "original text. This is useful for scenarios such as Retrieval Augmented Generation (RAG),\n",
      "where we want to search a database of information for text related to a user query. Any\n",
      "matching information can then be provided as input to Chat Completion, so that the AI\n",
      "Model has more context when answering the user query.\n",
      "When choosing an embedding model, you will need to consider the following:\n",
      "What is the size of the vectors generated by the model, and is it configurable, as\n",
      "this will affect your vector storage cost.\n",
      "What type of elements does the generated vectors contain, e.g. float32, float16,\n",
      "etc, as this will affect your vector storage cost.\n",
      "How fast does it generate vectors?\n",
      "How much does generation cost?\n",
      "Some of the AI Services can be hosted locally and may require some setup. Below are\n",
      "instructions for those that support this.\n",
      "Ó™Ä Tip\n",
      "For more information about storing and searching vectors see What are Semantic\n",
      "Kernel Vector Store connectors?\n",
      "Ó™Ä Tip\n",
      "For more information about using RAG with vector stores in Semantic Kernel, see\n",
      "How to use Vector Stores with Semantic Kernel Text Search and What are\n",
      "Semantic Kernel Text Search plugins?\n",
      "Setting up your local environment\n",
      "Azure OpenAI\n",
      "No local setup.\n",
      "Before adding embedding generation to your kernel, you will need to install the\n",
      "necessary packages. Below are the packages you will need to install for each AI service\n",
      "provider.\n",
      "Bash\n",
      "Now that you've installed the necessary packages, you can create a text embedd\n"
     ]
    }
   ],
   "source": [
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    doc.close()\n",
    "    return full_text\n",
    "\n",
    "# Load your Semantic Kernel PDF\n",
    "pdf_path = \"semantic-kernel.pdf\"  # update if different\n",
    "raw_text = extract_pdf_text(pdf_path)\n",
    "\n",
    "# Quick preview\n",
    "print(raw_text[:100000])  # Show first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102c86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-empty pages: 740\n",
      "Page 1 preview:\n",
      "Tell us about your PDF experience.\n",
      "Introduction to Semantic Kernel\n",
      "Article ‚Ä¢ 06/24/2024\n",
      "Semantic Kernel is a lightweight, open-source development kit that lets you easily build\n",
      "AI agents and integrate the latest AI models into your C#, Python, or Java codebase. It\n",
      "serves as an efficient middleware that enables rapid delivery of enterprise-grade\n",
      "solutions.\n",
      "Microsoft and other Fortune 500 companies are already leveraging Semantic Kernel\n",
      "because it‚Äôs flexible, modular, and observable. Backed with security enhancing\n",
      "capabilities like telemetry support, and hooks and filters so you‚Äôll feel confident you‚Äôre\n",
      "delivering responsible AI solutions at scale.\n",
      "Version 1.0+ support across C#, Python, and Java means it‚Äôs reliable, committed to non\n",
      "breaking changes. Any existing chat-based APIs are easily expanded to support\n",
      "additional modalities like voice and video.\n",
      "Semantic Kernel was designed to be future proof, easily connecting your code to the\n",
      "latest AI models evolving with the technology as it advances. When new models are\n",
      "released, you‚Äôll simply swap them out without needing to rewrite your entire codebase.\n",
      "Enterprise ready\n"
     ]
    }
   ],
   "source": [
    "# Load and chunk PDF page-wise\n",
    "def extract_pdf_pagewise(pdf_path: str):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    chunks = []\n",
    "\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        text = page.get_text().strip()\n",
    "        if text:  # Avoid blank pages\n",
    "            chunks.append({\n",
    "                \"page\": page_num,\n",
    "                \"text\": text\n",
    "            })\n",
    "\n",
    "    doc.close()\n",
    "    return chunks\n",
    "\n",
    "# Run the function\n",
    "page_chunks = extract_pdf_pagewise(\"semantic-kernel.pdf\")\n",
    "\n",
    "# Example output\n",
    "print(f\"Total non-empty pages: {len(page_chunks)}\")\n",
    "print(f\"Page 1 preview:\\n{page_chunks[0]['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed8287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a free, local embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f929b034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:32<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings: 740\n",
      "Shape of one embedding: (384,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract just the text for embedding\n",
    "texts = [chunk[\"text\"] for chunk in page_chunks]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Total embeddings: {len(embeddings)}\")\n",
    "print(f\"Shape of one embedding: {embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d934cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link Embedding to Chunk\n",
    "for i in range(len(page_chunks)):\n",
    "    page_chunks[i][\"embedding\"] = embeddings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ceacc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index has 740 vectors.\n"
     ]
    }
   ],
   "source": [
    "# Embeddings must be a 2D float32 numpy array\n",
    "embedding_matrix = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embedding_matrix.shape[1]  # typically 384 for MiniLM\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 = Euclidean Distance\n",
    "\n",
    "# Add vectors\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "print(f\"FAISS index has {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f51082f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the Index for Later\n",
    "faiss.write_index(index, \"semantic_kernel.index\")\n",
    "\n",
    "## To load it later\n",
    "# index = faiss.read_index(\"semantic_kernel.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a461e5bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GenerativeModel' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mHi\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Embed the query\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m query_embedding = model.encode([query]).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Search top-k most similar pages\u001b[39;00m\n\u001b[32m     10\u001b[39m k = \u001b[32m3\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'GenerativeModel' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Example query\n",
    "query = \"Hi\"\n",
    "\n",
    "# Embed the query\n",
    "query_embedding = model.encode([query]).astype('float32')\n",
    "\n",
    "# Search top-k most similar pages\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Display results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    page_info = page_chunks[idx]\n",
    "    print(f\"\\nüîπ Match {i+1} ‚Äî Page {page_info['page']}\")\n",
    "    print(f\"Text Preview:\\n{page_info['text'][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "736e98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Replace with your actual key\n",
    "GEMINI_API_KEY = \"AIzaSyDDCT4dPbOM9chf_Uaveg-BDYS_82nfaWs\"\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "535b986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemini model (text-only model)\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27295c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not mentioned.  The provided text describes an AI system's setup and a prompt template, but doesn't contain a response to \"Hi\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare prompt from top-k results\n",
    "retrieved_chunks = [page_chunks[idx][\"text\"] for idx in indices[0]]\n",
    "\n",
    "# You can trim or merge them\n",
    "context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "# Final prompt sent to Gemini\n",
    "prompt = f\"\"\"\n",
    "Hello! I have a task for you.\n",
    " \n",
    "You're a helpful and honest assistant. Please follow these rules carefully:\n",
    "1. Stick to the facts provided in the text.\n",
    "2. If something is not in the text, say \"Not mentioned\" or \"Cannot be determined.\"\n",
    "3. Be concise and clear.\n",
    "4. Do not make up or assume anything.\n",
    "5. **Do NOT generate or include any mature, violent, or harmful content.**\n",
    "6. **Avoid any language that could be interpreted as threatening or unsafe.**\n",
    "7. Be clear, concise, and respectful in your tone\n",
    " \n",
    "Now, based on the following content, please answer questions:\n",
    " \n",
    "Context:\n",
    "{context}\n",
    " \n",
    "Question: {query}\n",
    " \n",
    "Answer:\"\"\"\n",
    "# Get answer\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed303a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Lakshya', 'Bansal']\n",
      "['am', 'Lakshya', 'Bansal']\n",
      "I am Lakshya Bansal\n"
     ]
    }
   ],
   "source": [
    "Data = \"I am Lakshya Bansal\"\n",
    "\n",
    "word = Data.split()\n",
    "print(word)\n",
    "extra = []\n",
    "\n",
    "for i in word:\n",
    "    if i != \"I\":\n",
    "        extra.append(i)\n",
    "print(extra)\n",
    "\n",
    "result = \" \".join(word)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32551d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number\n"
     ]
    }
   ],
   "source": [
    "dic = {\n",
    "    (1, 2, 3): \"Number\"\n",
    "}\n",
    "\n",
    "print(dic[(1, 2, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bbcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41360ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b1f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['support@test.com', 'sales@company.org']\n",
      "hello\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_emails(text):\n",
    "    pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    emails = re.findall(pattern, text)\n",
    "    return emails\n",
    "\n",
    "# Example\n",
    "text = \"Contact us at support@test.com or sales@company.org\"\n",
    "print(extract_emails(text)) \n",
    "print(\"hello\") \n",
    "# ‚úÖ Output: ['support@test.com', 'sales@company.org']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c438c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine gninrael is yllaer interesting\n"
     ]
    }
   ],
   "source": [
    "def reverse_alternate_words(sentence):\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words)):\n",
    "        if i % 2 == 1:  # reverse alternate words (2nd, 4th, etc.)\n",
    "            words[i] = words[i][::-1]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Example\n",
    "sentence = \"Machine learning is really interesting\"\n",
    "print(reverse_alternate_words(sentence))  \n",
    "# ‚úÖ Output: \"Machine gninrael is yllaer interesting\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8cb02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
