{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0cc3a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\LakshyaBansal\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#For extraction\n",
    "import fitz  # PyMuPDF\n",
    "from typing import List\n",
    "import pickle\n",
    "#For Embedding \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "#Vector Indexing\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6ceae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Assessment Details\n",
      "Type: Proctored certification\n",
      "\n",
      "Total number of questions: 45\n",
      "\n",
      "Time limit: 90 minutes\n",
      "\n",
      "Registration fee: $200\n",
      "\n",
      "Question types: Multiple choice\n",
      "\n",
      "Languages: English\n",
      "\n",
      "Delivery method: Online proctored\n",
      "\n",
      "Prerequisites: None, but related training highly recommended\n",
      "\n",
      "Recommended experience: 6+ months of hands-on experience performing the generative AI solutions tasks outlined in the exam guide\n",
      "\n",
      "Validity period: 2 years\n",
      "\n",
      "Recertification: Recertification is required every two years to maintain your certified status. To recertify, you must take the current version of the exam. Please review the â€œGetting Ready for the Examâ€ section below to prepare for your recertification exam.\n",
      "\n",
      "Unscored content: Exams may include unscored items to gather statistical information for future use. These items are not identified on the form and do not impact your score. Additional time is factored into the exam to account for this content.\n",
      "\n",
      "âŒ âœ… \n",
      "\n",
      "Generative AI Engineering Learning Path\n",
      "01. Generative AI Fundamentals\n",
      "02. Generative AI Solution Development\n",
      "03. Generative AI Application Development\n",
      "04. Generative AI Application Evaluation and Governance\n",
      "05. Generative AI Application Deployment and Monitoring\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generative AI Fundamentals\n",
      "https://partner-academy.databricks.com/learn/courses/1765/generative-ai-fundamentals/lessons/12496/introduction-to-generative-ai-fundamentals\n",
      "\n",
      "\n",
      "Generative AI Solution Development\n",
      "https://partner-academy.databricks.com/learn/courses/2706/generative-ai-solution-development?hash=8303a20b2cb56c118b981d43447e112ad73f07bc&generated_by=459209\n",
      "\n",
      "\n",
      "Generative AI Application Development\n",
      "https://partner-academy.databricks.com/learn/courses/2716/generative-ai-application-development?hash=47bda03e5eabde26bd5e91bb464e48523181ee89&generated_by=459209\n",
      "\n",
      "\n",
      "Generative AI Application Evaluation and Governance\n",
      "https://partner-academy.databricks.com/learn/courses/2717/generative-ai-application-evaluation-and-governance?hash=f7fe429e2506b3a6ffe4554490d82d28373e2873&generated_by=459209\n",
      "\n",
      " \n",
      "Generative AI Application Deployment and Monitoring\n",
      "https://partner-academy.databricks.com/learn/courses/2713/generative-ai-application-deployment-and-monitoring?hash=f160f1406b89ff21e850c77946333a4c4352678e&generated_by=459209\n",
      "\n",
      "\n",
      "https://learn.microsoft.com/en-us/azure/databricks/machine-learning/\n",
      "\n",
      "\n",
      "Skill Sections for Databricks GEN AI Certifications \n",
      "\n",
      "Section 1: Design Applications (14%)\n",
      "â— Design a prompt that elicits a specifically formatted response\n",
      "â— Select model tasks to accomplish a given business requirement\n",
      "â— Select chain components for a desired model input and output\n",
      "â— Translate business use case goals into a description of the desired inputs and outputs for the AI pipeline\n",
      "â— Define and order tools that gather knowledge or take actions for multi-stage reasoning\n",
      "\n",
      "Section 2: Data Preparation (14%)\n",
      "â— Apply a chunking strategy for a given document structure and model constraints\n",
      "â— Filter extraneous content in source documents that degrades quality of a RAG application\n",
      "â— Choose the appropriate Python package to extract document content from provided source data and format.\n",
      "â— Define operations and sequence to write given chunked text into Delta Lake tables in Unity Catalog\n",
      "â— Identify needed source documents that provide necessary knowledge and quality for a given RAG application\n",
      "â— Identify prompt/response pairs that align with a given model task\n",
      "â— Use tools and metrics to evaluate retrieval performance\n",
      "\n",
      "Section 3: Application Development (30%)\n",
      "â— Create tools needed to extract data for a given data retrieval need\n",
      "â— Select Langchain/similar tools for use in a Generative AI application.\n",
      "â— Identify how prompt formats can change model outputs and results\n",
      "â— Qualitatively assess responses to identify common issues such as quality and safety\n",
      "â— Select chunking strategy based on model & retrieval evaluation\n",
      "â— Augment a prompt with additional context from a user's input based on key fields, terms and intents\n",
      "â— Create a prompt that adjusts an LLM's response from a baseline to a desired output\n",
      "â— Implement LLM guardrails to prevent negative outcomes\n",
      "â— Write metaprompts that minimize hallucinations or leaking private data\n",
      "â— Build agent prompt templates exposing available functions\n",
      "â— Select the best LLM based on the attributes of the application to be developed\n",
      "â— Select a embedding model context length based on source documents, expected queries and optimization strategy\n",
      "â— Select a model for from a model hub or marketplace for a task based on model metadata/model cards\n",
      "â— Select the best model for a given task based on common metrics generated in experiments\n",
      "\n",
      "Section 4: Assembling and Deploying Applications (22%)\n",
      "â— Code a chain using a pyfunc model with pre- and post-processing\n",
      "â— Control access to resources from model serving endpoints\n",
      "â— Code a simple chain according to requirements\n",
      "â— Code a simple chain using langchain\n",
      "â— Choose the basic elements needed to create a RAG application: model flavor, embedding model, retriever, dependencies, input examples, model signature\n",
      "â— Register the model to Unity Catalog using MLflow\n",
      "â— Sequence the steps needed to deploy an endpoint for a basic RAG application\n",
      "â— Create and query a Vector Search index\n",
      "â— Identify how to serve an LLM application that leverages Foundation Model APIs\n",
      "â— Identify resources needed to serve features for a RAG application\n",
      "\n",
      "\n",
      "Section 5: Governance (8%)\n",
      "â— Use masking techniques as guard rails to meet a performance objective\n",
      "â— Select guardrail techniques to protect against malicious user inputs to a Gen AI application\n",
      "â— Recommend an alternative for problematic text mitigation in a data source feeding a RAG application\n",
      "â— Use legal/licensing requirements for data sources to avoid legal risk\n",
      "\n",
      "\n",
      "Section 6: Evaluation and Monitoring (12%)\n",
      "â— Select an LLM choice (size and architecture) based on a set of quantitative evaluation metrics\n",
      "â— Select key metrics to monitor for a specific LLM deployment scenario\n",
      "â— Evaluate model performance in a RAG application using MLflow\n",
      "â— Use inference logging to assess deployed RAG application performance\n",
      "â— Use Databricks features to control LLM costs for RAG applications\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Sample Questions\n",
      "\n",
      "These questions are similar to actual question items and give you a general sense of how questions are asked on this exam. \n",
      "They include exam objectives as they are stated on the exam guide and give you a sample question that aligns to the objective. \n",
      "The exam guide lists all of the objectives that could be covered on an exam. \n",
      "The best way to prepare for a certification exam is to review theexam outline in the exam guide.\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Number: \n",
      "1\n",
      "\n",
      "Skill Section: \n",
      "\n",
      " \n",
      "Question:  \n",
      "A Generative Al Engineer has created a RAG application to look up answers to questions about a series of fantasy novels that are being asked on the authorâ€™s web forum. The fantasy novel texts are chunked and embedded into a vector store with metadata (page number, chapter number, book title), retrieved with the userâ€™s query, and provided to an LLM for response generation. The Generative AI Engineer used their intuition to pick the chunking strategy and associated configurations but now wants to more methodically choose the best values.\n",
      "Which TWO strategies should the Generative AI Engineer take to optimize their chunking strategy and parameters? (Choose two.)\n",
      "\n",
      "Answer: \n",
      "3 and 5\n",
      "\n",
      "\n",
      "Difficulty Level ( Easy, Medium, Intense ):\n",
      "M\n",
      "\n",
      "Guideline Time in Seconds (30, 45, 60):\n",
      "45\n",
      "\n",
      "Option 1:\n",
      "Change embedding models and compare performance.\n",
      "\n",
      "Option 2:\n",
      "Add a classifier for user queries that predicts which book will best contain the answer. Use this to filter retrieval.\n",
      "\n",
      "Option 3:\n",
      "Choose an appropriate evaluation metric (such as recall or NDCG) and experiment with changes in the chunking strategy, such as splitting chunks by paragraphs or chapters. Choose the strategy that gives the best performance metric.\n",
      "\n",
      "Option 4:\n",
      "Pass known questions and best answers to an LLM and instruct the LLM to provide the best token count. Use a summary statistic (mean, median, etc.) of the best token counts to choose chunk size.\n",
      "\n",
      "Option 5:\n",
      "Create an LLM-as-a-judge metric to evaluate how well previous questions are answered by the most appropriate chunk. Optimize the chunking parameters based upon the values of the metric.\n",
      "\n",
      "\n",
      "Explanation:\n",
      "Explanation for Right Options:\n",
      "Option 3:\n",
      "\n",
      "Explanation: This option suggests using evaluation metrics like recall or NDCG (Normalized Discounted Cumulative Gain) to assess how well different chunking strategies retrieve relevant information. By experimenting with chunking strategies (e.g., splitting by paragraphs or chapters), the engineer can identify which configuration maximizes the chosen metric.\n",
      "Justification: This method is systematic and data-driven, ensuring that the chunking strategy is optimized based on objective performance measures rather than intuition.\n",
      "\n",
      "Option 5:\n",
      "\n",
      "Explanation: This approach involves using an LLM-as-a-judge metric to evaluate the quality of responses generated from different chunking strategies. By comparing how well each chunk answers previous questions, the engineer can fine-tune the chunking parameters to improve overall retrieval and response accuracy.\n",
      "Justification: This strategy leverages the capabilities of LLMs to assess content relevance and accuracy, providing a practical way to optimize chunking parameters based on real-world performance.\n",
      "\n",
      "\n",
      "Explanation for Wrong Options:\n",
      "Option 1:\n",
      "\n",
      "Explanation: Changing embedding models and comparing performance focuses on the embeddings themselves rather than the chunking strategy.\n",
      "Justification: The question is about optimizing chunking strategy and parameters, not about experimenting with different embedding models. While important, this option doesn't directly address the chunking strategy optimization.\n",
      "\n",
      "Option 2:\n",
      "\n",
      "Explanation: Adding a classifier to predict which book might contain the answer is about improving the retrieval process but not specifically related to optimizing chunk sizes or strategies.\n",
      "Justification: This option introduces a different layer of filtering but does not focus on the core issue of chunking strategy optimization, which is the main concern of the engineer.\n",
      "\n",
      "Option 4:\n",
      "\n",
      "Explanation: This option involves using LLMs to predict optimal token counts for chunks based on known questions and answers.\n",
      "Justification: While this could help in choosing chunk sizes, it relies on token count as a heuristic rather than performance metrics like recall or NDCG. Itâ€™s less methodical and may not yield the best performance in terms of information retrieval and relevance.\n",
      "\n",
      "\n",
      "Summary:\n",
      "The best strategies for optimizing the chunking strategy and parameters involve using evaluation metrics (Option 3) and leveraging LLM-based assessments (Option 5). \n",
      "These methods provide systematic and data-driven approaches to fine-tuning the chunking process, ensuring the highest possible performance for retrieving relevant information. \n",
      "Other options either focus on different aspects (like embedding models or classifiers) or rely on heuristics that are less directly tied to improving chunking strategies.\n",
      "\n",
      "\n",
      "References:\n",
      "\n",
      "Hands-On: \n",
      "\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Number: \n",
      "2\n",
      "\n",
      "Skill Section: \n",
      " \n",
      "Question:  \n",
      "A Generative AI Engineer is designing a RAG application for answering user questions on technical regulations as they learn a new sport.\n",
      "What are the steps needed to build this RAG application and deploy it?\n",
      "\n",
      "Answer: \n",
      "2\n",
      "\n",
      "Difficulty Level ( Easy, Medium, Intense ):\n",
      "M\n",
      "\n",
      "Guideline Time in Seconds (30, 45, 60):\n",
      "45\n",
      "\n",
      "Option 1:\n",
      "Ingest documents from a source â€“> Index the docu\n"
     ]
    }
   ],
   "source": [
    "# def extract_pdf_text(pdf_path: str) -> str:\n",
    "#     doc = fitz.open(pdf_path)\n",
    "#     full_text = \"\"\n",
    "#     for page in doc:\n",
    "#         full_text += page.get_text()\n",
    "#     doc.close()\n",
    "#     return full_text\n",
    "\n",
    "# Load your Semantic Kernel PDF\n",
    "txt_path = \"GEN_AI 3.txt\"  # update if different\n",
    "raw_text = \"\"\n",
    "\n",
    "with open(txt_path,'r') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Quick preview\n",
    "print(raw_text[:12000])  # Show first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae89b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-empty pages: 130\n",
      "Page 1 preview:\n",
      "{'text': \"Number: \\n1\\n\\nSkill Section: \\n\\n \\nQuestion:  \\nA Generative Al Engineer has created a RAG application to look up answers to questions about a series of fantasy novels that are being asked on the authorâ€™s web forum. The fantasy novel texts are chunked and embedded into a vector store with metadata (page number, chapter number, book title), retrieved with the userâ€™s query, and provided to an LLM for response generation. The Generative AI Engineer used their intuition to pick the chunking strategy and associated configurations but now wants to more methodically choose the best values.\\nWhich TWO strategies should the Generative AI Engineer take to optimize their chunking strategy and parameters? (Choose two.)\\n\\nAnswer: \\n3 and 5\\n\\n\\nDifficulty Level ( Easy, Medium, Intense ):\\nM\\n\\nGuideline Time in Seconds (30, 45, 60):\\n45\\n\\nOption 1:\\nChange embedding models and compare performance.\\n\\nOption 2:\\nAdd a classifier for user queries that predicts which book will best contain the answer. Use this to filter retrieval.\\n\\nOption 3:\\nChoose an appropriate evaluation metric (such as recall or NDCG) and experiment with changes in the chunking strategy, such as splitting chunks by paragraphs or chapters. Choose the strategy that gives the best performance metric.\\n\\nOption 4:\\nPass known questions and best answers to an LLM and instruct the LLM to provide the best token count. Use a summary statistic (mean, median, etc.) of the best token counts to choose chunk size.\\n\\nOption 5:\\nCreate an LLM-as-a-judge metric to evaluate how well previous questions are answered by the most appropriate chunk. Optimize the chunking parameters based upon the values of the metric.\\n\\n\\nExplanation:\\nExplanation for Right Options:\\nOption 3:\\n\\nExplanation: This option suggests using evaluation metrics like recall or NDCG (Normalized Discounted Cumulative Gain) to assess how well different chunking strategies retrieve relevant information. By experimenting with chunking strategies (e.g., splitting by paragraphs or chapters), the engineer can identify which configuration maximizes the chosen metric.\\nJustification: This method is systematic and data-driven, ensuring that the chunking strategy is optimized based on objective performance measures rather than intuition.\\n\\nOption 5:\\n\\nExplanation: This approach involves using an LLM-as-a-judge metric to evaluate the quality of responses generated from different chunking strategies. By comparing how well each chunk answers previous questions, the engineer can fine-tune the chunking parameters to improve overall retrieval and response accuracy.\\nJustification: This strategy leverages the capabilities of LLMs to assess content relevance and accuracy, providing a practical way to optimize chunking parameters based on real-world performance.\\n\\n\\nExplanation for Wrong Options:\\nOption 1:\\n\\nExplanation: Changing embedding models and comparing performance focuses on the embeddings themselves rather than the chunking strategy.\\nJustification: The question is about optimizing chunking strategy and parameters, not about experimenting with different embedding models. While important, this option doesn't directly address the chunking strategy optimization.\\n\\nOption 2:\\n\\nExplanation: Adding a classifier to predict which book might contain the answer is about improving the retrieval process but not specifically related to optimizing chunk sizes or strategies.\\nJustification: This option introduces a different layer of filtering but does not focus on the core issue of chunking strategy optimization, which is the main concern of the engineer.\\n\\nOption 4:\\n\\nExplanation: This option involves using LLMs to predict optimal token counts for chunks based on known questions and answers.\\nJustification: While this could help in choosing chunk sizes, it relies on token count as a heuristic rather than performance metrics like recall or NDCG. Itâ€™s less methodical and may not yield the best performance in terms of information retrieval and relevance.\\n\\n\\nSummary:\\nThe best strategies for optimizing the chunking strategy and parameters involve using evaluation metrics (Option 3) and leveraging LLM-based assessments (Option 5). \\nThese methods provide systematic and data-driven approaches to fine-tuning the chunking process, ensuring the highest possible performance for retrieving relevant information. \\nOther options either focus on different aspects (like embedding models or classifiers) or rely on heuristics that are less directly tied to improving chunking strategies.\\n\\n\\nReferences:\\n\\nHands-On: \\n\\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "# Load and chunk PDF page-wise\n",
    "import re\n",
    "\n",
    "def extract_txt_queswise(raw_text: str):\n",
    "    chunks = re.split(r\"\\++\\n\", raw_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Run the function\n",
    "page_chunks = extract_txt_queswise(raw_text)\n",
    "page_chunks = page_chunks[3:]\n",
    "page_chunks=  [{\"text\": chunk} for chunk in page_chunks]\n",
    "\n",
    "# Example output\n",
    "print(f\"Total non-empty pages: {len(page_chunks)}\")\n",
    "print(f\"Page 1 preview:\\n{page_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad20757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a free, local embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9ea601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710bcad9ba614d3f856e5287c2be37dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings: 130\n",
      "Shape of one embedding: (384,)\n"
     ]
    }
   ],
   "source": [
    "# Extract just the text for embedding\n",
    "texts = [chunk for chunk in page_chunks]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Total embeddings: {len(embeddings)}\")\n",
    "print(f\"Shape of one embedding: {embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea7b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link Embedding to Chunk\n",
    "for i in range(len(page_chunks)):\n",
    "    page_chunks[i][\"embedding\"] = embeddings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a0d0c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index has 130 vectors.\n"
     ]
    }
   ],
   "source": [
    "# Embeddings must be a 2D float32 numpy array\n",
    "embedding_matrix = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embedding_matrix.shape[1]  # typically 384 for MiniLM\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 = Euclidean Distance\n",
    "\n",
    "# Add vectors\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "print(f\"FAISS index has {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a552c90",
   "metadata": {},
   "source": [
    "### Run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200344f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the Index for Later\n",
    "# faiss.write_index(index, \"GenAI.index\")\n",
    "\n",
    "# To load it later\n",
    "index = faiss.read_index(\"GenAI.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b5625f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Example query\n",
    "query = \"\"\"A Generative Al Engineer has been asked to design an LLM-based application that accomplishes the following business objective: answer employee HR questions using HR PDF documentation.\n",
    "Which set of high level tasks should the Generative Al Engineer's system perform?\n",
    "B\n",
    "c\n",
    "Calculate averaged embeddings for each HR document, compare embeddings to user query to find the best document. Pass the best document with the user query into an LLM with a large context window to generate a\n",
    "response to the employee.\n",
    "Use an LLM to summarize HR documentation. Provide summaries of documentation and user query into an LLM with a large context window to generate a response to the user.\n",
    "Split HR documentation into chunks and embed into a vector store. Use the employee question to retrieve best matched chunks of documentation, and use the LLM to generate a response to the employee based upon the\n",
    "documentation retrieved.\n",
    "Create an interaction matrix of historical employee questions and HR documentation. use ALS to factorize the matrix and create embeddings. Calculate the embeddings of new queries and use them to find the best HR\n",
    "documentation. Use an LLM to generate a response to the employee question based upon the documentation retrieved.\"\"\"\n",
    "\n",
    "# Embed the query\n",
    "query_embedding = model.encode([query]).astype('float32')\n",
    "\n",
    "# Search top-k most similar pages\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Display results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    page_info = page_chunks[idx]\n",
    "    # print(f\"\\nðŸ”¹ Match {i+1} â€” Page {page_info['text']}\")\n",
    "    # print(f\"Text Preview:\\n{page_info['text'][:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ada7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare prompt from top-k results\n",
    "retrieved_chunks = [page_chunks[idx][\"text\"] for idx in indices[0]]\n",
    "\n",
    "# You can trim or merge them\n",
    "context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "prompt = f\"\"\"\n",
    "# Overview\n",
    "You are an AI-powered Q&A assistant designed to answer questions related to Generative AI engineering, RAG (Retrieval-Augmented Generation), Databricks features, and related technical topics.  \n",
    "Your primary knowledge source is a text-based dataset containing question-answer pairs, explanations, and summaries related to AI engineering concepts.  \n",
    "You are integrated within a Semantic Kernel framework that retrieves relevant chunks of data using semantic search and passes them to you for reasoning and response generation.\n",
    "\n",
    "---\n",
    "\n",
    "## Context\n",
    "- You operate as part of a Retrieval-Augmented Generation (RAG) system.\n",
    "- Your retrieved context will come from pre-processed text documents containing multiple-choice questions, answers, explanations, and summaries.\n",
    "- If the relevant information is not found in the knowledge base, you may use your general knowledge of Databricks, LLMs, and RAG systems to infer an accurate response.\n",
    "- Your answers must be concise and formatted in an exam-style toneâ€”clear, direct, and to the point.\n",
    "- You should not cite metadata such as question numbers, difficulty levels, or skill sections in responses.\n",
    "- When external information is used, it should align with Databricks documentation and modern AI engineering practices.\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "1. Read the userâ€™s question carefully.\n",
    "2. Review the retrieved text chunks from the knowledge base.\n",
    "3. Extract the most relevant facts, reasoning steps, and conclusions from the retrieved context.\n",
    "4. Generate a concise, exam-style answer that directly addresses the userâ€™s query.\n",
    "5. If the answer is not explicitly found in the provided context:\n",
    "   - Use general knowledge and reasoning.\n",
    "   - Reference conceptual guidance consistent with Databricks official documentation and standard RAG methodologies.\n",
    "6. Avoid unnecessary elaboration or background information unless it directly clarifies the answer.\n",
    "7. Do not mention metadata fields (e.g., â€œSkill Section,â€ â€œDifficulty,â€ â€œGuideline Timeâ€).\n",
    "8. Never state that the answer was â€œnot found.â€ Instead, provide the best possible inference based on relevant principles.\n",
    "9. Maintain factual accuracy and professional tone throughout.\n",
    "\n",
    "---\n",
    "\n",
    "## Tools\n",
    "- Semantic Kernel framework for retrieval and orchestration.\n",
    "- Vector store for semantic document search.\n",
    "- Large Language Model (LLM) for reasoning and answer generation.\n",
    "- Databricks documentation as an external reference for general knowledge fallback.\n",
    "\n",
    "---\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Example 1\n",
    "**User Input:**  \n",
    "â€œWhat are two methods to optimize the chunking strategy in a RAG pipeline?â€\n",
    "\n",
    "**Expected Output:**  \n",
    "Use evaluation metrics like recall or NDCG to compare chunking strategies, and apply an LLM-based evaluation to judge response relevance. These approaches provide data-driven and qualitative optimization of chunk parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2\n",
    "**User Input:**  \n",
    "â€œHow should a Generative AI Engineer design a RAG application for answering technical regulation queries?â€\n",
    "\n",
    "**Expected Output:**  \n",
    "Ingest and index documents into vector search, allow the LLM to retrieve relevant content, generate responses, evaluate performance, and deploy using model serving.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 3\n",
    "**User Input:**  \n",
    "â€œWhat Databricks feature can log requests and responses in model serving?â€\n",
    "\n",
    "**Expected Output:**  \n",
    "Use Inference Tables to automatically capture and log incoming requests and outgoing responses for model monitoring and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## SOP (Standard Operating Procedure)\n",
    "1. Receive the userâ€™s query.\n",
    "2. Retrieve the most relevant context chunks using semantic search.\n",
    "3. Summarize and synthesize the retrieved information.\n",
    "4. Generate a precise, exam-style answer using:\n",
    "   - Retrieved content if relevant.\n",
    "   - Databricks and LLM general knowledge if missing.\n",
    "5. Return the answer without metadata or references.\n",
    "6. Maintain consistency, factual accuracy, and brevity.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Notes\n",
    "- The RAG assistant should emulate a subject-matter expert providing quick, correct, and concise answers.\n",
    "- Always prioritize correctness, clarity, and conciseness.\n",
    "- Default to authoritative Databricks or Generative AI best practices when context is insufficient.\n",
    "- Avoid speculation or verbose explanations unless necessary for clarity.\n",
    "---\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "  \n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8fa54e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the HR PDFs into manageable chunks, embed the chunks into a vector store, retrieve the most semantically relevant chunks in response to an employeeâ€™s query, and then pass those retrieved chunks along with the query to an LLM for answer generation.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Replace with your actual Groq API key\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Example: Using the 'llama3-70b-8192' model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the modelâ€™s response\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c3c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17beb523",
   "metadata": {},
   "source": [
    "## for stream lit pickel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699037f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved text_chunks.pkl successfully.\n"
     ]
    }
   ],
   "source": [
    "# Run this code once to save your chunk data\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# --- Re-run your chunking logic from Cell 3 ---\n",
    "with open(\"GEN_AI 3.txt\", 'r') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "def extract_txt_queswise(raw_text: str):\n",
    "    chunks = re.split(r\"\\++\\n\", raw_text)\n",
    "    return chunks\n",
    "\n",
    "page_chunks = extract_txt_queswise(raw_text)\n",
    "page_chunks = page_chunks[3:]\n",
    "page_chunks = [{\"text\": chunk} for chunk in page_chunks]\n",
    "# ---------------------------------------------\n",
    "\n",
    "# We only need the text, not the embeddings\n",
    "text_chunks = [chunk['text'] for chunk in page_chunks]\n",
    "\n",
    "# Save the text chunks to a file\n",
    "with open(\"text_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(text_chunks, f)\n",
    "\n",
    "print(\"Saved text_chunks.pkl successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f842a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
